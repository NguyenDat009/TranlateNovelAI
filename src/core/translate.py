import os
# import google.generativeai as genai  # Removed - using 100% OpenRouter
import time
import json
import re
import concurrent.futures
import threading
from multiprocessing import cpu_count
import math
from typing import Optional
from itertools import cycle

# Import ENHANCED rate limiter for Google AI (with TPM/RPD tracking)
try:
    from .enhanced_rate_limiter import EnhancedRateLimiter, ImprovedKeyRotator
    from .rate_limiter import exponential_backoff_sleep, is_rate_limit_error, _get_key_hash
except ImportError:
    try:
        from enhanced_rate_limiter import EnhancedRateLimiter, ImprovedKeyRotator
        from rate_limiter import exponential_backoff_sleep, is_rate_limit_error, _get_key_hash
    except ImportError:
        print("âš ï¸ Enhanced rate limiter module not found, falling back to basic")
        try:
            from .rate_limiter import get_rate_limiter, exponential_backoff_sleep, is_rate_limit_error, _get_key_hash
            EnhancedRateLimiter = None
            ImprovedKeyRotator = None
        except ImportError:
            from rate_limiter import get_rate_limiter, exponential_backoff_sleep, is_rate_limit_error, _get_key_hash
            EnhancedRateLimiter = None
            ImprovedKeyRotator = None
        def exponential_backoff_sleep(retry_count, base_delay=2.0, max_delay=120.0):
            """
            Improved exponential backoff vá»›i jitter Ä‘á»ƒ trÃ¡nh thundering herd
            """
            import random
            
            # TÃ­nh delay cÆ¡ báº£n vá»›i exponential backoff
            delay = base_delay * (2 ** retry_count)
            
            # ThÃªm jitter (random factor) Ä‘á»ƒ trÃ¡nh nhiá»u thread retry cÃ¹ng lÃºc
            jitter = random.uniform(0.1, 0.5)  # 10-50% jitter
            delay = delay * (1 + jitter)
            
            # Giá»›i háº¡n max delay
            delay = min(delay, max_delay)
            
            print(f"ğŸ’¤ Exponential backoff: {delay:.1f}s (retry #{retry_count + 1})")
            time.sleep(delay)
        def is_rate_limit_error(error_message):
            return "429" in str(error_message).lower() or "rate limit" in str(error_message).lower()
        def _get_key_hash(api_key):
            import hashlib
            return hashlib.md5(api_key.encode()).hexdigest()[:8]

# Import reformat function
try:
    from .reformat import fix_text_format
    CAN_REFORMAT = True
    print("Da import thanh cong chuc nang reformat")
except ImportError:
    CAN_REFORMAT = False
    print("Khong the import reformat.py - chuc nang reformat se bi tat")

# --- Cáº¤U HÃŒNH CÃC Háº°NG Sá» ---
MAX_RETRIES_ON_SAFETY_BLOCK = 5
MAX_RETRIES_ON_BAD_TRANSLATION = 5
MAX_RETRIES_ON_RATE_LIMIT = 5  # TÄƒng sá»‘ láº§n retry khi gáº·p rate limit Ä‘á»ƒ xá»­ lÃ½ tá»‘t hÆ¡n
RETRY_DELAY_SECONDS = 2
PROGRESS_FILE_SUFFIX = ".progress.json"
CHUNK_SIZE = 1024 * 1024  # 1MB (KhÃ´ng cÃ²n dÃ¹ng trá»±c tiáº¿p CHUNK_SIZE cho viá»‡c Ä‘á»c file ná»¯a)

# --- DEBUG RESPONSE LOGGING ---
DEBUG_RESPONSE_ENABLED = True  # Báº­t/táº¯t debug logging
DEBUG_RESPONSE_LOCK = threading.Lock()

def save_debug_response(chunk_index, response_text, chunk_lines, input_file, provider="Unknown", model_name="Unknown", key_hash="Unknown"):
    """
    LÆ°u response ngay láº­p tá»©c vÃ o file debug Ä‘á»ƒ kiá»ƒm tra.
    File debug sáº½ Ä‘Æ°á»£c lÆ°u cÃ¹ng thÆ° má»¥c vá»›i input file.
    
    Args:
        chunk_index: Sá»‘ thá»© tá»± chunk
        response_text: Ná»™i dung response tá»« API
        chunk_lines: Ná»™i dung gá»‘c cá»§a chunk
        input_file: ÄÆ°á»ng dáº«n file input
        provider: Provider name (OpenRouter/Google AI)
        model_name: TÃªn model
        key_hash: Hash cá»§a API key Ä‘ang dÃ¹ng
    """
    if not DEBUG_RESPONSE_ENABLED:
        return
    
    try:
        # Táº¡o tÃªn file debug dá»±a trÃªn input file
        input_dir = os.path.dirname(input_file)
        input_basename = os.path.basename(input_file)
        input_name = os.path.splitext(input_basename)[0]
        
        debug_file = os.path.join(input_dir, f"{input_name}_debug_responses.txt")
        
        # LÆ°u vÃ o file vá»›i thread-safe
        with DEBUG_RESPONSE_LOCK:
            with open(debug_file, 'a', encoding='utf-8') as f:
                # ThÃªm separator vÃ  metadata
                f.write("\n" + "="*80 + "\n")
                f.write(f"CHUNK #{chunk_index} - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Provider: {provider} | Model: {model_name} | Key: ***{key_hash}\n")
                f.write("-"*80 + "\n")
                
                # Ghi ná»™i dung gá»‘c
                f.write("ã€ORIGINAL TEXTã€‘:\n")
                f.write("\n".join(chunk_lines[:3]))  # Chá»‰ lÆ°u 3 dÃ²ng Ä‘áº§u Ä‘á»ƒ tham kháº£o
                if len(chunk_lines) > 3:
                    f.write(f"\n... ({len(chunk_lines) - 3} more lines)")
                f.write("\n\n")
                
                # Ghi response
                f.write("ã€API RESPONSEã€‘:\n")
                f.write(response_text)
                f.write("\n")
                f.write("="*80 + "\n\n")
        
        # Log thÃ´ng bÃ¡o (chá»‰ log láº§n Ä‘áº§u)
        if chunk_index <= 1:
            print(f"ğŸ› Debug mode ON - Responses Ä‘Æ°á»£c lÆ°u vÃ o: {os.path.basename(debug_file)}")
            
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi lÆ°u debug response: {e}")

# --- ADAPTIVE THREAD SCALING ---
class AdaptiveThreadManager:
    """
    Quáº£n lÃ½ adaptive thread scaling - tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh threads dá»±a trÃªn rate limit
    """
    def __init__(self, initial_threads, min_threads=2, max_threads=50):
        self.current_threads = initial_threads
        self.initial_threads = initial_threads
        self.min_threads = min_threads
        self.max_threads = max_threads
        
        # Tracking rate limit
        self.rate_limit_count = 0
        self.total_requests = 0
        self.successful_requests = 0
        
        # Scaling parameters
        self.rate_limit_threshold = 0.3  # 30% rate limit triggers scaling down
        self.scale_down_factor = 0.6     # Giáº£m 40% threads
        self.scale_up_factor = 1.2       # TÄƒng 20% threads
        self.min_requests_for_scaling = 20  # Tá»‘i thiá»ƒu requests Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
        
        # Cooldown Ä‘á»ƒ trÃ¡nh oscillation
        self.last_scale_time = 0
        self.scale_cooldown = 30  # 30 giÃ¢y cooldown
        
        import threading
        self.lock = threading.Lock()
        
    def report_rate_limit(self):
        """BÃ¡o cÃ¡o gáº·p rate limit"""
        with self.lock:
            self.rate_limit_count += 1
            self.total_requests += 1
            self._evaluate_scaling()
    
    def report_success(self):
        """BÃ¡o cÃ¡o request thÃ nh cÃ´ng"""
        with self.lock:
            self.successful_requests += 1
            self.total_requests += 1
            self._evaluate_scaling()
    
    def report_other_error(self):
        """BÃ¡o cÃ¡o lá»—i khÃ¡c (khÃ´ng pháº£i rate limit)"""
        with self.lock:
            self.total_requests += 1
    
    def _evaluate_scaling(self):
        """ÄÃ¡nh giÃ¡ vÃ  thá»±c hiá»‡n scaling náº¿u cáº§n"""
        import time
        
        # Chá»‰ Ä‘Ã¡nh giÃ¡ sau khi cÃ³ Ä‘á»§ data
        if self.total_requests < self.min_requests_for_scaling:
            return
            
        # Kiá»ƒm tra cooldown
        current_time = time.time()
        if current_time - self.last_scale_time < self.scale_cooldown:
            return
        
        # TÃ­nh rate limit ratio
        rate_limit_ratio = self.rate_limit_count / self.total_requests
        success_ratio = self.successful_requests / self.total_requests
        
        print(f"ğŸ“Š Thread Manager Stats: Rate Limit: {rate_limit_ratio:.1%}, Success: {success_ratio:.1%}, Current Threads: {self.current_threads}")
        
        # Scale down náº¿u rate limit cao
        if rate_limit_ratio > self.rate_limit_threshold and self.current_threads > self.min_threads:
            new_threads = max(int(self.current_threads * self.scale_down_factor), self.min_threads)
            if new_threads < self.current_threads:
                self.current_threads = new_threads
                self.last_scale_time = current_time
                self._reset_stats()
                print(f"ğŸ”» SCALE DOWN: Giáº£m threads xuá»‘ng {self.current_threads} do rate limit cao ({rate_limit_ratio:.1%})")
                return True
        
        # Scale up náº¿u success rate cao vÃ  Ã­t rate limit
        elif rate_limit_ratio < 0.1 and success_ratio > 0.8 and self.current_threads < self.initial_threads:
            new_threads = min(int(self.current_threads * self.scale_up_factor), self.initial_threads)
            if new_threads > self.current_threads:
                self.current_threads = new_threads
                self.last_scale_time = current_time
                self._reset_stats()
                print(f"ğŸ”º SCALE UP: TÄƒng threads lÃªn {self.current_threads} do performance tá»‘t")
                return True
                
        return False
    
    def _reset_stats(self):
        """Reset statistics sau khi scale"""
        self.rate_limit_count = 0
        self.total_requests = 0
        self.successful_requests = 0
    
    def get_current_threads(self):
        """Láº¥y sá»‘ threads hiá»‡n táº¡i"""
        with self.lock:
            return self.current_threads
    
    def should_restart_with_new_threads(self):
        """Kiá»ƒm tra xem cÃ³ cáº§n restart vá»›i threads má»›i khÃ´ng"""
        with self.lock:
            return self.current_threads != self.initial_threads

# KÃ­ch thÆ°á»›c cá»­a sá»• ngá»¯ cáº£nh (sá»‘ Ä‘oáº¡n vÄƒn báº£n trÆ°á»›c Ä‘Ã³ dÃ¹ng lÃ m ngá»¯ cáº£nh)
CONTEXT_WINDOW_SIZE = 5
# KÃ½ tá»± Ä‘áº·c biá»‡t Ä‘á»ƒ Ä‘Ã¡nh dáº¥u pháº§n cáº§n dá»‹ch trong prompt gá»­i Ä‘áº¿n AI
TRANSLATE_TAG_START = "<translate_this>"
TRANSLATE_TAG_END = "</translate_this>"

# Sá»‘ dÃ²ng gom láº¡i thÃ nh má»™t chunk Ä‘á»ƒ dá»‹ch
CHUNK_SIZE_LINES = 100

# Global stop event Ä‘á»ƒ dá»«ng tiáº¿n trÃ¬nh dá»‹ch
_stop_event = threading.Event()

# Global quota exceeded flag
_quota_exceeded = threading.Event()

# Key rotation class for Google AI multiple keys
class KeyRotator:
    """
    Thread-safe key rotator cho Google AI multiple keys
    Sá»­ dá»¥ng round-robin Ä‘á»ƒ xoay vÃ²ng giá»¯a cÃ¡c keys
    """
    def __init__(self, api_keys):
        """
        Args:
            api_keys: list of API keys hoáº·c single API key string
        """
        if isinstance(api_keys, list):
            self.keys = api_keys
            self.is_multi_key = len(api_keys) > 1
        else:
            self.keys = [api_keys]
            self.is_multi_key = False
        
        self.key_iterator = cycle(self.keys)
        self.lock = threading.Lock()
        self.key_usage = {key: 0 for key in self.keys}  # Track usage count
        
        if self.is_multi_key:
            print(f"Key Rotator: Da khoi tao voi {len(self.keys)} keys")
            print(f"He thong se tu dong xoay vong giua cac keys de toi uu RPM")
    
    def get_next_key(self):
        """Get next API key trong rotation"""
        with self.lock:
            key = next(self.key_iterator)
            self.key_usage[key] += 1
            return key
    
    def get_usage_stats(self):
        """Get usage statistics for all keys"""
        with self.lock:
            return dict(self.key_usage)
    
    def print_stats(self):
        """Print usage statistics"""
        if not self.is_multi_key:
            return
        
        stats = self.get_usage_stats()
        print("\nğŸ“Š Key Usage Statistics:")
        for idx, (key, count) in enumerate(stats.items(), 1):
            masked_key = key[:10] + "***" + key[-10:] if len(key) > 20 else "***"
            print(f"   Key #{idx} ({masked_key}): {count} requests")
        print()


def create_key_rotator(api_keys, same_project=False):
    """
    Táº¡o key rotator (ImprovedKeyRotator náº¿u cÃ³, fallback vá» KeyRotator)
    
    Args:
        api_keys: List of API keys hoáº·c single key
        same_project: Táº¥t cáº£ keys cÃ³ cÃ¹ng project khÃ´ng
        
    Returns:
        KeyRotator instance (Improved hoáº·c basic)
    """
    if ImprovedKeyRotator is not None:
        # Sá»­ dá»¥ng ImprovedKeyRotator vá»›i health tracking
        print("âœ¨ Sá»­ dá»¥ng ImprovedKeyRotator (vá»›i health tracking)")
        return ImprovedKeyRotator(api_keys, same_project=same_project)
    else:
        # Fallback vá» KeyRotator cÆ¡ báº£n
        print("âš ï¸ Fallback vá» KeyRotator cÆ¡ báº£n")
        return KeyRotator(api_keys)


def set_stop_translation():
    """Dá»«ng tiáº¿n trÃ¬nh dá»‹ch"""
    global _stop_event
    _stop_event.set()
    print("ğŸ›‘ ÄÃ£ yÃªu cáº§u dá»«ng tiáº¿n trÃ¬nh dá»‹ch...")

def clear_stop_translation():
    """XÃ³a flag dá»«ng Ä‘á»ƒ cÃ³ thá»ƒ tiáº¿p tá»¥c dá»‹ch"""
    global _stop_event, _quota_exceeded
    _stop_event.clear()
    _quota_exceeded.clear()
    print("â–¶ï¸ ÄÃ£ xÃ³a flag dá»«ng, sáºµn sÃ ng tiáº¿p tá»¥c...")


def is_translation_stopped():
    """Kiá»ƒm tra xem cÃ³ yÃªu cáº§u dá»«ng khÃ´ng"""
    global _stop_event
    return _stop_event.is_set()

def set_quota_exceeded():
    """ÄÃ¡nh dáº¥u API Ä‘Ã£ háº¿t quota"""
    global _quota_exceeded, _stop_event
    _quota_exceeded.set()
    _stop_event.set()  # CÅ©ng dá»«ng dá»‹ch
    print("API Ä‘Ã£ háº¿t quota - dá»«ng tiáº¿n trÃ¬nh dá»‹ch")

def is_quota_exceeded():
    """Kiá»ƒm tra xem API cÃ³ háº¿t quota khÃ´ng"""
    global _quota_exceeded
    return _quota_exceeded.is_set()

def check_openrouter_rate_limit_error(error_message):
    """Kiá»ƒm tra lá»—i Rate Limit (429) - cÃ³ thá»ƒ retry"""
    error_str = str(error_message).lower()
    rate_limit_keywords = [
        "rate limit exceeded",
        "rate_limit_exceeded", 
        "429",
        "too many requests",
        "requests per minute",
        "requests per second"
    ]
    return any(keyword in error_str for keyword in rate_limit_keywords)

def check_openrouter_quota_error(error_message):
    """Kiá»ƒm tra lá»—i Quota/Credit Insufficient (402) - cáº§n náº¡p credit"""
    error_str = str(error_message).lower()
    quota_keywords = [
        "402",
        "insufficient credits",
        "insufficient_credits",
        "exceeded your current quota", 
        "quota exceeded",
        "billing",
        "please check your plan",
        "credits",
        "balance"
    ]
    # KHÃ”NG BAO Gá»’M "429" vÃ  "rate limit" - Ä‘Ã³ lÃ  lá»—i khÃ¡c!
    return any(keyword in error_str for keyword in quota_keywords)

def check_openrouter_api_key_error(error_message):
    """Kiá»ƒm tra lá»—i API Key khÃ´ng há»£p lá»‡ (401)"""
    error_str = str(error_message).lower()
    api_key_keywords = [
        "401",
        "unauthorized", 
        "invalid credentials",
        "invalid_credentials",
        "api key not valid", 
        "invalid api key", 
        "authentication failed",
        "api_key_invalid", 
        "invalid_api_key", 
        "api key is invalid", 
        "bad api key"
    ]
    return any(keyword in error_str for keyword in api_key_keywords)

def check_openrouter_moderation_error(error_message):
    """Kiá»ƒm tra lá»—i Moderation (403) - ná»™i dung bá»‹ cáº¥m"""
    error_str = str(error_message).lower()
    moderation_keywords = [
        "403",
        "moderation",
        "content policy",
        "content_policy",
        "policy violation",
        "blocked content",
        "inappropriate content"
    ]
    return any(keyword in error_str for keyword in moderation_keywords)

def check_openrouter_timeout_error(error_message):
    """Kiá»ƒm tra lá»—i Timeout (408) - cÃ³ thá»ƒ retry"""
    error_str = str(error_message).lower()
    timeout_keywords = [
        "408",
        "timeout",
        "request timeout",
        "gateway timeout",
        "timed out"
    ]
    return any(keyword in error_str for keyword in timeout_keywords)

def check_openrouter_service_error(error_message):
    """Kiá»ƒm tra lá»—i Service (502, 503) - cÃ³ thá»ƒ retry"""
    error_str = str(error_message).lower()
    service_keywords = [
        "502",
        "503", 
        "bad gateway",
        "service unavailable",
        "server error",
        "internal server error",
        "model unavailable",
        "provider unavailable"
    ]
    return any(keyword in error_str for keyword in service_keywords)

# Legacy functions for backward compatibility
def check_quota_error(error_message):
    """Legacy function - sá»­ dá»¥ng check_openrouter_quota_error thay tháº¿"""
    return check_openrouter_quota_error(error_message)

def check_api_key_error(error_message):
    """Legacy function - sá»­ dá»¥ng check_openrouter_api_key_error thay tháº¿"""
    return check_openrouter_api_key_error(error_message)

def validate_api_key_before_translation(api_key, model_name, provider="OpenRouter"):
    """Validate API key trÆ°á»›c khi báº¯t Ä‘áº§u translation"""
    try:
        if provider == "Google AI":
            # Test Google AI API
            import google.generativeai as genai
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)
            
            # Test vá»›i content nhá» Ä‘á»ƒ kiá»ƒm tra quota
            test_content = "Hello, test quota"
            response = model.generate_content(test_content)
            
            if response and response.text:
                # ThÃªm thÃ´ng tin vá» project ID náº¿u cÃ³ thá»ƒ
                masked_key = api_key[:10] + "***" + api_key[-10:] if len(api_key) > 20 else "***"
                return True, f"Google AI API key há»£p lá»‡ ({masked_key})"
            else:
                return False, "Google AI API tráº£ vá» response rá»—ng"
                
        elif provider == "OpenRouter":
            # Test OpenRouter API
            import requests
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/TranslateNovelAI",
                "X-Title": "TranslateNovelAI"
            }
            
            payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": "Test"}],
                "max_tokens": 10
            }
            
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                return True, "OpenRouter API key há»£p lá»‡"
            elif response.status_code == 401:
                return False, "OpenRouter API Key khÃ´ng há»£p lá»‡ (401: Invalid Credentials)"
            elif response.status_code == 402:
                return False, "TÃ i khoáº£n OpenRouter háº¿t credit (402: Insufficient Credits)"
            elif response.status_code == 403:
                return False, "OpenRouter API bá»‹ cháº·n (403: Moderation Error)"
            elif response.status_code == 429:
                return False, "OpenRouter API bá»‹ rate limit (429: Too Many Requests) - thá»­ láº¡i sau"
            elif response.status_code in [502, 503]:
                return False, f"OpenRouter service táº¡m thá»i lá»—i ({response.status_code}) - thá»­ láº¡i sau"
            else:
                return False, f"Lá»—i OpenRouter API: HTTP {response.status_code}"
        else:
            return False, f"Provider khÃ´ng há»£p lá»‡: {provider}"
            
    except Exception as e:
        error_msg = str(e)
        if check_api_key_error(error_msg):
            return False, f"API Key khÃ´ng há»£p lá»‡: {error_msg}"
        elif check_quota_error(error_msg):
            return False, f"API háº¿t quota: {error_msg}"
        else:
            return False, f"Lá»—i káº¿t ná»‘i API: {error_msg}"

def get_optimal_threads(num_api_keys=1, provider="OpenRouter"):
    """
    Tá»± Ä‘á»™ng tÃ­nh toÃ¡n sá»‘ threads tá»‘i Æ°u dá»±a trÃªn cáº¥u hÃ¬nh mÃ¡y vÃ  sá»‘ lÆ°á»£ng API keys.
    
    Args:
        num_api_keys: Sá»‘ lÆ°á»£ng API keys (Ä‘á»ƒ tÃ­nh toÃ¡n threads phÃ¹ há»£p)
        provider: Provider Ä‘ang sá»­ dá»¥ng
    """
    try:
        # Láº¥y sá»‘ CPU cores
        cpu_cores = cpu_count()
        
        if provider == "Google AI" and num_api_keys > 1:
            # Vá»›i Google AI multiple keys, tÃ­nh toÃ¡n dá»±a trÃªn keys
            base_threads_per_key = 1.5  # Trung bÃ¬nh 1.5 threads/key
            threads_from_keys = int(num_api_keys * base_threads_per_key)
            threads_from_cpu = min(cpu_cores * 3, 50)  # I/O bound
            
            optimal_threads = min(threads_from_keys, threads_from_cpu)
            optimal_threads = max(optimal_threads, min(num_api_keys, 5))  # Tá»‘i thiá»ƒu
            optimal_threads = min(optimal_threads, 50)  # Tá»‘i Ä‘a
            
            print(f"Phat hien {cpu_cores} CPU cores")
            print(f"Google AI voi {num_api_keys} keys:")
            print(f"  Keys: {num_api_keys} x {base_threads_per_key} = {threads_from_keys} threads")
            print(f"  CPU: {cpu_cores} x 3 = {threads_from_cpu} threads")
            print(f"  Threads toi uu: {optimal_threads}")
        else:
            # Logic cÅ© cho single key hoáº·c OpenRouter
            optimal_threads = min(max(cpu_cores * 2, 4), 20)
            
            print(f"Phat hien {cpu_cores} CPU cores")
            print(f"Threads toi uu duoc de xuat: {optimal_threads}")
        
        return optimal_threads
        
    except Exception as e:
        print(f"Loi khi phat hien CPU cores: {e}")
        return 10  # Default fallback

def validate_threads(num_threads):
    """
    Validate sá»‘ threads Ä‘á»ƒ Ä‘áº£m báº£o trong khoáº£ng há»£p lÃ½.
    """
    try:
        num_threads = int(num_threads)
        if num_threads < 1:
            return 1
        elif num_threads > 50:  # Giá»›i háº¡n tá»‘i Ä‘a Ä‘á»ƒ trÃ¡nh rate limiting
            return 50
        return num_threads
    except (ValueError, TypeError):
        return get_optimal_threads()

def validate_chunk_size(chunk_size):
    """
    Validate chunk size Ä‘á»ƒ Ä‘áº£m báº£o trong khoáº£ng há»£p lÃ½.
    """
    try:
        chunk_size = int(chunk_size)
        if chunk_size < 10:
            return 10
        elif chunk_size > 2000:  # TrÃ¡nh chunks quÃ¡ lá»›n
            return 2000
        return chunk_size
    except (ValueError, TypeError):
        return 100  # Default


# Enhanced rate limiter cache
_enhanced_rate_limiters = {}
_enhanced_lock = threading.Lock()


def get_enhanced_rate_limiter(model_name: str, provider: str = "Google AI", api_key: str = None, is_paid_key: bool = False, desired_rpm: Optional[int] = None):
    """
    Get hoáº·c táº¡o ENHANCED rate limiter vá»›i TPM/RPD tracking
    
    IMPORTANT: Google AI Free tier rate limits are PER-PROJECT (not per-key!)
    Multiple keys from same project share the SAME rate limit.
    
    Args:
        model_name: TÃªn model
        provider: Provider (chá»‰ Ã¡p dá»¥ng cho Google AI)
        api_key: API key (IGNORED for free keys - use global limiter)
        is_paid_key: Key tráº£ phÃ­ hay free
        
    Returns:
        EnhancedRateLimiter instance hoáº·c None náº¿u khÃ´ng cáº§n rate limiting
    """
    # Chá»‰ rate limit cho Google AI
    if provider != "Google AI":
        return None
    
    # Fallback náº¿u khÃ´ng cÃ³ EnhancedRateLimiter
    if EnhancedRateLimiter is None:
        print("âš ï¸ EnhancedRateLimiter not available, skipping rate limiting")
        return None
    
    with _enhanced_lock:
        # ğŸš¨ CRITICAL: Free keys use GLOBAL limiter (per-project rate limit)
        # Paid keys can use per-key limiter (higher limits)
        if is_paid_key and api_key:
            key_hash = _get_key_hash(api_key)
            limiter_key = f"{model_name}_{key_hash}"
        else:
            # FREE KEYS: Use GLOBAL limiter for all keys (same project = shared limit)
            limiter_key = f"{model_name}_GLOBAL_FREE"
        
        if limiter_key not in _enhanced_rate_limiters:
            # XÃ¡c Ä‘á»‹nh RPM, TPM, RPD dá»±a trÃªn model
            rpm = 10  # Default
            tpm = None
            rpd = None
            
            if is_paid_key:
                # Paid keys: Very high limits
                rpm = 900
                tpm = 4000000  # 4M TPM
                rpd = None  # Unlimited
                safe_rpm = rpm
                safe_tpm = tpm
                
                key_display = f"key_***{key_hash}" if api_key else "default"
                print(f"ğŸ”§ [Enhanced] Táº¡o rate limiter cho model: {model_name} ({key_display})")
                print(f"   ğŸ’³ Paid Key: {safe_rpm} RPM, {safe_tpm:,} TPM, Unlimited RPD")
            else:
                # Free keys: Model-specific limits
                # Reference: https://ai.google.dev/gemini-api/docs/rate-limits
                # Updated October 2025: gemini-2.5-flash RPM reduced to 5
                if "2.0-flash-lite" in model_name.lower():
                    rpm, tpm, rpd = 30, 1000000, 200
                elif "2.0-flash" in model_name.lower():
                    rpm, tpm, rpd = 15, 1000000, 200
                elif "2.5-flash-lite" in model_name.lower():
                    rpm, tpm, rpd = 15, 1000000, 200
                elif "2.5-flash" in model_name.lower():
                    rpm, tpm, rpd = 5, 250000, 250  # âš ï¸ UPDATED: 5 RPM (not 10)
                elif "2.5-pro" in model_name.lower():
                    rpm, tpm, rpd = 5, 250000, 250
                elif "1.5-flash" in model_name.lower():
                    rpm, tpm, rpd = 15, 1000000, 1500
                elif "1.5-pro" in model_name.lower():
                    rpm, tpm, rpd = 2, 32000, 50
                else:
                    rpm, tpm, rpd = 15, 1000000, 200  # Default safe
                
                # Safety factor 85%
                safe_rpm = int(rpm * 0.85)
                safe_tpm = int(tpm * 0.85) if tpm else None
                safe_rpd = int(rpd * 0.85) if rpd else None
                
                if safe_rpm < 1:
                    safe_rpm = 1

            # Apply user-desired RPM override (clamped to safe_rpm)
            if desired_rpm is not None:
                try:
                    desired_rpm = int(desired_rpm)
                    if desired_rpm > 0:
                        original_safe = safe_rpm
                        safe_rpm = max(1, min(safe_rpm, desired_rpm))
                        if original_safe != safe_rpm:
                            print(f"ğŸ›ï¸ Override RPM tá»« UI: {original_safe} â†’ {safe_rpm} RPM (clamped to model-safe)")
                    else:
                        print("âš ï¸ desired_rpm khÃ´ng há»£p lá»‡ (<=0), bá» qua override")
                except (ValueError, TypeError):
                    print("âš ï¸ desired_rpm khÃ´ng há»£p lá»‡, bá» qua override")
                
                # Display info based on limiter type
                if limiter_key.endswith("_GLOBAL_FREE"):
                    print(f"ğŸ”§ [Enhanced] Táº¡o GLOBAL rate limiter cho model: {model_name}")
                    print(f"   ğŸ“Š Gá»‘c: {rpm} RPM, {tpm:,} TPM, {rpd} RPD (PER-PROJECT)")
                    print(f"   ğŸ›¡ï¸ Safe (85%): {safe_rpm} RPM, {safe_tpm:,} TPM, {safe_rpd} RPD")
                    print(f"   ğŸŒ GLOBAL: Táº¥t cáº£ keys chia sáº» CHUNG rate limit nÃ y")
                    print(f"   â„¹ï¸ Multiple keys CHá»ˆ Ä‘á»ƒ failover/backup, KHÃ”NG tÄƒng throughput")
                else:
                    key_display = f"key_***{key_hash}" if api_key else "default"
                    print(f"ğŸ”§ [Enhanced] Táº¡o rate limiter cho model: {model_name} ({key_display})")
                    print(f"   ğŸ“Š Gá»‘c: {rpm} RPM, {tpm:,} TPM, {rpd} RPD")
                    print(f"   ğŸ›¡ï¸ Safe (85%): {safe_rpm} RPM, {safe_tpm:,} TPM, {safe_rpd} RPD")
                    print(f"   â„¹ï¸ Per-key rate limit (paid key)")
            
            # Táº¡o EnhancedRateLimiter
            _enhanced_rate_limiters[limiter_key] = EnhancedRateLimiter(
                requests_per_minute=safe_rpm,
                tokens_per_minute=safe_tpm,
                requests_per_day=safe_rpd,
                window_seconds=60
            )
        
        return _enhanced_rate_limiters[limiter_key]


def estimate_tokens(text: str) -> int:
    """
    Æ¯á»›c tÃ­nh sá»‘ tokens tá»« text
    
    Args:
        text: Text cáº§n Æ°á»›c tÃ­nh
        
    Returns:
        Sá»‘ tokens Æ°á»›c tÃ­nh
        
    Note:
        - Tiáº¿ng Anh: ~4 chars/token
        - Tiáº¿ng Viá»‡t/Trung: ~2-3 chars/token
        - Conservative estimate Ä‘á»ƒ an toÃ n
    """
    if not text:
        return 0
    
    char_count = len(text)
    
    # Heuristic: 2.5 chars per token (conservative for Vietnamese/Chinese)
    # English lÃ  ~4 chars/token nhÆ°ng Asian languages dÃ y hÆ¡n
    estimated_tokens = int(char_count / 2.5)
    
    # Add 10% buffer for safety
    estimated_tokens = int(estimated_tokens * 1.1)
    
    return max(1, estimated_tokens)  # At least 1 token


# Default values
NUM_WORKERS = get_optimal_threads()  # Tá»± Ä‘á»™ng tÃ­nh theo mÃ¡y

def format_error_chunk(error_type: str, error_message: str, original_lines: list, line_range: str) -> str:
    """
    Format chunk bá»‹ lá»—i vá»›i ná»™i dung gá»‘c Ä‘á»ƒ lÆ°u vÃ o file
    
    Args:
        error_type: Loáº¡i lá»—i (API, QUOTA, SAFETY, etc.)
        error_message: ThÃ´ng bÃ¡o lá»—i chi tiáº¿t
        original_lines: Ná»™i dung gá»‘c cá»§a chunk
        line_range: Line range (vÃ­ dá»¥: "123:223")
    
    Returns:
        Formatted error text vá»›i ná»™i dung gá»‘c
    """
    original_text = ''.join(original_lines)  # Join lines, giá»¯ nguyÃªn line breaks
    
    error_output = f"""[[Lá»–I {error_type}: {error_message}

--- Ná»˜I DUNG Gá»C Cáº¦N Dá»ŠCH Láº I ---
{original_text}
--- Háº¾T Ná»˜I DUNG Gá»C ---
] [lines: {line_range}]]

"""
    return error_output


def threads_from_rpm(rpm: int, avg_latency_s: float = 2.0, safety: float = 0.85, max_threads: int = 50, min_threads: int = 1) -> int:
    """
    TÃ­nh sá»‘ threads Ä‘á» xuáº¥t dá»±a trÃªn RPM má»¥c tiÃªu Ä‘á»ƒ trÃ¡nh rate limit.

    Ã tÆ°á»Ÿng (Little's Law): concurrency â‰ˆ throughput Ã— latency.
    - throughput = rpm/60 (requests/second)
    - latency: thá»i gian trung bÃ¬nh má»™t request hoÃ n táº¥t (giÃ¢y)
    - safety: há»‡ sá»‘ an toÃ n Ä‘á»ƒ khÃ´ng cháº¡m tráº§n RPM (máº·c Ä‘á»‹nh 85%)

    Args:
        rpm: Requests Per Minute má»¥c tiÃªu (per-project Ä‘á»‘i vá»›i Google AI free)
        avg_latency_s: Äá»™ trá»… trung bÃ¬nh má»—i request (giÃ¢y). 2.0s lÃ  báº£o thá»§ cho Google AI free.
        safety: Há»‡ sá»‘ an toÃ n (<1.0) Ä‘á»ƒ trÃ¡nh va vÃ o giá»›i háº¡n.
        max_threads: Giá»›i háº¡n trÃªn threads Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i há»‡ thá»‘ng.
        min_threads: Giá»›i háº¡n dÆ°á»›i threads.

    Returns:
        Sá»‘ threads Ä‘á» xuáº¥t (int)
    """
    try:
        rpm = int(rpm)
        if rpm <= 0:
            return min_threads
    except (ValueError, TypeError):
        return min_threads

    req_per_sec_safe = (rpm / 60.0) * max(0.1, min(safety, 0.99))
    concurrency = math.ceil(req_per_sec_safe * max(0.2, avg_latency_s))
    return max(min_threads, min(max_threads, concurrency))


def is_bad_translation(text, input_text=None):
    """
    Kiá»ƒm tra xem báº£n dá»‹ch cá»§a chunk cÃ³ Ä‘áº¡t yÃªu cáº§u khÃ´ng.
    
    Args:
        text: VÄƒn báº£n Ä‘Ã£ dá»‹ch
        input_text: VÄƒn báº£n gá»‘c Ä‘á»ƒ so sÃ¡nh kÃ­ch thÆ°á»›c
        
    Returns:
        True náº¿u báº£n dá»‹ch khÃ´ng Ä‘áº¡t yÃªu cáº§u, False náº¿u Ä‘áº¡t yÃªu cáº§u.
    """
    if text is None or text.strip() == "":
        # Chunk dá»‹ch ra rá»—ng hoáº·c chá»‰ tráº¯ng => coi lÃ  bad translation
        return True

    # CÃ¡c tá»« khÃ³a chá»‰ bÃ¡o báº£n dá»‹ch khÃ´ng Ä‘áº¡t yÃªu cáº§u
    bad_keywords = [
        "tÃ´i khÃ´ng thá»ƒ dá»‹ch",
        "khÃ´ng thá»ƒ dá»‹ch",
        "xin lá»—i, tÃ´i khÃ´ng",
        "tÃ´i xin lá»—i",
        "ná»™i dung bá»‹ cháº·n",
        "as an ai",
        "as a language model",
        "i am unable",
        "i cannot",
        "i'm sorry",
        "[bá»‹ cáº¯t - cáº§n chunk nhá» hÆ¡n]",
        "[cÃ³ thá»ƒ bá»‹ thiáº¿u]"
    ]

    text_lower = text.lower()
    for keyword in bad_keywords:
        if keyword in text_lower:
            return True

    text_stripped = text.strip()
    
    # KÃ½ tá»± cuá»‘i há»£p lá»‡ (response hoÃ n chá»‰nh) - define globally
    valid_ending_chars = '.!?ã€‚ï¼ï¼Ÿ"ã€ã€)ï¼‰â€¦â€”'
    
    # Kiá»ƒm tra response cÃ³ hoÃ n chá»‰nh khÃ´ng dá»±a trÃªn kÃ½ tá»± cuá»‘i
    last_char = text_stripped[-1] if text_stripped else ''
    
    if len(text_stripped) > 20:  # Chá»‰ check vá»›i text Ä‘á»§ dÃ i
        # KÃ½ tá»± cuá»‘i khÃ´ng há»£p lá»‡ (response chÆ°a hoÃ n chá»‰nh)
        invalid_ending_chars = ' \t\n'  # space, tab, newline
        
        # Náº¿u káº¿t thÃºc báº±ng kÃ½ tá»± khÃ´ng há»£p lá»‡ -> response chÆ°a hoÃ n chá»‰nh
        if last_char in invalid_ending_chars:
            print(f"âš ï¸ Response chÆ°a hoÃ n chá»‰nh: káº¿t thÃºc báº±ng kÃ½ tá»± tráº¯ng '{repr(last_char)}'")
            return True
            
    # User request: Náº¿u response dÃ i tá»« 80-100% so vá»›i gá»‘c, bá» qua kiá»ƒm tra kÃ½ tá»± cuá»‘i
    if input_text:
        input_length = len(input_text.strip())
        output_length = len(text_stripped)
        ratio = output_length / input_length if input_length > 0 else 0
        if 0.8 < ratio < 1.0:
            print(f"âœ… Response cÃ³ Ä‘á»™ dÃ i phÃ¹ há»£p ({ratio:.1%}), bá» qua kiá»ƒm tra kÃ½ tá»± cuá»‘i.")
            return False # Coi lÃ  hoÃ n thÃ nh
            
    # Kiá»ƒm tra trÆ°á»ng há»£p ngoáº¡i lá»‡: tiÃªu Ä‘á» chÆ°Æ¡ng vÃ  ná»™i dung chÆ°Æ¡ng
    text_lower = text_stripped.lower()
    is_chapter_title = False
    is_chapter_content = False
    
    # CÃ¡c pattern tiÃªu Ä‘á» chÆ°Æ¡ng (thÆ°á»ng á»Ÿ Ä‘áº§u dÃ²ng)
    chapter_patterns = [
        r'^chÆ°Æ¡ng\s+\d+',          # "chÆ°Æ¡ng 1", "chÆ°Æ¡ng 23"
        r'^chÆ°Æ¡ng\s+[ivxlc]+',     # "chÆ°Æ¡ng i", "chÆ°Æ¡ng iv"  
        r'^chapter\s+\d+',         # "chapter 1", "chapter 23"
        r'^ç¬¬\d+ç« ',                # "ç¬¬1ç« ", "ç¬¬23ç« "
        r'^pháº§n\s+\d+',            # "pháº§n 1", "pháº§n 2"
        r'^táº­p\s+\d+',             # "táº­p 1", "táº­p 2"
    ]
    
    # Kiá»ƒm tra xem cÃ³ pháº£i tiÃªu Ä‘á» chÆ°Æ¡ng thuáº§n tÃºy khÃ´ng (ngáº¯n, chá»‰ cÃ³ tiÃªu Ä‘á»)
    for pattern in chapter_patterns:
        if re.search(pattern, text_lower) and len(text_stripped) < 200:
            is_chapter_title = True
            break
    
    # Náº¿u khÃ´ng pháº£i tiÃªu Ä‘á» chÆ°Æ¡ng thuáº§n tÃºy, kiá»ƒm tra cÃ³ pháº£i ná»™i dung chá»©a chÆ°Æ¡ng khÃ´ng
    if not is_chapter_title:
        chapter_keywords = ['chÆ°Æ¡ng', 'chapter', 'ç¬¬', 'pháº§n', 'táº­p']
        for keyword in chapter_keywords:
            if keyword in text_lower:
                is_chapter_content = True
                break
    
    # Xá»­ lÃ½ theo loáº¡i ná»™i dung
    if is_chapter_title:
        # TiÃªu Ä‘á» chÆ°Æ¡ng thuáº§n tÃºy (ngáº¯n) - cÃ³ thá»ƒ káº¿t thÃºc báº±ng chá»¯ cÃ¡i/sá»‘
        valid_chapter_endings = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789:-â€“â€”')
        if last_char in valid_chapter_endings or last_char in valid_ending_chars:
            print(f"âœ… PhÃ¡t hiá»‡n tiÃªu Ä‘á» chÆ°Æ¡ng, cho phÃ©p káº¿t thÃºc báº±ng '{last_char}'")
            # TiÃªu Ä‘á» chÆ°Æ¡ng khÃ´ng cáº§n kiá»ƒm tra strict vá» kÃ½ tá»± cuá»‘i
            pass  
        else:
            print(f"âš ï¸ TiÃªu Ä‘á» chÆ°Æ¡ng nhÆ°ng káº¿t thÃºc báº¥t thÆ°á»ng: '{last_char}'")
            return True
    elif is_chapter_content:
        # Ná»™i dung cÃ³ chá»©a chÆ°Æ¡ng (dÃ i) - Ã¡p dá»¥ng rule thÃ´ng thÆ°á»ng nhÆ°ng linh hoáº¡t hÆ¡n
        if last_char in valid_ending_chars:
            print(f"âœ… Ná»™i dung chÆ°Æ¡ng káº¿t thÃºc há»£p lá»‡ báº±ng '{last_char}'")
            # Dáº¥u cÃ¢u há»£p lá»‡, khÃ´ng coi lÃ  bad
            pass
        elif last_char.isalpha():
            print(f"âš ï¸ Ná»™i dung chÆ°Æ¡ng cÃ³ thá»ƒ chÆ°a hoÃ n chá»‰nh: káº¿t thÃºc báº±ng chá»¯ cÃ¡i '{last_char}'")
            return True
        elif last_char.isdigit():
            print(f"â„¹ï¸ Ná»™i dung chÆ°Æ¡ng káº¿t thÃºc báº±ng sá»‘ '{last_char}' - cÃ³ thá»ƒ há»£p lá»‡")
            # Sá»‘ cÃ³ thá»ƒ há»£p lá»‡ trong ná»™i dung chÆ°Æ¡ng, khÃ´ng coi lÃ  bad
            pass
        else:
            print(f"âš ï¸ Ná»™i dung chÆ°Æ¡ng káº¿t thÃºc báº¥t thÆ°á»ng: '{last_char}'")
            return True
    else:
        # Ná»™i dung thÃ´ng thÆ°á»ng - Ã¡p dá»¥ng rule nghiÃªm ngáº·t
        if last_char.isalpha():
            print(f"âš ï¸ Response cÃ³ thá»ƒ chÆ°a hoÃ n chá»‰nh: káº¿t thÃºc báº±ng chá»¯ cÃ¡i '{last_char}'")
            return True
        
    # Náº¿u káº¿t thÃºc báº±ng dáº¥u cÃ¢u há»£p lá»‡ -> response cÃ³ thá»ƒ hoÃ n chá»‰nh
    if last_char in valid_ending_chars:
        # NhÆ°ng váº«n cáº§n kiá»ƒm tra kÃ­ch thÆ°á»›c náº¿u cÃ³ input_text
        pass
    
    # Kiá»ƒm tra kÃ­ch thÆ°á»›c output so vá»›i input (50-60% threshold)
    if input_text and len(input_text.strip()) > 50:  # Chá»‰ check vá»›i input Ä‘á»§ dÃ i
        input_length = len(input_text.strip())
        output_length = len(text_stripped)
        
        # TÃ­nh tá»· lá»‡ output/input
        ratio = output_length / input_length if input_length > 0 else 0
        
        # Sá»­ dá»¥ng cá» is_chapter_content hoáº·c is_chapter_title Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh á»Ÿ trÃªn
        # Náº¿u chÆ°a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh, kiá»ƒm tra láº¡i
        if not (is_chapter_content or is_chapter_title):
            text_lower = text_stripped.lower()
            input_lower = input_text.lower()
            chapter_keywords = ['chÆ°Æ¡ng', 'chapter', 'ç¬¬', 'pháº§n', 'táº­p']
            for keyword in chapter_keywords:
                if keyword in text_lower or keyword in input_lower:
                    is_chapter_content = True
                    break
        
        # Náº¿u lÃ  ná»™i dung cÃ³ chÆ°Æ¡ng, Ã¡p dá»¥ng threshold linh hoáº¡t hÆ¡n
        if is_chapter_content or is_chapter_title:
            # TiÃªu Ä‘á» chÆ°Æ¡ng thÆ°á»ng ngáº¯n hÆ¡n, threshold tháº¥p hÆ¡n (30% thay vÃ¬ 50%)
            min_ratio = 0.3
            warning_ratio = 0.4
            
            if ratio < min_ratio:
                print(f"âš ï¸ Output quÃ¡ ngáº¯n so vá»›i input (chÆ°Æ¡ng): {ratio:.2%} (Input: {input_length} chars, Output: {output_length} chars)")
                return True
            elif ratio < warning_ratio:
                print(f"â„¹ï¸ Output hÆ¡i ngáº¯n nhÆ°ng cÃ³ thá»ƒ lÃ  tiÃªu Ä‘á» chÆ°Æ¡ng: {ratio:.2%} (Input: {input_length} chars, Output: {output_length} chars)")
                # Äá»‘i vá»›i tiÃªu Ä‘á» chÆ°Æ¡ng, chá»‰ coi lÃ  bad náº¿u káº¿t thÃºc ráº¥t báº¥t thÆ°á»ng
                if len(text_stripped) > 20:
                    last_char = text_stripped[-1]
                    if last_char in ' \t\n':  # Chá»‰ coi lÃ  bad náº¿u káº¿t thÃºc báº±ng whitespace
                        return True
        else:
            # Ná»™i dung thÃ´ng thÆ°á»ng, Ã¡p dá»¥ng threshold chuáº©n
            if ratio < 0.5:
                print(f"âš ï¸ Output quÃ¡ ngáº¯n so vá»›i input: {ratio:.2%} (Input: {input_length} chars, Output: {output_length} chars)")
                return True
            elif ratio < 0.6:
                print(f"âš ï¸ Output hÆ¡i ngáº¯n so vá»›i input: {ratio:.2%} (Input: {input_length} chars, Output: {output_length} chars)")
                # Chá»‰ coi lÃ  bad náº¿u káº¿t thÃºc khÃ´ng há»£p lá»‡
                if len(text_stripped) > 20:
                    last_char = text_stripped[-1]
                    if last_char.isalpha() or last_char in ' \t\n':
                        return True
    
    return False

def translate_chunk(model, chunk_lines, system_instruction, context="modern"):
    """
    Dá»‹ch má»™t chunk gá»“m nhiá»u dÃ²ng vÄƒn báº£n.
    chunk_lines: danh sÃ¡ch cÃ¡c dÃ²ng vÄƒn báº£n
    context: "modern" (hiá»‡n Ä‘áº¡i) hoáº·c "ancient" (cá»• Ä‘áº¡i)
    system_instruction: Chá»‰ dáº«n há»‡ thá»‘ng Ä‘áº§y Ä‘á»§ tá»« GUI
    Tráº£ vá» (translated_text, is_safety_blocked_flag, is_bad_translation_flag).
    """
    # Gom cÃ¡c dÃ²ng thÃ nh má»™t chuá»—i lá»›n Ä‘á»ƒ gá»­i Ä‘i
    full_text_to_translate = "\n".join(chunk_lines)
    
    # Bá» qua cÃ¡c chunk chá»‰ chá»©a cÃ¡c dÃ²ng trá»‘ng hoáº·c chá»‰ tráº¯ng
    if not full_text_to_translate.strip():
        return ("", False, False) # Tráº£ vá» chuá»—i rá»—ng, khÃ´ng bá»‹ cháº·n, khÃ´ng bad translation

    try:
        # Sá»­ dá»¥ng system_instruction Ä‘Æ°á»£c truyá»n vÃ o vÃ  thÃªm vÄƒn báº£n cáº§n dá»‹ch
        # Äiá»u nÃ y Ä‘áº£m báº£o prompt tá»« GUI Ä‘Æ°á»£c sá»­ dá»¥ng
        prompt = f"{system_instruction}\n\n{full_text_to_translate}"
        
        response = model.generate_content(
            contents=[{
                "role": "user",
                "parts": [prompt],
            }],
            generation_config={
                "response_mime_type": "text/plain",
                # CÃ³ thá»ƒ thÃªm cÃ¡c tham sá»‘ khÃ¡c náº¿u cáº§n
                # "temperature": 0.5,
                # "top_p": 0.95,
                # "top_k": 64,
                # "max_output_tokens": 8192,
            },
        )

        # 1. Kiá»ƒm tra xem prompt (Ä‘áº§u vÃ o) cÃ³ bá»‹ cháº·n khÃ´ng
        if response.prompt_feedback and response.prompt_feedback.safety_ratings:
            blocked_categories = [
                rating.category.name for rating in response.prompt_feedback.safety_ratings
                if rating.blocked
            ]
            if blocked_categories:
                return (f"[Ná»˜I DUNG Gá»C Bá»Š CHáº¶N Bá»I Bá»˜ Lá»ŒC AN TOÃ€N - PROMPT: {', '.join(blocked_categories)}]", True, False)

        # 2. Kiá»ƒm tra xem cÃ³ báº¥t ká»³ á»©ng cá»­ viÃªn nÃ o Ä‘Æ°á»£c táº¡o ra khÃ´ng
        if not response.candidates:
            return ("[Ná»˜I Dá»ŠCH Bá»Š CHáº¶N HOÃ€N TOÃ€N Bá»I Bá»˜ Lá»ŒC AN TOÃ€N - KHÃ”NG CÃ“ á»¨NG Cá»¬ VIÃŠN]", True, False)

        # 3. Kiá»ƒm tra lÃ½ do káº¿t thÃºc cá»§a á»©ng cá»­ viÃªn Ä‘áº§u tiÃªn (náº¿u cÃ³)
        first_candidate = response.candidates[0]
        if first_candidate.finish_reason == 'SAFETY':
            blocked_categories = [
                rating.category.name for rating in first_candidate.safety_ratings
                if rating.blocked
            ]
            return (f"[Ná»˜I Dá»ŠCH Bá»Š CHáº¶N Bá»I Bá»˜ Lá»ŒC AN TOÃ€N - OUTPUT: {', '.join(blocked_categories)}]", True, False)
        
        # 4. Kiá»ƒm tra náº¿u response bá»‹ cáº¯t do vÆ°á»£t quÃ¡ max_tokens
        finish_reason_name = str(first_candidate.finish_reason)
        if 'MAX_TOKENS' in finish_reason_name or finish_reason_name == 'LENGTH':
            print(f"âš ï¸ Cáº£nh bÃ¡o Google AI: Response bá»‹ cáº¯t (finish_reason={finish_reason_name})")
            translated_text = response.text
            # ÄÃ¡nh dáº¥u lÃ  bad translation Ä‘á»ƒ trigger re-chunk logic
            return (translated_text + " [Bá»Š Cáº®T - Cáº¦N CHUNK NHá» HÆ N]", False, True)

        # Náº¿u khÃ´ng bá»‹ cháº·n, tráº£ vá» vÄƒn báº£n dá»‹ch
        translated_text = response.text
        is_bad = is_bad_translation(translated_text, full_text_to_translate)
        
        # ğŸ› DEBUG: LÆ°u response ngay láº­p tá»©c (sáº½ Ä‘Æ°á»£c gá»i tá»« process_chunk vá»›i metadata Ä‘áº§y Ä‘á»§)
        # Note: chunk_index sáº½ Ä‘Æ°á»£c truyá»n tá»« process_chunk
        
        return (translated_text, False, is_bad)

    except Exception as e:
        # Báº¯t cÃ¡c lá»—i khÃ¡c (vÃ­ dá»¥: lá»—i máº¡ng, lá»—i API)
        error_message = str(e)
        
        # Kiá»ƒm tra lá»—i quota exceeded
        if check_quota_error(error_message):
            set_quota_exceeded()
            return (f"[API Háº¾T QUOTA]", False, True)
        
        return (f"[Lá»–I API KHI Dá»ŠCH CHUNK: {e}]", False, True)

def get_progress(progress_file_path):
    """Äá»c tiáº¿n Ä‘á»™ dá»‹ch tá»« file (sá»‘ chunk Ä‘Ã£ hoÃ n thÃ nh)."""
    if os.path.exists(progress_file_path):
        try:
            with open(progress_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # LÆ°u sá»‘ chunk Ä‘Ã£ hoÃ n thÃ nh
                return data.get('completed_chunks', 0)
        except json.JSONDecodeError:
            print(f"Cáº£nh bÃ¡o: File tiáº¿n Ä‘á»™ '{progress_file_path}' bá»‹ há»ng hoáº·c khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng JSON. Báº¯t Ä‘áº§u tá»« Ä‘áº§u.")
            return 0
    return 0

def save_progress(progress_file_path, completed_chunks):
    """LÆ°u tiáº¿n Ä‘á»™ dá»‹ch (sá»‘ chunk Ä‘Ã£ hoÃ n thÃ nh) vÃ o file."""
    try:
        with open(progress_file_path, 'w', encoding='utf-8') as f:
            json.dump({
                'completed_chunks': completed_chunks
            }, f)
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi lÆ°u file tiáº¿n Ä‘á»™: {e}")

def save_progress_with_line_info(progress_file_path, completed_chunks, current_chunk_info=None, error_info=None):
    """LÆ°u tiáº¿n Ä‘á»™ dá»‹ch vá»›i thÃ´ng tin line range vÃ  error details"""
    try:
        progress_data = {
            'completed_chunks': completed_chunks,
            'timestamp': time.time()
        }
        
        # ThÃªm thÃ´ng tin chunk hiá»‡n táº¡i náº¿u cÃ³
        if current_chunk_info:
            progress_data['current_chunk'] = current_chunk_info
        
        # ThÃªm thÃ´ng tin lá»—i náº¿u cÃ³
        if error_info:
            progress_data['last_error'] = error_info
        
        with open(progress_file_path, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, indent=2, ensure_ascii=False)
            
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi lÆ°u file tiáº¿n Ä‘á»™: {e}")

def load_progress_with_info(progress_file_path):
    """Táº£i tiáº¿n Ä‘á»™ vá»›i thÃ´ng tin chi tiáº¿t"""
    if os.path.exists(progress_file_path):
        try:
            with open(progress_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data
        except json.JSONDecodeError:
            print(f"Cáº£nh bÃ¡o: File tiáº¿n Ä‘á»™ '{progress_file_path}' bá»‹ há»ng. Báº¯t Ä‘áº§u tá»« Ä‘áº§u.")
            return {'completed_chunks': 0}
    return {'completed_chunks': 0}

def split_large_chunk(chunk_lines, max_lines=50):
    """
    Chia má»™t chunk lá»›n thÃ nh cÃ¡c chunks nhá» hÆ¡n khi gáº·p lá»—i context length exceeded
    """
    if len(chunk_lines) <= max_lines:
        return [chunk_lines]
    
    sub_chunks = []
    for i in range(0, len(chunk_lines), max_lines):
        sub_chunk = chunk_lines[i:i + max_lines]
        sub_chunks.append(sub_chunk)
    
    return sub_chunks

def translate_sub_chunk_recursive(model, sub_chunk, system_instruction, context, chunk_index, sub_index, 
                                   level=1, max_level=3, use_google_ai=True, use_openrouter=False, 
                                   api_key=None, model_name=None, openrouter_translate_chunk=None, 
                                   key_rotator=None, tried_keys=None):
    """
    Dá»‹ch sub-chunk vá»›i kháº£ nÄƒng chia nhá» recursive Ä‘áº¿n 3 cáº¥p Ä‘á»™.
    Náº¿u chia 3 levels váº«n tháº¥t báº¡i, thá»­ retry vá»›i API key khÃ¡c (Google AI only).
    
    Args:
        level: Cáº¥p Ä‘á»™ hiá»‡n táº¡i (1, 2, hoáº·c 3)
        max_level: Cáº¥p Ä‘á»™ tá»‘i Ä‘a (default 3)
        key_rotator: KeyRotator object Ä‘á»ƒ láº¥y key khÃ¡c khi cáº§n retry
        tried_keys: Set cÃ¡c keys Ä‘Ã£ thá»­ Ä‘á»ƒ trÃ¡nh retry láº·p láº¡i
        
    Returns:
        (translated_text, success_flag)
    """
    level_prefix = "   " * level  # Indent theo level
    
    # Initialize tried_keys tracking
    if tried_keys is None:
        tried_keys = set()
    
    if level > max_level:
        print(f"{level_prefix}âš ï¸ ÄÃ£ Ä‘áº¡t cáº¥p Ä‘á»™ tá»‘i Ä‘a ({max_level}), lÆ°u káº¿t quáº£ hiá»‡n táº¡i")
        return (f"[Cáº¤P Äá»˜ Tá»I ÄA - KHÃ”NG THá»‚ CHIA NHá» HÆ N]", False)
    
    # Kiá»ƒm tra chunk quÃ¡ nhá»
    min_lines_per_level = [10, 5, 3]  # Level 1: 10, Level 2: 5, Level 3: 3
    min_lines = min_lines_per_level[min(level - 1, len(min_lines_per_level) - 1)]
    
    if len(sub_chunk) < min_lines:
        print(f"{level_prefix}âš ï¸ Sub-chunk quÃ¡ nhá» ({len(sub_chunk)} dÃ²ng), khÃ´ng thá»ƒ chia thÃªm")
        return (f"[QUÃ NHá» - {len(sub_chunk)} DÃ’NG]", False)
    
    try:
        print(f"{level_prefix}ğŸ”„ Level {level} - Äang dá»‹ch sub-chunk {sub_index} ({len(sub_chunk)} dÃ²ng)...")
        
        # Thá»­ dá»‹ch sub-chunk
        if use_google_ai:
            translated_sub, safety_sub, is_bad_sub = translate_chunk(model, sub_chunk, system_instruction, context)
        elif use_openrouter:
            translated_sub, safety_sub, is_bad_sub = openrouter_translate_chunk(api_key, model_name, system_instruction, sub_chunk, context)
        else:
            return (f"[PROVIDER ERROR]", False)
        
        # Xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p response
        if safety_sub:
            print(f"{level_prefix}âš ï¸ Level {level} - Bá»‹ safety block, váº«n lÆ°u káº¿t quáº£")
            return (translated_sub + f" [SAFETY-L{level}]", True)  # True vÃ¬ váº«n cÃ³ káº¿t quáº£
        
        # Kiá»ƒm tra náº¿u bá»‹ cáº¯t
        if "[Bá»Š Cáº®T - Cáº¦N CHUNK NHá» HÆ N]" in translated_sub:
            print(f"{level_prefix}ğŸ”„ Level {level} - Sub-chunk {sub_index} bá»‹ cáº¯t, chia nhá» xuá»‘ng level {level + 1}...")
            result, success = split_and_translate_recursive(model, sub_chunk, system_instruction, context, 
                                                chunk_index, sub_index, level + 1, max_level,
                                                use_google_ai, use_openrouter, api_key, model_name, 
                                                openrouter_translate_chunk, key_rotator, tried_keys)
            if success:
                print(f"{level_prefix}âœ… Level {level} - Sub-chunk {sub_index} Ä‘Ã£ xá»­ lÃ½ thÃ nh cÃ´ng qua recursive splitting")
            return (result, success)
        
        if not is_bad_sub:
            print(f"{level_prefix}âœ… Level {level} - Sub-chunk {sub_index} thÃ nh cÃ´ng")
            return (translated_sub, True)
        else:
            # Bad translation - retry 1 láº§n rá»“i chia nhá»
            print(f"{level_prefix}âš ï¸ Level {level} - Bad translation, retry 1 láº§n...")
            time.sleep(1)
            
            if use_google_ai:
                translated_retry, safety_retry, is_bad_retry = translate_chunk(model, sub_chunk, system_instruction, context)
            elif use_openrouter:
                translated_retry, safety_retry, is_bad_retry = openrouter_translate_chunk(api_key, model_name, system_instruction, sub_chunk, context)
            
            if not is_bad_retry and not safety_retry:
                print(f"{level_prefix}âœ… Level {level} - Retry thÃ nh cÃ´ng")
                return (translated_retry, True)
            else:
                # Váº«n bad sau retry - chia nhá»
                print(f"{level_prefix}ğŸ”„ Level {level} - Váº«n bad sau retry, chia nhá» xuá»‘ng level {level + 1}...")
                return split_and_translate_recursive(model, sub_chunk, system_instruction, context,
                                                    chunk_index, sub_index, level + 1, max_level,
                                                    use_google_ai, use_openrouter, api_key, model_name,
                                                    openrouter_translate_chunk, key_rotator, tried_keys)
    
    except Exception as e:
        error_msg = str(e)
        print(f"{level_prefix}âŒ Level {level} - Lá»—i: {error_msg[:100]}")
        
        # Kiá»ƒm tra cÃ¡c lá»—i cÃ³ thá»ƒ chia nhá»
        if ("context" in error_msg.lower() and "length" in error_msg.lower()) or \
           ("too long" in error_msg.lower()) or \
           ("maximum" in error_msg.lower()):
            print(f"{level_prefix}ğŸ”„ Level {level} - Context/length error, chia nhá» xuá»‘ng level {level + 1}...")
            return split_and_translate_recursive(model, sub_chunk, system_instruction, context,
                                                chunk_index, sub_index, level + 1, max_level,
                                                use_google_ai, use_openrouter, api_key, model_name,
                                                openrouter_translate_chunk, key_rotator, tried_keys)
        else:
            # Lá»—i khÃ¡c - khÃ´ng thá»ƒ xá»­ lÃ½
            return (f"[Lá»–I L{level}: {error_msg[:100]}]", False)

def split_and_translate_recursive(model, chunk_lines, system_instruction, context, chunk_index, 
                                   parent_index, level, max_level, use_google_ai, use_openrouter, 
                                   api_key, model_name, openrouter_translate_chunk, 
                                   key_rotator=None, tried_keys=None):
    """
    Chia chunk vÃ  dá»‹ch recursive tá»«ng pháº§n.
    Náº¿u tháº¥t báº¡i á»Ÿ level tá»‘i Ä‘a, thá»­ retry vá»›i API key khÃ¡c (Google AI only).
    
    Returns:
        (combined_text, success_flag)
    """
    level_prefix = "   " * level
    
    # Initialize tried_keys tracking
    if tried_keys is None:
        tried_keys = set()
    
    # Chia chunk thÃ nh 2 pháº§n
    mid_point = len(chunk_lines) // 2
    if mid_point < 3:  # QuÃ¡ nhá» Ä‘á»ƒ chia
        print(f"{level_prefix}âš ï¸ Chunk quÃ¡ nhá» ({len(chunk_lines)} dÃ²ng), khÃ´ng thá»ƒ chia thÃªm")
        return (f"[QUÃ NHá» - {len(chunk_lines)} DÃ’NG]", False)
    
    first_half = chunk_lines[:mid_point]
    second_half = chunk_lines[mid_point:]
    
    print(f"{level_prefix}ğŸ“¦ Chia thÃ nh 2 pháº§n: {len(first_half)} + {len(second_half)} dÃ²ng")
    
    # Dá»‹ch pháº§n 1
    first_result, first_success = translate_sub_chunk_recursive(
        model, first_half, system_instruction, context, chunk_index, f"{parent_index}.1",
        level, max_level, use_google_ai, use_openrouter, api_key, model_name, openrouter_translate_chunk,
        key_rotator, tried_keys
    )
    
    # Dá»‹ch pháº§n 2
    second_result, second_success = translate_sub_chunk_recursive(
        model, second_half, system_instruction, context, chunk_index, f"{parent_index}.2",
        level, max_level, use_google_ai, use_openrouter, api_key, model_name, openrouter_translate_chunk,
        key_rotator, tried_keys
    )
    
    # Káº¿t há»£p káº¿t quáº£
    combined = first_result
    if not first_result.endswith('\n'):
        combined += '\n'
    combined += second_result
    
    success = first_success and second_success
    
    # Náº¿u tháº¥t báº¡i á»Ÿ level tá»‘i Ä‘a vÃ  cÃ³ key_rotator (Google AI), thá»­ vá»›i key khÃ¡c
    if not success and level == max_level and use_google_ai and key_rotator and key_rotator.is_multi_key:
        current_key_hash = _get_key_hash(api_key) if api_key else None
        
        # ÄÃ¡nh dáº¥u key hiá»‡n táº¡i Ä‘Ã£ thá»­
        if current_key_hash:
            tried_keys.add(current_key_hash)
        
        # Thá»­ láº¥y key khÃ¡c chÆ°a thá»­
        available_keys = [k for k in key_rotator.keys if _get_key_hash(k) not in tried_keys]
        
        if available_keys:
            new_key = available_keys[0]
            new_key_hash = _get_key_hash(new_key)
            tried_keys.add(new_key_hash)
            
            print(f"{level_prefix}ğŸ”„ Chia 3 levels váº«n tháº¥t báº¡i, thá»­ láº¡i vá»›i API key khÃ¡c (Key #{len(tried_keys)})...")
            
            # Táº¡o model má»›i vá»›i key má»›i
            import google.generativeai as genai
            genai.configure(api_key=new_key)
            new_model = genai.GenerativeModel(
                model_name=model_name,
                generation_config={
                    "temperature": 0.7,
                    "top_p": 0.95,
                    "top_k": 40,
                    "max_output_tokens": 8192,
                },
                safety_settings={
                    "HARM_CATEGORY_HARASSMENT": "BLOCK_NONE",
                    "HARM_CATEGORY_HATE_SPEECH": "BLOCK_NONE",
                    "HARM_CATEGORY_SEXUALLY_EXPLICIT": "BLOCK_NONE",
                    "HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_NONE",
                }
            )
            
            # Retry toÃ n bá»™ chunk vá»›i key má»›i tá»« level 1
            retry_result, retry_success = split_and_translate_recursive(
                new_model, chunk_lines, system_instruction, context, chunk_index,
                parent_index, 1, max_level, use_google_ai, use_openrouter,
                new_key, model_name, openrouter_translate_chunk,
                key_rotator, tried_keys
            )
            
            if retry_success:
                print(f"{level_prefix}âœ… Retry vá»›i key khÃ¡c THÃ€NH CÃ”NG!")
                return (retry_result, True)
            else:
                print(f"{level_prefix}âŒ Retry vá»›i key khÃ¡c váº«n tháº¥t báº¡i")
                # Tráº£ vá» káº¿t quáº£ ban Ä‘áº§u vá»›i marker
                return (combined + f"\n[ÄÃƒ THá»¬ {len(tried_keys)} KEYS - VáºªN THáº¤T Báº I]", False)
        else:
            print(f"{level_prefix}âš ï¸ ÄÃ£ thá»­ háº¿t {len(tried_keys)} keys, khÃ´ng cÃ²n key nÃ o khÃ¡c")
    
    return (combined, success)

def process_chunk(api_key, model_name, system_instruction, chunk_data, provider="OpenRouter", log_callback=None, key_rotator=None, context="modern", is_paid_key=False, adaptive_thread_manager=None, input_file=None, model_settings=None):
    """
    Xá»­ lÃ½ dá»‹ch má»™t chunk vá»›i retry logic, rate limiting vÃ  re-chunking.
    chunk_data: tuple (chunk_index, chunk_lines, chunk_start_line_index)
    Tráº£ vá»: (chunk_index, translated_text, lines_count, line_range)
    
    Args:
        key_rotator: KeyRotator instance náº¿u sá»­ dá»¥ng multiple keys (Google AI only)
        context: "modern" (hiá»‡n Ä‘áº¡i) hoáº·c "ancient" (cá»• Ä‘áº¡i) Ä‘á»ƒ xÃ¡c Ä‘á»‹nh danh xÆ°ng ngÆ°á»i ká»ƒ chuyá»‡n
        input_file: ÄÆ°á»ng dáº«n file input (dÃ¹ng cho debug logging)
        model_settings: Dict chá»©a cÃ¡c cÃ i Ä‘áº·t model (thinking_mode, thinking_budget, etc.)
    """
    chunk_index, chunk_lines, chunk_start_line_index = chunk_data
    
    # Extract model settings
    if model_settings is None:
        model_settings = {}
    
    thinking_mode = model_settings.get("thinking_mode", False)
    thinking_budget = model_settings.get("thinking_budget", 0)
    
    # TÃ­nh toÃ¡n line range cho chunk hiá»‡n táº¡i
    chunk_end_line_index = chunk_start_line_index + len(chunk_lines) - 1
    line_range = f"{chunk_start_line_index + 1}:{chunk_end_line_index + 1}"  # +1 vÃ¬ line numbers báº¯t Ä‘áº§u tá»« 1
    
    # Get current API key (from rotator if available)
    current_api_key = key_rotator.get_next_key() if key_rotator else api_key
    
    # Get ENHANCED rate limiter cho Google AI vá»›i specific key (None cho OpenRouter)
    rate_limiter = get_enhanced_rate_limiter(
        model_name, 
        provider, 
        current_api_key if provider == "Google AI" else None, 
        is_paid_key=is_paid_key,
        desired_rpm=model_settings.get("target_rpm") if provider == "Google AI" else None
    )
    
    # Estimate tokens for this chunk (for TPM tracking)
    chunk_text = "\n".join(chunk_lines)
    estimated_tokens = estimate_tokens(chunk_text) if rate_limiter else 0
    
    # Debug logging vá»›i detailed state
    if rate_limiter and provider == "Google AI":
        stats = rate_limiter.get_stats()
        rpm_usage = stats.get('rpm_usage', 0)
        rpm_max = stats.get('rpm_max', 0)
        rpm_utilization = stats.get('rpm_utilization', 0)
        tpm_usage = stats.get('tpm_usage', 0)
        tpm_max = stats.get('tpm_max', 0)
        
        # Show stats periodically or when high utilization
        if rpm_usage > 0 and (chunk_index % 20 == 0 or rpm_utilization > 0.8):
            print(f"â±ï¸ Chunk {chunk_index}: RPM {rpm_usage}/{rpm_max} ({rpm_utilization:.0%}), TPM {tpm_usage:,}/{tpm_max:,}, Est: {estimated_tokens} tokens")
            
            # Debug detailed state khi rate limit gáº§n full
            if rpm_utilization > 0.9:
                print(f"âš ï¸ WARNING: RPM usage at {rpm_utilization:.0%} - detailed debug:")
                rate_limiter.debug_state()
    
    # Kiá»ƒm tra flag dá»«ng vÃ  quota exceeded trÆ°á»›c khi báº¯t Ä‘áº§u
    if is_translation_stopped() or is_quota_exceeded():
        if is_quota_exceeded():
            error_text = format_error_chunk("API Háº¾T QUOTA", "API Ä‘Ã£ háº¿t quota, cáº§n náº¡p thÃªm credit hoáº·c Ä‘á»•i API key", chunk_lines, line_range)
            return (chunk_index, error_text, len(chunk_lines), line_range)
        else:
            error_text = format_error_chunk("Dá»ªNG Bá»I NGÆ¯á»œI DÃ™NG", "NgÆ°á»i dÃ¹ng Ä‘Ã£ dá»«ng quÃ¡ trÃ¬nh dá»‹ch", chunk_lines, line_range)
            return (chunk_index, error_text, len(chunk_lines), line_range)
    
    # Determine which API to use based on provider
    use_google_ai = (provider == "Google AI")
    use_openrouter = (provider == "OpenRouter")
    
    # Khá»Ÿi táº¡o biáº¿n openrouter_translate_chunk trÆ°á»›c (Ä‘á»ƒ trÃ¡nh lá»—i UnboundLocalError)
    openrouter_translate_chunk = None
    
    if use_google_ai:
        # Setup Google AI (vá»›i current API key tá»« rotator)
        try:
            import google.generativeai as genai
            genai.configure(api_key=current_api_key)
            
            # Build generation config vá»›i thinking mode support
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 8192,
            }
            
            # Add thinking config náº¿u enabled (chá»‰ cho Gemini 2.5+)
            if thinking_mode and thinking_budget > 0:
                generation_config["thinking_config"] = {
                    "thinking_budget": thinking_budget
                }
                print(f"ğŸ§  Chunk {chunk_index}: Thinking Mode enabled (budget: {thinking_budget} tokens)")
            
            model = genai.GenerativeModel(
                model_name=model_name,
                generation_config=generation_config,
                safety_settings={
                    "HARM_CATEGORY_HARASSMENT": "BLOCK_NONE",
                    "HARM_CATEGORY_HATE_SPEECH": "BLOCK_NONE",
                    "HARM_CATEGORY_SEXUALLY_EXPLICIT": "BLOCK_NONE",
                    "HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_NONE",
                }
            )
        except ImportError:
            error_text = format_error_chunk("IMPORT ERROR", "Google AI module khÃ´ng tÃ¬m tháº¥y. Vui lÃ²ng cÃ i Ä‘áº·t: pip install google-generativeai", chunk_lines, line_range)
            return (chunk_index, error_text, len(chunk_lines), line_range)
    
    if use_openrouter:
        # Import OpenRouter translate function - dÃ¹ng tÃªn táº¡m Ä‘á»ƒ trÃ¡nh UnboundLocalError
        try:
            from .open_router_translate import translate_chunk as _openrouter_func
            openrouter_translate_chunk = _openrouter_func
        except ImportError:
            try:
                from open_router_translate import translate_chunk as _openrouter_func
                openrouter_translate_chunk = _openrouter_func
            except ImportError:
                error_text = format_error_chunk("IMPORT ERROR", "OpenRouter module khÃ´ng tÃ¬m tháº¥y", chunk_lines, line_range)
                return (chunk_index, error_text, len(chunk_lines), line_range)
    
    # Thá»­ láº¡i vá»›i lá»—i báº£o máº­t
    safety_retries = 0
    is_safety_blocked = False  # Khá»Ÿi táº¡o biáº¿n
    while safety_retries < MAX_RETRIES_ON_SAFETY_BLOCK:
        # Kiá»ƒm tra flag dá»«ng vÃ  quota exceeded trong quÃ¡ trÃ¬nh retry
        if is_translation_stopped() or is_quota_exceeded():
            if is_quota_exceeded():
                error_text = format_error_chunk("API Háº¾T QUOTA", "API Ä‘Ã£ háº¿t quota trong quÃ¡ trÃ¬nh retry", chunk_lines, line_range)
                return (chunk_index, error_text, len(chunk_lines), line_range)
            else:
                error_text = format_error_chunk("Dá»ªNG Bá»I NGÆ¯á»œI DÃ™NG", "NgÆ°á»i dÃ¹ng Ä‘Ã£ dá»«ng quÃ¡ trÃ¬nh dá»‹ch trong retry", chunk_lines, line_range)
                return (chunk_index, error_text, len(chunk_lines), line_range)
            
        # Thá»­ láº¡i vá»›i báº£n dá»‹ch xáº¥u  
        bad_translation_retries = 0
        while bad_translation_retries < MAX_RETRIES_ON_BAD_TRANSLATION:
            # Kiá»ƒm tra flag dá»«ng vÃ  quota exceeded trong quÃ¡ trÃ¬nh retry
            if is_translation_stopped() or is_quota_exceeded():
                if is_quota_exceeded():
                    error_text = format_error_chunk("API Háº¾T QUOTA", "API Ä‘Ã£ háº¿t quota trong bad translation retry", chunk_lines, line_range)
                    return (chunk_index, error_text, len(chunk_lines), line_range)
                else:
                    error_text = format_error_chunk("Dá»ªNG Bá»I NGÆ¯á»œI DÃ™NG", "NgÆ°á»i dÃ¹ng Ä‘Ã£ dá»«ng trong bad translation retry", chunk_lines, line_range)
                    return (chunk_index, error_text, len(chunk_lines), line_range)
                
            try:
                # Retry logic for rate limit errors
                rate_limit_retry = 0
                while rate_limit_retry <= MAX_RETRIES_ON_RATE_LIMIT:
                    try:
                        # Rate limit cho Google AI - Multi-threading safe vá»›i TPM tracking
                        if rate_limiter and use_google_ai:
                            rate_limiter.acquire(estimated_tokens=estimated_tokens)  # Enhanced acquire vá»›i TPM
                        
                        if use_google_ai:
                            # Dá»‹ch vá»›i Google AI sá»­ dá»¥ng hÃ m translate_chunk vá»›i system_instruction Ä‘áº§y Ä‘á»§
                            translated_text, is_safety_blocked, is_bad = translate_chunk(model, chunk_lines, system_instruction, context)
                            
                            # ğŸ› DEBUG: LÆ°u response ngay láº­p tá»©c
                            key_hash = _get_key_hash(current_api_key) if current_api_key else "unknown"
                            if input_file:
                                save_debug_response(
                                    chunk_index=chunk_index,
                                    response_text=translated_text,
                                    chunk_lines=chunk_lines,
                                    input_file=input_file,
                                    provider=provider,
                                    model_name=model_name,
                                    key_hash=key_hash
                                )
                            
                            # BÃ¡o success cho adaptive throttling
                            if rate_limiter:
                                rate_limiter.on_success()
                            
                            # BÃ¡o success cho key rotator (ImprovedKeyRotator)
                            if key_rotator and hasattr(key_rotator, 'report_success'):
                                key_rotator.report_success(current_api_key)
                            
                            # BÃ¡o success cho adaptive thread manager
                            if adaptive_thread_manager:
                                adaptive_thread_manager.report_success()
                            
                            break  # Success, thoÃ¡t khá»i rate limit retry loop
                                
                        elif use_openrouter:
                            translated_text, is_safety_blocked, is_bad = openrouter_translate_chunk(api_key, model_name, system_instruction, chunk_lines, context)
                            
                            # ğŸ› DEBUG: LÆ°u response ngay láº­p tá»©c
                            key_hash = _get_key_hash(api_key) if api_key else "unknown"
                            if input_file:
                                save_debug_response(
                                    chunk_index=chunk_index,
                                    response_text=translated_text,
                                    chunk_lines=chunk_lines,
                                    input_file=input_file,
                                    provider=provider,
                                    model_name=model_name,
                                    key_hash=key_hash
                                )
                            
                            # BÃ¡o success cho adaptive thread manager
                            if adaptive_thread_manager:
                                adaptive_thread_manager.report_success()
                            
                            break  # Success, thoÃ¡t khá»i rate limit retry loop
                        else:
                            error_text = format_error_chunk("PROVIDER ERROR", f"Provider khÃ´ng Ä‘Æ°á»£c há»— trá»£: {provider}", chunk_lines, line_range)
                            return (chunk_index, error_text, len(chunk_lines), line_range)
                            
                    except Exception as rate_error:
                        error_msg = str(rate_error)
                        
                        # Kiá»ƒm tra náº¿u lÃ  rate limit error
                        if is_rate_limit_error(error_msg) and rate_limit_retry < MAX_RETRIES_ON_RATE_LIMIT:
                            rate_limit_retry += 1
                            print(f"ğŸ”„ Rate limit error á»Ÿ chunk {chunk_index}, retry {rate_limit_retry}/{MAX_RETRIES_ON_RATE_LIMIT}")
                            print(f"ğŸ“ Error detail: {error_msg[:200]}...")  # Log chi tiáº¿t lá»—i
                            
                            # BÃ¡o rate limit error cho adaptive throttling
                            if rate_limiter and use_google_ai:
                                rate_limiter.on_rate_limit_error()
                            
                            # BÃ¡o rate limit error cho key rotator (ImprovedKeyRotator)
                            if key_rotator and hasattr(key_rotator, 'report_error'):
                                key_rotator.report_error(current_api_key, is_rate_limit=True)
                            
                            # BÃ¡o rate limit cho adaptive thread manager
                            if adaptive_thread_manager:
                                adaptive_thread_manager.report_rate_limit()
                            
                            # Sá»­ dá»¥ng exponential backoff tá»‘t hÆ¡n vá»›i base delay cao hÆ¡n cho rate limit
                            exponential_backoff_sleep(rate_limit_retry - 1, base_delay=8.0, max_delay=300.0)
                            continue
                        else:
                            # KhÃ´ng pháº£i rate limit error hoáº·c háº¿t retry
                            # BÃ¡o lá»—i khÃ¡c cho adaptive thread manager
                            if adaptive_thread_manager:
                                adaptive_thread_manager.report_other_error()
                            raise  # Re-raise Ä‘á»ƒ xá»­ lÃ½ á»Ÿ catch block bÃªn ngoÃ i
                
                # Kiá»ƒm tra quota exceeded sau khi dá»‹ch
                if is_quota_exceeded():
                    error_text = format_error_chunk("API Háº¾T QUOTA", "API Ä‘Ã£ háº¿t quota sau khi dá»‹ch", chunk_lines, line_range)
                    return (chunk_index, error_text, len(chunk_lines), line_range)
                
                # Log successful request vá»›i key info Ä‘á»ƒ track quota usage
                if use_google_ai and current_api_key:
                    key_hash = _get_key_hash(current_api_key)
                    print(f"âœ… Chunk {chunk_index}: Key ***{key_hash} - Success")
                
                if is_safety_blocked:
                    break # ThoÃ¡t khá»i vÃ²ng láº·p bad translation, sáº½ retry safety
                    
                if not is_bad:
                    return (chunk_index, translated_text, len(chunk_lines), line_range) # ThÃ nh cÃ´ng
                    
                # Báº£n dá»‹ch xáº¥u, thá»­ láº¡i
                bad_translation_retries += 1
                
                # Kiá»ƒm tra náº¿u bá»‹ cáº¯t do max_tokens - chia nhá» ngay láº­p tá»©c vá»›i recursive 3 level
                if "[Bá»Š Cáº®T - Cáº¦N CHUNK NHá» HÆ N]" in translated_text and len(chunk_lines) > 3:
                    print(f"ğŸ”„ Chunk {chunk_index} bá»‹ cáº¯t (max_tokens), sá»­ dá»¥ng recursive splitting...")
                    
                    # Sá»­ dá»¥ng recursive splitting vá»›i key_rotator support
                    combined_result, success = split_and_translate_recursive(
                        model, chunk_lines, system_instruction, context, chunk_index, "cut",
                        level=1, max_level=3, use_google_ai=use_google_ai, use_openrouter=use_openrouter,
                        api_key=api_key, model_name=model_name, openrouter_translate_chunk=openrouter_translate_chunk,
                        key_rotator=key_rotator
                    )
                    
                    if success:
                        print(f"âœ… Chunk {chunk_index} Ä‘Ã£ Ä‘Æ°á»£c chia nhá» recursive vÃ  dá»‹ch thÃ nh cÃ´ng")
                    else:
                        print(f"âš ï¸ Chunk {chunk_index} Ä‘Ã£ chia nhá» recursive nhÆ°ng má»™t sá»‘ pháº§n tháº¥t báº¡i")
                    
                    return (chunk_index, combined_result, len(chunk_lines), line_range)
                
                if bad_translation_retries < MAX_RETRIES_ON_BAD_TRANSLATION:
                    print(f"âš ï¸ Chunk {chunk_index} - báº£n dá»‹ch xáº¥u láº§n {bad_translation_retries}, thá»­ láº¡i...")
                    time.sleep(RETRY_DELAY_SECONDS)
                else:
                    # Háº¿t láº§n thá»­ bad translation, thá»­ chia nhá» chunk vá»›i recursive 3 level
                    if len(chunk_lines) > 3:
                        print(f"ğŸ”„ Chunk {chunk_index} váº«n bad sau {MAX_RETRIES_ON_BAD_TRANSLATION} láº§n thá»­, sá»­ dá»¥ng recursive splitting...")
                        
                        # Sá»­ dá»¥ng recursive splitting vá»›i key_rotator support
                        combined_result, success = split_and_translate_recursive(
                            model, chunk_lines, system_instruction, context, chunk_index, "bad",
                            level=1, max_level=3, use_google_ai=use_google_ai, use_openrouter=use_openrouter,
                            api_key=api_key, model_name=model_name, openrouter_translate_chunk=openrouter_translate_chunk,
                            key_rotator=key_rotator
                        )
                        
                        if success:
                            print(f"âœ… Chunk {chunk_index} Ä‘Ã£ Ä‘Æ°á»£c chia nhá» recursive vÃ  dá»‹ch thÃ nh cÃ´ng")
                        else:
                            print(f"âš ï¸ Chunk {chunk_index} Ä‘Ã£ chia nhá» recursive nhÆ°ng má»™t sá»‘ pháº§n tháº¥t báº¡i")
                        
                        return (chunk_index, combined_result, len(chunk_lines), line_range)
                    else:
                        # Chunk Ä‘Ã£ nhá», khÃ´ng thá»ƒ chia thÃªm
                        print(f"ğŸ’¾ Chunk {chunk_index} - Ä‘Ã£ thá»­ {MAX_RETRIES_ON_BAD_TRANSLATION} láº§n vÃ  quÃ¡ nhá» Ä‘á»ƒ chia, lÆ°u káº¿t quáº£ hiá»‡n táº¡i")
                        return (chunk_index, translated_text + " [KHÃ”NG Cáº¢I THIá»†N ÄÆ¯á»¢C]", len(chunk_lines), line_range)
                    
            except Exception as e:
                error_msg = str(e)
                
                # Xá»­ lÃ½ lá»—i theo provider
                if use_google_ai:
                    # Google AI specific error handling
                    if check_quota_error(error_msg):
                        # Google AI quota exceeded
                        set_quota_exceeded()
                        
                        # BÃ¡o error cho key rotator
                        if key_rotator and hasattr(key_rotator, 'report_error'):
                            key_rotator.report_error(current_api_key, is_rate_limit=False)
                        
                        error_text = format_error_chunk("API Háº¾T QUOTA", f"Google AI háº¿t quota: {error_msg}", chunk_lines, line_range)
                        return (chunk_index, error_text, len(chunk_lines), line_range)
                    elif is_rate_limit_error(error_msg):
                        # Google AI rate limit - cÃ³ thá»ƒ retry
                        print(f"âš ï¸ Google AI rate limit táº¡i chunk {chunk_index}, sáº½ retry...")
                        
                        # BÃ¡o rate limit error cho key rotator
                        if key_rotator and hasattr(key_rotator, 'report_error'):
                            key_rotator.report_error(current_api_key, is_rate_limit=True)
                        
                        continue
                    elif "context_length" in error_msg.lower() or "too long" in error_msg.lower() or "maximum" in error_msg.lower():
                        # Context length error - chia nhá» chunk vá»›i recursive 3 level
                        if len(chunk_lines) > 3:
                            print(f"ğŸ”„ Chunk {chunk_index} quÃ¡ lá»›n cho Google AI (context_length), sá»­ dá»¥ng recursive splitting...")
                            
                            # Sá»­ dá»¥ng recursive splitting vá»›i key_rotator support
                            combined_result, success = split_and_translate_recursive(
                                model, chunk_lines, system_instruction, context, chunk_index, "ctx",
                                level=1, max_level=3, use_google_ai=use_google_ai, use_openrouter=use_openrouter,
                                api_key=api_key, model_name=model_name, openrouter_translate_chunk=openrouter_translate_chunk,
                                key_rotator=key_rotator
                            )
                            
                            if success:
                                print(f"âœ… Chunk {chunk_index} context_length Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng")
                            else:
                                print(f"âš ï¸ Chunk {chunk_index} context_length xá»­ lÃ½ nhÆ°ng cÃ³ má»™t sá»‘ pháº§n tháº¥t báº¡i")
                            
                            return (chunk_index, combined_result, len(chunk_lines), line_range)
                        else:
                            # Chunk quÃ¡ nhá» nhÆ°ng váº«n context_length error - lá»—i nghiÃªm trá»ng
                            error_text = format_error_chunk("CONTEXT LENGTH ERROR", f"Chunk quÃ¡ nhá» ({len(chunk_lines)} dÃ²ng) nhÆ°ng váº«n bá»‹ context_length: {error_msg}", chunk_lines, line_range)
                            return (chunk_index, error_text, len(chunk_lines), line_range)
                    else:
                        # Google AI generic error
                        error_text = format_error_chunk("GOOGLE AI ERROR", f"Lá»—i Google AI: {error_msg}", chunk_lines, line_range)
                        return (chunk_index, error_text, len(chunk_lines), line_range)
                
                elif use_openrouter:
                    # OpenRouter specific error handling (existing logic)
                    if check_openrouter_quota_error(error_msg):
                        # 402: Insufficient Credits - dá»«ng hoÃ n toÃ n
                        set_quota_exceeded()
                        error_text = format_error_chunk("API Háº¾T QUOTA", f"OpenRouter háº¿t credit (402): {error_msg}", chunk_lines, line_range)
                        return (chunk_index, error_text, len(chunk_lines), line_range)
                
                    elif check_openrouter_api_key_error(error_msg):
                        # 401: Invalid Credentials - dá»«ng hoÃ n toÃ n
                        error_text = format_error_chunk("API KEY ERROR", f"API key khÃ´ng há»£p lá»‡ (401): {error_msg}", chunk_lines, line_range)
                        return (chunk_index, error_text, len(chunk_lines), line_range)
                
                    elif check_openrouter_rate_limit_error(error_msg):
                        # 429: Rate Limit - cÃ³ thá»ƒ retry
                        print(f"âš ï¸ Rate limit (429) táº¡i chunk {chunk_index}, sáº½ retry...")
                        # Äá»ƒ tiáº¿p tá»¥c retry loop thay vÃ¬ return ngay
                        continue
                
                    elif check_openrouter_moderation_error(error_msg):
                        # 403: Moderation - content bá»‹ block
                        error_text = format_error_chunk("MODERATION ERROR", f"Ná»™i dung vi pháº¡m chÃ­nh sÃ¡ch (403): {error_msg}", chunk_lines, line_range)
                        return (chunk_index, error_text, len(chunk_lines), line_range)
                
                    elif check_openrouter_timeout_error(error_msg):
                        # 408: Timeout - cÃ³ thá»ƒ retry
                        print(f"âš ï¸ Timeout (408) táº¡i chunk {chunk_index}, sáº½ retry...")
                        continue
                
                    elif check_openrouter_service_error(error_msg):
                        # 502, 503: Service errors - cÃ³ thá»ƒ retry
                        print(f"âš ï¸ Service error (502/503) táº¡i chunk {chunk_index}, sáº½ retry...")
                        continue
                
                else:
                    # Generic error cho cáº£ hai provider
                    error_text = format_error_chunk("API ERROR", f"Lá»—i khi gá»i API: {error_msg}", chunk_lines, line_range)
                    return (chunk_index, error_text, len(chunk_lines), line_range)
        
        # Náº¿u bá»‹ cháº·n safety, thá»­ láº¡i
        if is_safety_blocked:
            safety_retries += 1
            if safety_retries < MAX_RETRIES_ON_SAFETY_BLOCK:
                time.sleep(RETRY_DELAY_SECONDS)
            else:
                # Háº¿t láº§n thá»­ safety, tráº£ vá» vá»›i ná»™i dung gá»‘c
                error_text = format_error_chunk("SAFETY BLOCKED", f"Ná»™i dung bá»‹ cháº·n bá»Ÿi bá»™ lá»c an toÃ n sau {MAX_RETRIES_ON_SAFETY_BLOCK} láº§n thá»­. Dá»‹ch thá»§ cÃ´ng: {translated_text}", chunk_lines, line_range)
                return (chunk_index, error_text, len(chunk_lines), line_range)
    
    # Fallback (khÃ´ng nÃªn Ä‘áº¿n Ä‘Ã¢y)
    error_text = format_error_chunk("UNKNOWN ERROR", "KhÃ´ng thá»ƒ dá»‹ch chunk sau táº¥t cáº£ cÃ¡c láº§n thá»­", chunk_lines, line_range)
    return (chunk_index, error_text, len(chunk_lines), line_range)

def retry_failed_chunks(input_file, output_file, progress_file_path, api_key, model_name, system_instruction, provider="OpenRouter", context="modern", is_paid_key=False):
    """
    Retry cÃ¡c chunks Ä‘Ã£ failed tá»« láº§n dá»‹ch trÆ°á»›c
    Tráº£ vá»: sá»‘ chunks Ä‘Ã£ retry thÃ nh cÃ´ng
    """
    if not os.path.exists(progress_file_path):
        return 0
    
    try:
        progress_data = load_progress_with_info(progress_file_path)
        if 'last_error' not in progress_data:
            return 0
        
        print("ğŸ”„ Äang retry cÃ¡c chunks bá»‹ lá»—i tá»« láº§n dá»‹ch trÆ°á»›c...")
        
        # Äá»c file Ä‘á»ƒ tÃ¬m cÃ¡c chunks cÃ³ error markers
        with open(output_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # TÃ¬m cÃ¡c error chunks
        error_pattern = r'\[\[Lá»–I.*?\]\]'
        error_matches = re.findall(error_pattern, content, re.DOTALL)
        
        if not error_matches:
            print("âœ… KhÃ´ng tÃ¬m tháº¥y chunks lá»—i cáº§n retry")
            return 0
        
        print(f"ğŸ“ TÃ¬m tháº¥y {len(error_matches)} chunks cáº§n retry")
        
        # TODO: Implement logic retry cÃ¡c chunks cá»¥ thá»ƒ
        # Hiá»‡n táº¡i chá»‰ return 0 Ä‘á»ƒ khÃ´ng break existing code
        return 0
        
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi retry failed chunks: {e}")
        return 0

def generate_output_filename(input_filepath):
    """
    Tá»± Ä‘á»™ng táº¡o tÃªn file output tá»« input file.
    VÃ­ dá»¥: "test.txt" -> "test_TranslateAI.txt"
    """
    # TÃ¡ch tÃªn file vÃ  pháº§n má»Ÿ rá»™ng
    file_dir = os.path.dirname(input_filepath)
    file_name = os.path.basename(input_filepath)
    name_without_ext, ext = os.path.splitext(file_name)
    
    # Táº¡o tÃªn file má»›i
    new_name = f"{name_without_ext}_TranslateAI{ext}"
    
    # Káº¿t há»£p vá»›i thÆ° má»¥c (náº¿u cÃ³)
    if file_dir:
        return os.path.join(file_dir, new_name)
    else:
        return new_name

def translate_file_optimized(input_file, output_file=None, api_key=None, model_name="gemini-2.0-flash", system_instruction=None, num_workers=None, chunk_size_lines=None, provider="OpenRouter", context="modern", is_paid_key=False, model_settings=None):
    """
    PhiÃªn báº£n dá»‹ch file vá»›i multi-threading chunks.
    
    Args:
        api_key: String (OpenRouter) hoáº·c List (Google AI multiple keys)
        context: "modern" (hiá»‡n Ä‘áº¡i - dÃ¹ng "tÃ´i") hoáº·c "ancient" (cá»• Ä‘áº¡i - dÃ¹ng "ta")
        is_paid_key: True náº¿u sá»­ dá»¥ng Google AI key tráº£ phÃ­
        model_settings: Dict chá»©a cÃ¡c cÃ i Ä‘áº·t model (thinking_mode, thinking_budget, temperature, etc.)
    """
    # Clear stop flag khi báº¯t Ä‘áº§u dá»‹ch má»›i
    clear_stop_translation()
    
    # Extract model settings náº¿u cÃ³
    if model_settings is None:
        model_settings = {}
    
    thinking_mode = model_settings.get("thinking_mode", False)
    thinking_budget = model_settings.get("thinking_budget", 0)
    
    # Log thinking mode status
    if thinking_mode and thinking_budget > 0:
        print(f"ğŸ§  Thinking Mode: Báº¬T (Budget: {thinking_budget} tokens)")
    else:
        print(f"ğŸ§  Thinking Mode: Táº®T")
    
    # Setup key rotator náº¿u cÃ³ multiple Google AI keys
    key_rotator = None
    if provider == "Google AI" and isinstance(api_key, list) and len(api_key) > 1:
        # âœ¨ Sá»­ dá»¥ng create_key_rotator Ä‘á»ƒ tá»± Ä‘á»™ng chá»n ImprovedKeyRotator hoáº·c KeyRotator
        # same_project=False vÃ¬ ta Ä‘Ã£ xÃ¡c nháº­n keys tá»« different projects
        key_rotator = create_key_rotator(api_key, same_project=False)
        # DÃ¹ng key Ä‘áº§u tiÃªn Ä‘á»ƒ validate
        validation_key = api_key[0]
    elif provider == "Google AI" and isinstance(api_key, list):
        # Chá»‰ cÃ³ 1 key trong list
        validation_key = api_key[0] if api_key else None
    else:
        validation_key = api_key
    
    # Validate vÃ  thiáº¿t láº­p parameters
    if num_workers is None:
        num_workers = NUM_WORKERS
    else:
        num_workers = validate_threads(num_workers)
    
    # ğŸ”§ Tá»° Äá»˜NG TÃNH TOÃN THREADS CHO GOOGLE AI + FREE KEYS + MULTI-KEY
    # User KHÃ”NG THá»‚ override khi dÃ¹ng nhiá»u free keys
    if provider == "Google AI" and not is_paid_key:
        is_multi_key = isinstance(api_key, list) and len(api_key) > 1
        
        if is_multi_key:
            num_keys = len(api_key)
            
            # XÃ¡c Ä‘á»‹nh RPM dá»±a trÃªn model
            # Updated October 2025: gemini-2.5-flash RPM reduced to 5
            if "2.0-flash-lite" in model_name.lower():
                base_rpm = 30
            elif "2.0-flash" in model_name.lower():
                base_rpm = 15
            elif "2.5-flash" in model_name.lower():
                base_rpm = 5  # âš ï¸ UPDATED: 5 RPM (not 10)
            elif "2.5-pro" in model_name.lower():
                base_rpm = 5
            elif "1.5-flash" in model_name.lower():
                base_rpm = 15
            elif "1.5-pro" in model_name.lower():
                base_rpm = 2
            elif "pro" in model_name.lower():
                base_rpm = 2
            else:
                base_rpm = 10  # Default safe
            
            # ğŸš¨ FORCE AUTO-CALCULATE: User input bá»‹ bá» qua!
            # Calculate safe RPM (same logic as rate limiter)
            safe_rpm = int(base_rpm * 0.85)
            if safe_rpm < 1:
                safe_rpm = 1
            
            # ğŸŒ GLOBAL RATE LIMIT (per-project, not per-key!)
            # Multiple keys from SAME project share the SAME rate limit
            # â†’ Threads = safe_rpm (NOT multiplied by num_keys!)
            optimal_threads = safe_rpm
            
            # Minimum: at least 1 thread per 2 keys (for rotation)
            min_threads = max(1, num_keys // 2)
            optimal_threads = max(optimal_threads, min_threads)
            
            # Maximum: never exceed safe_rpm (no benefit, causes rate limit)
            optimal_threads = min(optimal_threads, safe_rpm)
            
            print(f"ğŸ”§ Google AI Free Keys - AUTO MODE (User input Bá»Š Bá» QUA)")
            print(f"   ğŸ“Š Model: {model_name}")
            print(f"   ğŸ”‘ Keys: {num_keys} keys")
            print(f"   ğŸ“ˆ Base RPM: {base_rpm}, Safe RPM: {safe_rpm} (Ã—0.85)")
            print(f"   ğŸŒ GLOBAL LIMIT: Táº¥t cáº£ keys chia sáº» {safe_rpm} RPM")
            print(f"   ğŸ¯ Auto-calculated threads: {optimal_threads}")
            print(f"   ğŸ’¡ Formula: safe_rpm = {safe_rpm} (KHÃ”NG nhÃ¢n vá»›i sá»‘ keys!)")
            print(f"   âš ï¸  Multiple keys CHá»ˆ Ä‘á»ƒ rotate/failover, KHÃ”NG tÄƒng throughput")
            
            if num_workers != optimal_threads:
                print(f"   âš ï¸  User input ({num_workers}) â†’ OVERRIDDEN â†’ {optimal_threads} threads")
            else:
                print(f"   âœ… Threads Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»‘i Æ°u")
            
            # FORCE override user input
            num_workers = optimal_threads
            
        else:
            # Single free key: User cÃ³ thá»ƒ tá»± set, nhÆ°ng warning náº¿u quÃ¡ cao
            print(f"Google AI (1 Free Key):")
            print(f"   âœ… Sá»­ dá»¥ng {num_workers} threads theo cÃ i Ä‘áº·t cá»§a báº¡n")
            print(f"   âš ï¸  LÆ°u Ã½: Free key cÃ³ giá»›i háº¡n RPM tháº¥p, trÃ¡nh set threads quÃ¡ cao!")
            
    elif provider == "Google AI" and is_paid_key:
        # Paid key: User tá»± quáº£n lÃ½, khÃ´ng can thiá»‡p
        print(f"Google AI (Paid Key):")
        print(f"   ğŸ’³ Paid key detected - high rate limits")
        print(f"   âœ… Sá»­ dá»¥ng {num_workers} threads theo cÃ i Ä‘áº·t cá»§a báº¡n")
        print(f"   ğŸ’¡ Paid keys cÃ³ thá»ƒ handle threads cao hÆ¡n")
        
    # OpenRouter vÃ  cÃ¡c provider khÃ¡c: User tá»± quáº£n lÃ½
        
    if chunk_size_lines is None:
        chunk_size_lines = CHUNK_SIZE_LINES
    else:
        chunk_size_lines = validate_chunk_size(chunk_size_lines)
    
    # Tá»± Ä‘á»™ng táº¡o tÃªn file output náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
    if output_file is None:
        output_file = generate_output_filename(input_file)
        print(f"ğŸ“ Tá»± Ä‘á»™ng táº¡o tÃªn file output: {output_file}")
    
    print(f"Báº¯t Ä‘áº§u dá»‹ch file: {input_file}")
    print(f"File output: {output_file}")
    print(f"Provider: {provider}")
    print(f"Sá»‘ worker threads: {num_workers}")
    print(f"KÃ­ch thÆ°á»›c chunk: {chunk_size_lines} dÃ²ng")
    
    # Validate API key trÆ°á»›c khi báº¯t Ä‘áº§u translation
    print("ğŸ”‘ Äang kiá»ƒm tra API key...")
    
    # Test tá»«ng key riÃªng biá»‡t Ä‘á»ƒ xÃ¡c Ä‘á»‹nh quota isolation
    if isinstance(api_key, list) and len(api_key) > 1:
        print(f"ğŸ§ª Testing quota isolation vá»›i {len(api_key)} keys...")
        for i, key in enumerate(api_key[:3], 1):  # Test 3 keys Ä‘áº§u
            is_valid, validation_message = validate_api_key_before_translation(key, model_name, provider)
            if is_valid:
                print(f"âœ… Key #{i}: {validation_message}")
            else:
                print(f"âŒ Key #{i}: {validation_message}")
    else:
        is_valid, validation_message = validate_api_key_before_translation(validation_key, model_name, provider)
        if not is_valid:
            print(f"âŒ {validation_message}")
            return False
        else:
            print(f"âœ… {validation_message}")

    progress_file_path = f"{input_file}{PROGRESS_FILE_SUFFIX}"

    # Láº¥y tiáº¿n Ä‘á»™ tá»« file vá»›i thÃ´ng tin chi tiáº¿t
    progress_data = load_progress_with_info(progress_file_path)
    completed_chunks = progress_data.get('completed_chunks', 0)
    
    # Hiá»ƒn thá»‹ thÃ´ng tin lá»—i cuá»‘i náº¿u cÃ³
    if 'last_error' in progress_data:
        last_error = progress_data['last_error']
        print(f"âš ï¸ Lá»—i cuá»‘i: {last_error['message']} (chunk {last_error['chunk_index']}, lines {last_error['line_range']})")
    
    print(f"ÄÃ£ hoÃ n thÃ nh {completed_chunks} chunk trÆ°á»›c Ä‘Ã³.")

    # Thá»i gian báº¯t Ä‘áº§u Ä‘á»ƒ tÃ­nh hiá»‡u suáº¥t
    start_time = time.time()
    
    # System instruction cho AI - sá»­ dá»¥ng custom hoáº·c default
    if system_instruction is None:
        system_instruction = """NHIá»†M Vá»¤ CHÃNH: Dá»‹ch vÄƒn báº£n sang tiáº¿ng Viá»‡t hiá»‡n Ä‘áº¡i, tá»± nhiÃªn, Ä‘áº£m báº£o xÆ°ng hÃ´ chÃ­nh xÃ¡c vÃ  phÃ¹ há»£p vá»›i má»‘i quan há»‡ nhÃ¢n váº­t.

QUY Táº®C PHÃ‚N TÃCH VÃ€ Dá»ŠCH THUáº¬T:
1.  **XÃC Äá»ŠNH Bá»I Cáº¢NH:** TrÆ°á»›c khi dá»‹ch, hÃ£y phÃ¢n tÃ­ch ká»¹ lÆ°á»¡ng bá»‘i cáº£nh, vai váº¿, tuá»•i tÃ¡c, vÃ  cáº¥p báº­c giá»¯a cÃ¡c nhÃ¢n váº­t Ä‘á»ƒ xÃ¡c Ä‘á»‹nh má»‘i quan há»‡ chÃ­nh xÃ¡c (vÃ­ dá»¥: con cÃ¡i - cha máº¹, cáº¥p dÆ°á»›i - cáº¥p trÃªn, vá»£ chá»“ng, ngÆ°á»i yÃªu, báº¡n bÃ¨ thÃ¢n thiáº¿t, ngÆ°á»i láº¡...).
2.  **VÄ‚N PHONG CHUNG:**
    * **NgÃ´n ngá»¯:** Sá»­ dá»¥ng tiáº¿ng Viá»‡t giao tiáº¿p hÃ ng ngÃ y, tá»± nhiÃªn, lÆ°u loÃ¡t.
    * **Tá»« ngá»¯:** Háº¡n cháº¿ tá»‘i Ä‘a tá»« HÃ¡n Viá»‡t cá»©ng nháº¯c, thay tháº¿ báº±ng tá»« ngá»¯ phá»• thÃ´ng hiá»‡n Ä‘áº¡i. VÃ­ dá»¥: "cáº£m tháº¥y" thay vÃ¬ "cáº£m nháº­n", "ngÆ°á»i kia" hoáº·c "Anh áº¥y/CÃ´ áº¥y" thay vÃ¬ "Háº¯n/NÃ ng".
3.  **XÆ¯NG HÃ” Cá» Äá»ŠNH (NgÆ°á»i Ká»ƒ Chuyá»‡n/Ngoáº¡i Cáº£nh):**
    * **NgÆ°á»i Ká»ƒ Chuyá»‡n (Thá»© Ba hoáº·c Thá»© Nháº¥t):** LuÃ´n xÆ°ng "tÃ´i" (hiá»‡n Ä‘áº¡i) hoáº·c "ta" (bá»‘i cáº£nh cá»• Ä‘áº¡i/giáº£ tÆ°á»Ÿng).
    * **Äá»‘i tÆ°á»£ng Ä‘Æ°á»£c nháº¯c Ä‘áº¿n:** LuÃ´n gá»i lÃ  "Anh áº¥y/CÃ´ áº¥y" (khi lÃ  nhÃ¢n váº­t chÃ­nh) hoáº·c dÃ¹ng danh xÆ°ng tÆ°Æ¡ng á»©ng vá»›i má»‘i quan há»‡ cá»§a nhÃ¢n váº­t chÃ­nh (vÃ­ dá»¥: "máº¹ cá»§a anh áº¥y", "sáº¿p cá»§a cÃ´ áº¥y").
    * **TUYá»†T Äá»I KHÃ”NG** dÃ¹ng danh xÆ°ng gia Ä‘Ã¬nh (bá»‘/máº¹/anh/chá»‹/em/con) Ä‘á»ƒ gá»i ngÆ°á»i ká»ƒ chuyá»‡n hoáº·c cÃ¡c nhÃ¢n váº­t Ä‘Æ°á»£c nháº¯c Ä‘áº¿n á»Ÿ ngÃ´i thá»© ba.
4.  **XÆ¯NG HÃ” Lá»œI THOáº I (Trá»±c tiáº¿p):**
    * Lá»i thoáº¡i pháº£i Ä‘Æ°á»£c Ä‘áº·t trong dáº¥u ngoáº·c kÃ©p "...".
    * **Danh XÆ°ng:** Dá»±a trÃªn Má»I QUAN Há»† Ä‘Ã£ xÃ¡c Ä‘á»‹nh á»Ÿ Má»¥c 1 Ä‘á»ƒ chá»n danh xÆ°ng (anh/em, cÃ´/chÃº, mÃ y/tao, bá»‘/con, sáº¿p/tÃ´i...) SAO CHO Tá»° NHIÃŠN vÃ  PHÃ™ Há»¢P Vá»šI VÄ‚N HÃ“A GIAO TIáº¾P Cá»¦A NGÆ¯á»œI VIá»†T.
5.  **Tá»ª NGá»® Gá»¢I Dá»¤C, KÃCH THÃCH, KHIÃŠU DÃ‚M:**
    * DÃ¹ng TRá»°C TIáº¾P cÃ¡c tá»«  ngá»¯ Gá»¢I Dá»¤C, KÃCH THÃCH, KHIÃŠU DÃ‚M. KhÃ´ng Ä‘Æ°á»£c dÃ¹ng tá»« nÃ³i giáº£m nÃ³i trÃ¡nh nhÆ° "cÃ¡i áº¥y", "cÃ¡i Ä‘Ã³",....
6. 
âš ï¸ QUAN TRá»ŒNG: CHá»ˆ TRáº¢ Vá»€ Báº¢N Dá»ŠCH, KHÃ”NG GIáº¢I THÃCH, KHÃ”NG BÃŒNH LUáº¬N, KHÃ”NG Äáº¶T TÃŠN NHÃ‚N Váº¬T, KHÃ”NG CÃ“ Báº¤T Ká»² THÃ”NG TIN PHá»¤ NÃ€O KHÃC!

VÄƒn báº£n cáº§n dá»‹ch:
"""
    
    print(f"ğŸ¯ System instruction: {system_instruction[:100]}...")  # Log first 100 chars

    try:
        # Äá»c toÃ n bá»™ file vÃ  chia thÃ nh chunks
        with open(input_file, 'r', encoding='utf-8', errors='replace') as infile:
            all_lines = infile.readlines()
        
        total_lines = len(all_lines)
        print(f"Tá»•ng sá»‘ dÃ²ng trong file: {total_lines}")
        
        # Chia thÃ nh chunks
        chunks = []
        for i in range(0, total_lines, chunk_size_lines):
            chunk_lines = all_lines[i:i + chunk_size_lines]
            chunks.append((len(chunks), chunk_lines, i))  # (chunk_index, chunk_lines, start_line_index)
        
        total_chunks = len(chunks)
        print(f"Tá»•ng sá»‘ chunks: {total_chunks}")
        
        # Kiá»ƒm tra náº¿u Ä‘Ã£ dá»‹ch háº¿t file rá»“i
        if completed_chunks >= total_chunks:
            print(f"âœ… File Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch hoÃ n toÃ n ({completed_chunks}/{total_chunks} chunks).")
            if os.path.exists(progress_file_path):
                os.remove(progress_file_path)
                print(f"ÄÃ£ xÃ³a file tiáº¿n Ä‘á»™: {os.path.basename(progress_file_path)}")
            return True

        # Táº¡o adaptive thread manager Ä‘á»ƒ quáº£n lÃ½ threads Ä‘á»™ng
        adaptive_thread_manager = AdaptiveThreadManager(
            initial_threads=num_workers,
            min_threads=max(1, num_workers // 4),  # Tá»‘i thiá»ƒu 25% threads ban Ä‘áº§u
            max_threads=num_workers * 2  # Tá»‘i Ä‘a 2x threads ban Ä‘áº§u
        )
        
        # Má»Ÿ file output Ä‘á»ƒ ghi káº¿t quáº£
        mode = 'a' if completed_chunks > 0 else 'w'  # Append náº¿u cÃ³ tiáº¿n Ä‘á»™ cÅ©, write náº¿u báº¯t Ä‘áº§u má»›i
        with open(output_file, mode, encoding='utf-8') as outfile:
            
            # Loop chÃ­nh vá»›i adaptive thread management
            current_workers = num_workers
            restart_needed = False
            translation_completed = False  # Flag Ä‘á»ƒ track completion
            
            while not translation_completed:
                print(f"ğŸ”§ Khá»Ÿi Ä‘á»™ng thread pool vá»›i {current_workers} workers...")
                
                # Dictionary Ä‘á»ƒ lÆ°u trá»¯ káº¿t quáº£ dá»‹ch theo thá»© tá»± chunk index
                translated_chunks_results = {}
                next_expected_chunk_to_write = completed_chunks
                total_lines_processed = completed_chunks * chunk_size_lines

                with concurrent.futures.ThreadPoolExecutor(max_workers=current_workers) as executor:
                    
                    futures = {} # LÆ°u trá»¯ cÃ¡c future: {future_object: chunk_index}
                    
                    # Gá»­i cÃ¡c chunks cáº§n dá»‹ch Ä‘áº¿n thread pool
                    chunks_to_process = chunks[completed_chunks:]  # Chá»‰ xá»­ lÃ½ chunks chÆ°a hoÃ n thÃ nh
                    
                    # Context Ä‘Ã£ Ä‘Æ°á»£c truyá»n tá»« GUI
                    print(f"ğŸ¯ Sá»­ dá»¥ng context: {context} ({'hiá»‡n Ä‘áº¡i - tÃ´i' if context == 'modern' else 'cá»• Ä‘áº¡i - ta'})")
                    
                    print(f"Gá»­i {len(chunks_to_process)} chunks Ä‘áº¿n thread pool...")
                    
                    for chunk_data in chunks_to_process:
                        # Kiá»ƒm tra flag dá»«ng trÆ°á»›c khi submit
                        if is_translation_stopped():
                            print("ğŸ›‘ Dá»«ng gá»­i chunks má»›i do ngÆ°á»i dÃ¹ng yÃªu cáº§u")
                            break
                            
                        # Submit vá»›i key_rotator, context, adaptive_thread_manager vÃ  input_file
                        future = executor.submit(process_chunk, api_key, model_name, system_instruction, chunk_data, provider, None, key_rotator, context, is_paid_key, adaptive_thread_manager, input_file, model_settings)
                        futures[future] = chunk_data[0]  # chunk_index
                    
                    # Thu tháº­p káº¿t quáº£ khi cÃ¡c threads hoÃ n thÃ nh
                    for future in concurrent.futures.as_completed(futures):
                        # Kiá»ƒm tra flag dá»«ng vÃ  quota exceeded
                        if is_translation_stopped():
                            if is_quota_exceeded():
                                print("Dá»«ng xá»­ lÃ½ káº¿t quáº£ do API háº¿t quota")
                            else:
                                print("ğŸ›‘ Dá»«ng xá»­ lÃ½ káº¿t quáº£ do ngÆ°á»i dÃ¹ng yÃªu cáº§u")
                            
                            # Há»§y cÃ¡c future chÆ°a hoÃ n thÃ nh
                            for f in futures:
                                if not f.done():
                                    f.cancel()
                            break
                            
                        chunk_index = futures[future]
                        try:
                            result = future.result()  # (chunk_index, translated_text, lines_count, line_range)
                            
                            # Handle result vá»›i line info
                            if len(result) == 4:  # New format with line_range
                                processed_chunk_index, translated_text, lines_count, line_range = result
                            else:  # Old format fallback
                                processed_chunk_index, translated_text, lines_count = result
                                # TÃ­nh toÃ¡n line_range tá»« chunk data
                                chunk_data = chunks[processed_chunk_index]
                                start_line = chunk_data[2]
                                line_range = f"{start_line + 1}:{start_line + len(chunk_data[1])}"
                            
                            # Check for errors
                            if translated_text.startswith('[') and ('Háº¾T QUOTA' in translated_text or 'Lá»–I' in translated_text):
                                # LÆ°u lá»—i vá»›i line info
                                error_info = {
                                    'message': translated_text,
                                    'chunk_index': processed_chunk_index,
                                    'line_range': line_range,
                                    'timestamp': time.time()
                                }
                                save_progress_with_line_info(progress_file_path, next_expected_chunk_to_write, None, error_info)
                                print(f"âŒ Lá»—i táº¡i chunk {processed_chunk_index + 1} (lines {line_range}): {translated_text}")
                                
                                # Náº¿u lÃ  lá»—i quota thÃ¬ dá»«ng ngay
                                if 'Háº¾T QUOTA' in translated_text:
                                    set_quota_exceeded()
                                    break
                                # CÃ¡c lá»—i khÃ¡c váº«n lÆ°u vÃ o buffer Ä‘á»ƒ ghi (vá»›i error message)
                            
                            # LÆ°u káº¿t quáº£ vÃ o buffer táº¡m chá» ghi theo thá»© tá»± (bao gá»“m cáº£ lá»—i)
                            translated_chunks_results[processed_chunk_index] = (translated_text, lines_count, line_range)
                            
                            print(f"âœ… HoÃ n thÃ nh chunk {processed_chunk_index + 1}/{total_chunks}")
                            
                            # Ghi cÃ¡c chunks Ä‘Ã£ hoÃ n thÃ nh vÃ o file output theo Ä‘Ãºng thá»© tá»±
                            while next_expected_chunk_to_write in translated_chunks_results:
                                chunk_text, chunk_lines_count, chunk_line_range = translated_chunks_results.pop(next_expected_chunk_to_write)
                                outfile.write(chunk_text)
                                if not chunk_text.endswith('\n'):
                                    outfile.write('\n')
                                outfile.flush()
                                
                                # Cáº­p nháº­t tiáº¿n Ä‘á»™
                                next_expected_chunk_to_write += 1
                                total_lines_processed += chunk_lines_count
                                
                                # LÆ°u tiáº¿n Ä‘á»™ sau má»—i chunk hoÃ n thÃ nh vá»›i line info
                                current_chunk_info = {
                                    'chunk_index': next_expected_chunk_to_write - 1,
                                    'line_range': chunk_line_range,
                                    'lines_count': chunk_lines_count
                                }
                                save_progress_with_line_info(progress_file_path, next_expected_chunk_to_write, current_chunk_info)
                                
                                # Hiá»ƒn thá»‹ thÃ´ng tin tiáº¿n Ä‘á»™
                                current_time = time.time()
                                elapsed_time = current_time - start_time
                                progress_percent = (next_expected_chunk_to_write / total_chunks) * 100
                                avg_speed = total_lines_processed / elapsed_time if elapsed_time > 0 else 0
                                
                                print(f"Tiáº¿n Ä‘á»™: {next_expected_chunk_to_write}/{total_chunks} chunks ({progress_percent:.1f}%) - {avg_speed:.1f} dÃ²ng/giÃ¢y")
                                
                        except Exception as e:
                            print(f"âŒ Lá»—i khi xá»­ lÃ½ chunk {chunk_index}: {e}")
                    
                    # Ghi ná»‘t cÃ¡c chunks cÃ²n sÃ³t láº¡i trong buffer (náº¿u cÃ³)
                    if translated_chunks_results:
                        print("âš ï¸ Ghi cÃ¡c chunks cÃ²n sÃ³t láº¡i...")
                        sorted_remaining_chunks = sorted(translated_chunks_results.items())
                        for chunk_idx, chunk_data in sorted_remaining_chunks:
                            try:
                                if len(chunk_data) == 3:  # New format with line_range
                                    chunk_text, chunk_lines_count, chunk_line_range = chunk_data
                                else:  # Old format fallback
                                    chunk_text, chunk_lines_count = chunk_data
                                    chunk_line_range = f"unknown"
                                
                                outfile.write(chunk_text)
                                if not chunk_text.endswith('\n'):
                                    outfile.write('\n')
                                outfile.flush()
                                next_expected_chunk_to_write += 1
                                
                                # LÆ°u progress vá»›i line info
                                current_chunk_info = {
                                    'chunk_index': chunk_idx,
                                    'line_range': chunk_line_range,
                                    'lines_count': chunk_lines_count
                                }
                                save_progress_with_line_info(progress_file_path, next_expected_chunk_to_write, current_chunk_info)
                                print(f"âœ… Ghi chunk bá»‹ sÃ³t: {chunk_idx + 1} (lines {chunk_line_range})")
                            except Exception as e:
                                print(f"âŒ Lá»—i khi ghi chunk {chunk_idx}: {e}")
                
                # Sau khi ThreadPoolExecutor hoÃ n thÃ nh, kiá»ƒm tra xem Ä‘Ã£ dá»‹ch háº¿t chÆ°a
                if next_expected_chunk_to_write >= total_chunks:
                    print(f"ğŸ‰ ÄÃ£ hoÃ n thÃ nh táº¥t cáº£ {total_chunks} chunks!")
                    translation_completed = True
                    break  # ThoÃ¡t vÃ²ng láº·p while
                
                # Kiá»ƒm tra náº¿u bá»‹ dá»«ng
                if is_translation_stopped():
                    print(f"âš ï¸ PhÃ¡t hiá»‡n yÃªu cáº§u dá»«ng, thoÃ¡t vÃ²ng láº·p...")
                    translation_completed = True  # ÄÃ¡nh dáº¥u Ä‘á»ƒ thoÃ¡t
                    break
            
            # Kiá»ƒm tra xem cÃ³ bá»‹ dá»«ng giá»¯a chá»«ng khÃ´ng
            if is_translation_stopped():
                if is_quota_exceeded():
                    print(f"API Ä‘Ã£ háº¿t quota!")
                    print(f"Äá»ƒ tiáº¿p tá»¥c dá»‹ch, vui lÃ²ng:")
                    print(f" 1. Táº¡o tÃ i khoáº£n Google Cloud má»›i")
                    print(f" 2. Nháº­n 300$ credit miá»…n phÃ­") 
                    print(f" 3. Táº¡o API key má»›i tá»« ai.google.dev")
                    print(f" 4. Cáº­p nháº­t API key vÃ  tiáº¿p tá»¥c dá»‹ch")
                    print(f"ÄÃ£ xá»­ lÃ½ {next_expected_chunk_to_write}/{total_chunks} chunks.")
                    print(f"Tiáº¿n Ä‘á»™ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u Ä‘á»ƒ tiáº¿p tá»¥c sau.")
                    return False
                else:
                    print(f"ğŸ›‘ Tiáº¿n trÃ¬nh dá»‹ch Ä‘Ã£ bá»‹ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng.")
                    print(f"ÄÃ£ xá»­ lÃ½ {next_expected_chunk_to_write}/{total_chunks} chunks.")
                    print(f"ğŸ’¾ Tiáº¿n Ä‘á»™ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u. Báº¡n cÃ³ thá»ƒ tiáº¿p tá»¥c dá»‹ch sau.")
                    return False

            # HoÃ n thÃ nh
            total_time = time.time() - start_time
            if next_expected_chunk_to_write >= total_chunks:
                print(f"âœ… Dá»‹ch hoÃ n thÃ nh file: {os.path.basename(input_file)}")
                print(f"ÄÃ£ dá»‹ch {total_chunks} chunks ({total_lines} dÃ²ng) trong {total_time:.2f}s")
                print(f"Tá»‘c Ä‘á»™ trung bÃ¬nh: {total_lines / total_time:.2f} dÃ²ng/giÃ¢y")
                print(f"File dá»‹ch Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_file}")
                
                # Print key usage stats if using key rotator
                if key_rotator:
                    key_rotator.print_stats()
                    
                    # Print health summary for ImprovedKeyRotator
                    if hasattr(key_rotator, 'get_health_summary'):
                        summary = key_rotator.get_health_summary()
                        print(f"\nğŸ“Š Key Health Summary:")
                        print(f"   Healthy keys: {summary['healthy_keys']}/{summary['total_keys']}")
                        print(f"   Total success: {summary['total_success']}")
                        print(f"   Total errors: {summary['total_error']}")
                        print(f"   Rate limit errors: {summary['total_rate_limit']}")
                        print(f"   Overall success rate: {summary['success_rate']:.1f}%")
                        print()
                
                # Print ENHANCED rate limiter stats for Google AI
                if provider == "Google AI" and key_rotator:
                    print("\nğŸ“Š Enhanced Rate Limiter Statistics:")
                    for i, key in enumerate(key_rotator.keys if hasattr(key_rotator, 'keys') else key_rotator.api_keys, 1):
                        limiter = get_enhanced_rate_limiter(model_name, provider, key, is_paid_key)
                        if limiter:
                            stats = limiter.get_stats()
                            key_display = f"key_***{_get_key_hash(key)}"
                            print(f"   Key #{i} ({key_display}):")
                            print(f"     RPM: {stats['rpm_usage']}/{stats['rpm_max']} ({stats['rpm_utilization']:.1%})")
                            
                            if stats.get('tpm_max'):
                                print(f"     TPM: {stats['tpm_usage']:,}/{stats['tpm_max']:,} ({stats['tpm_utilization']:.1%})")
                            
                            if stats.get('rpd_max'):
                                print(f"     RPD: {stats['rpd_usage']}/{stats['rpd_max']} ({stats['rpd_remaining']} remaining)")
                            
                            if stats.get('throttle_factor', 1.0) < 1.0:
                                print(f"     Throttle: {stats['throttle_factor']:.1%} (errors: {stats['consecutive_errors']})")
                    print()

                # XÃ³a file tiáº¿n Ä‘á»™ khi hoÃ n thÃ nh
                if os.path.exists(progress_file_path):
                    os.remove(progress_file_path)
                    print(f"ÄÃ£ xÃ³a file tiáº¿n Ä‘á»™: {os.path.basename(progress_file_path)}")
                
                # Tá»± Ä‘á»™ng reformat file sau khi dá»‹ch xong
                if CAN_REFORMAT:
                    print("\nğŸ”§ Báº¯t Ä‘áº§u reformat file Ä‘Ã£ dá»‹ch...")
                    try:
                        fix_text_format(output_file)
                        print("âœ… Reformat hoÃ n thÃ nh!")
                    except Exception as e:
                        print(f"âš ï¸ Lá»—i khi reformat: {e}")
                else:
                    print("âš ï¸ Chá»©c nÄƒng reformat khÃ´ng kháº£ dá»¥")
                
                # Káº¿t thÃºc ThreadPoolExecutor - hoÃ n thÃ nh
                print(f"âœ… Dá»‹ch hoÃ n thÃ nh!")
                return True  # Exit function successfully
            
        # ThoÃ¡t khá»i adaptive loop khi hoÃ n thÃ nh
        return True

    except FileNotFoundError:
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file Ä‘áº§u vÃ o '{input_file}'.")
        return False
    except Exception as e:
        print(f"âŒ ÄÃ£ xáº£y ra lá»—i khÃ´ng mong muá»‘n: {e}")
        print("Tiáº¿n Ä‘á»™ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u. Báº¡n cÃ³ thá»ƒ cháº¡y láº¡i chÆ°Æ¡ng trÃ¬nh Ä‘á»ƒ tiáº¿p tá»¥c.")
        return False


def load_api_key():
    """Tá»± Ä‘á»™ng load API key tá»« environment variable hoáº·c file config"""
    # Thá»­ load tá»« environment variable
    import os
    api_key = os.getenv('GOOGLE_AI_API_KEY')
    if api_key:
        print(f"âœ… ÄÃ£ load API key tá»« environment variable")
        return api_key
    
    # Thá»­ load tá»« file config.json
    try:
        if os.path.exists('config.json'):
            with open('config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
                api_key = config.get('api_key')
                if api_key:
                    print(f"âœ… ÄÃ£ load API key tá»« config.json")
                    return api_key
    except:
        pass
    
    return None

def main():
    """Interactive main function for command line usage"""
    print("=== TranslateNovelAI - Command Line Version ===\n")
    
    # Thá»­ tá»± Ä‘á»™ng load API Key
    api_key = load_api_key()
    
    if not api_key:
        # Nháº­p API Key manually
        api_key = input("Nháº­p Google AI API Key: ").strip()
        if not api_key:
            print("âŒ API Key khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng!")
            return
        
        # Há»i cÃ³ muá»‘n lÆ°u vÃ o config.json khÃ´ng
        save_key = input("ğŸ’¾ LÆ°u API key vÃ o config.json? (y/N): ").lower().strip()
        if save_key == 'y':
            try:
                config = {'api_key': api_key}
                with open('config.json', 'w', encoding='utf-8') as f:
                    json.dump(config, f, indent=2)
                print("âœ… ÄÃ£ lÆ°u API key vÃ o config.json")
            except Exception as e:
                print(f"âš ï¸ Lá»—i lÆ°u config: {e}")
    else:
        print(f"ğŸ”‘ API Key: {api_key[:10]}***{api_key[-10:]}")
    
    # Nháº­p Ä‘Æ°á»ng dáº«n file input
    input_file = input("Nháº­p Ä‘Æ°á»ng dáº«n file truyá»‡n cáº§n dá»‹ch: ").strip()
    if not input_file:
        print("âŒ ÄÆ°á»ng dáº«n file khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng!")
        return
    
    # Kiá»ƒm tra file tá»“n táº¡i
    if not os.path.exists(input_file):
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {input_file}")
        return
    
    # TÃ¹y chá»n file output (cÃ³ thá»ƒ Ä‘á»ƒ trá»‘ng)
    output_file = input("Nháº­p Ä‘Æ°á»ng dáº«n file output (Ä‘á»ƒ trá»‘ng Ä‘á»ƒ tá»± Ä‘á»™ng táº¡o): ").strip()
    if not output_file:
        output_file = None
        print("ğŸ“ Sáº½ tá»± Ä‘á»™ng táº¡o tÃªn file output")
    
    # TÃ¹y chá»n model
    print("\nChá»n model:")
    print("1. gemini-2.0-flash (khuyáº¿n nghá»‹)")
    print("2. gemini-1.5-flash")
    print("3. gemini-1.5-pro")
    
    model_choice = input("Nháº­p lá»±a chá»n (1-3, máº·c Ä‘á»‹nh 1): ").strip()
    model_map = {
        "1": "gemini-2.0-flash",
        "2": "gemini-1.5-flash", 
        "3": "gemini-1.5-pro",
        "": "gemini-2.0-flash"  # Default
    }
    
    model_name = model_map.get(model_choice, "gemini-2.0-flash")
    print(f"ğŸ“± Sá»­ dá»¥ng model: {model_name}")
    
    # XÃ¡c nháº­n trÆ°á»›c khi báº¯t Ä‘áº§u
    print(f"\nğŸ“‹ ThÃ´ng tin dá»‹ch:")
    print(f"  Input: {input_file}")
    print(f"  Output: {output_file or 'Tá»± Ä‘á»™ng táº¡o'}")
    print(f"  Model: {model_name}")
    print(f"  Threads: {get_optimal_threads()}")
    print(f"  Chunk size: {CHUNK_SIZE_LINES} dÃ²ng")
    
    confirm = input("\nğŸš€ Báº¯t Ä‘áº§u dá»‹ch? (y/N): ").lower().strip()
    if confirm != 'y':
        print("âŒ Há»§y bá».")
        return
    
    # Báº¯t Ä‘áº§u dá»‹ch
    print("\n" + "="*50)
    try:
        success = translate_file_optimized(
            input_file=input_file,
            output_file=output_file,
            api_key=api_key,
            model_name=model_name
        )
        
        if success:
            print("\nğŸ‰ Dá»‹ch hoÃ n thÃ nh thÃ nh cÃ´ng!")
        else:
            print("\nâš ï¸ Dá»‹ch chÆ°a hoÃ n thÃ nh.")
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ NgÆ°á»i dÃ¹ng dá»«ng chÆ°Æ¡ng trÃ¬nh.")
        print("ğŸ’¾ Tiáº¿n Ä‘á»™ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u, cÃ³ thá»ƒ tiáº¿p tá»¥c sau.")
    except Exception as e:
        print(f"\nâŒ Lá»—i khÃ´ng mong muá»‘n: {e}")


# --- DEBUG MODE CONTROL ---
def enable_debug_response():
    """Báº­t cháº¿ Ä‘á»™ debug - lÆ°u táº¥t cáº£ responses vÃ o file"""
    global DEBUG_RESPONSE_ENABLED
    DEBUG_RESPONSE_ENABLED = True
    print("ğŸ› Debug mode: ENABLED - Responses sáº½ Ä‘Æ°á»£c lÆ°u vÃ o file debug")

def disable_debug_response():
    """Táº¯t cháº¿ Ä‘á»™ debug"""
    global DEBUG_RESPONSE_ENABLED
    DEBUG_RESPONSE_ENABLED = False
    print("ğŸ› Debug mode: DISABLED")

def is_debug_enabled():
    """Kiá»ƒm tra tráº¡ng thÃ¡i debug mode"""
    return DEBUG_RESPONSE_ENABLED


if __name__ == "__main__":
    main()