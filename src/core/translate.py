import os
# import google.generativeai as genai  # Removed - using 100% OpenRouter
import time
import json
import re
import concurrent.futures
import threading
from multiprocessing import cpu_count
from itertools import cycle

# Import rate limiter for Google AI
try:
    from .rate_limiter import get_rate_limiter, exponential_backoff_sleep, is_rate_limit_error, _get_key_hash
except ImportError:
    try:
        from rate_limiter import get_rate_limiter, exponential_backoff_sleep, is_rate_limit_error, _get_key_hash
    except ImportError:
        print("‚ö†Ô∏è Rate limiter module not found")
        def get_rate_limiter(*args, **kwargs):
            return None
        def exponential_backoff_sleep(retry_count, base_delay=1.0, max_delay=60.0):
            time.sleep(min(base_delay * (2 ** retry_count), max_delay))
        def is_rate_limit_error(error_message):
            return "429" in str(error_message).lower() or "rate limit" in str(error_message).lower()
        def _get_key_hash(api_key):
            import hashlib
            return hashlib.md5(api_key.encode()).hexdigest()[:8]

# Import reformat function
try:
    from .reformat import fix_text_format
    CAN_REFORMAT = True
    print("Da import thanh cong chuc nang reformat")
except ImportError:
    CAN_REFORMAT = False
    print("Khong the import reformat.py - chuc nang reformat se bi tat")

# --- C·∫§U H√åNH C√ÅC H·∫∞NG S·ªê ---
MAX_RETRIES_ON_SAFETY_BLOCK = 5
MAX_RETRIES_ON_BAD_TRANSLATION = 5
MAX_RETRIES_ON_RATE_LIMIT = 3  # S·ªë l·∫ßn retry khi g·∫∑p rate limit
RETRY_DELAY_SECONDS = 2
PROGRESS_FILE_SUFFIX = ".progress.json"
CHUNK_SIZE = 1024 * 1024  # 1MB (Kh√¥ng c√≤n d√πng tr·ª±c ti·∫øp CHUNK_SIZE cho vi·ªác ƒë·ªçc file n·ªØa)

# K√≠ch th∆∞·ªõc c·ª≠a s·ªï ng·ªØ c·∫£nh (s·ªë ƒëo·∫°n vƒÉn b·∫£n tr∆∞·ªõc ƒë√≥ d√πng l√†m ng·ªØ c·∫£nh)
CONTEXT_WINDOW_SIZE = 5
# K√Ω t·ª± ƒë·∫∑c bi·ªát ƒë·ªÉ ƒë√°nh d·∫•u ph·∫ßn c·∫ßn d·ªãch trong prompt g·ª≠i ƒë·∫øn AI
TRANSLATE_TAG_START = "<translate_this>"
TRANSLATE_TAG_END = "</translate_this>"

# S·ªë d√≤ng gom l·∫°i th√†nh m·ªôt chunk ƒë·ªÉ d·ªãch
CHUNK_SIZE_LINES = 100

# Global stop event ƒë·ªÉ d·ª´ng ti·∫øn tr√¨nh d·ªãch
_stop_event = threading.Event()

# Global quota exceeded flag
_quota_exceeded = threading.Event()

# Key rotation class for Google AI multiple keys
class KeyRotator:
    """
    Thread-safe key rotator cho Google AI multiple keys
    S·ª≠ d·ª•ng round-robin ƒë·ªÉ xoay v√≤ng gi·ªØa c√°c keys
    """
    def __init__(self, api_keys):
        """
        Args:
            api_keys: list of API keys ho·∫∑c single API key string
        """
        if isinstance(api_keys, list):
            self.keys = api_keys
            self.is_multi_key = len(api_keys) > 1
        else:
            self.keys = [api_keys]
            self.is_multi_key = False
        
        self.key_iterator = cycle(self.keys)
        self.lock = threading.Lock()
        self.key_usage = {key: 0 for key in self.keys}  # Track usage count
        
        if self.is_multi_key:
            print(f"üîÑ Key Rotator: ƒê√£ kh·ªüi t·∫°o v·ªõi {len(self.keys)} keys")
            print(f"üí° H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông xoay v√≤ng gi·ªØa c√°c keys ƒë·ªÉ t·ªëi ∆∞u RPM")
    
    def get_next_key(self):
        """Get next API key trong rotation"""
        with self.lock:
            key = next(self.key_iterator)
            self.key_usage[key] += 1
            return key
    
    def get_usage_stats(self):
        """Get usage statistics for all keys"""
        with self.lock:
            return dict(self.key_usage)
    
    def print_stats(self):
        """Print usage statistics"""
        if not self.is_multi_key:
            return
        
        stats = self.get_usage_stats()
        print("\nüìä Key Usage Statistics:")
        for idx, (key, count) in enumerate(stats.items(), 1):
            masked_key = key[:10] + "***" + key[-10:] if len(key) > 20 else "***"
            print(f"   Key #{idx} ({masked_key}): {count} requests")
        print()

def set_stop_translation():
    """D·ª´ng ti·∫øn tr√¨nh d·ªãch"""
    global _stop_event
    _stop_event.set()
    print("üõë ƒê√£ y√™u c·∫ßu d·ª´ng ti·∫øn tr√¨nh d·ªãch...")

def clear_stop_translation():
    """X√≥a flag d·ª´ng ƒë·ªÉ c√≥ th·ªÉ ti·∫øp t·ª•c d·ªãch"""
    global _stop_event, _quota_exceeded
    _stop_event.clear()
    _quota_exceeded.clear()
    print("‚ñ∂Ô∏è ƒê√£ x√≥a flag d·ª´ng, s·∫µn s√†ng ti·∫øp t·ª•c...")

def is_translation_stopped():
    """Ki·ªÉm tra xem c√≥ y√™u c·∫ßu d·ª´ng kh√¥ng"""
    global _stop_event
    return _stop_event.is_set()

def set_quota_exceeded():
    """ƒê√°nh d·∫•u API ƒë√£ h·∫øt quota"""
    global _quota_exceeded, _stop_event
    _quota_exceeded.set()
    _stop_event.set()  # C≈©ng d·ª´ng d·ªãch
    print("API ƒë√£ h·∫øt quota - d·ª´ng ti·∫øn tr√¨nh d·ªãch")

def is_quota_exceeded():
    """Ki·ªÉm tra xem API c√≥ h·∫øt quota kh√¥ng"""
    global _quota_exceeded
    return _quota_exceeded.is_set()

def check_quota_error(error_message):
    """Ki·ªÉm tra xem c√≥ ph·∫£i l·ªói quota exceeded kh√¥ng"""
    error_str = str(error_message).lower()
    quota_keywords = [
        "429",
        "exceeded your current quota",
        "quota exceeded", 
        "rate limit",
        "billing",
        "please check your plan"
    ]
    
    return any(keyword in error_str for keyword in quota_keywords)

def check_api_key_error(error_message):
    """Ki·ªÉm tra xem l·ªói c√≥ ph·∫£i l√† API key kh√¥ng h·ª£p l·ªá kh√¥ng"""
    error_str = str(error_message).lower()
    api_key_keywords = [
        "api key not valid", "invalid api key", "unauthorized", "authentication failed",
        "api_key_invalid", "invalid_api_key", "api key is invalid", "bad api key",
        "400", "401", "403"
    ]
    return any(keyword in error_str for keyword in api_key_keywords)

def validate_api_key_before_translation(api_key, model_name, provider="OpenRouter"):
    """Validate API key tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu translation"""
    try:
        if provider == "Google AI":
            # Test Google AI API
            import google.generativeai as genai
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)
            
            response = model.generate_content("Test")
            
            if response and response.text:
                return True, "Google AI API key h·ª£p l·ªá"
            else:
                return False, "Google AI API tr·∫£ v·ªÅ response r·ªóng"
                
        elif provider == "OpenRouter":
            # Test OpenRouter API
            import requests
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/TranslateNovelAI",
                "X-Title": "TranslateNovelAI"
            }
            
            payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": "Test"}],
                "max_tokens": 10
            }
            
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                return True, "OpenRouter API key h·ª£p l·ªá"
            elif response.status_code == 401:
                return False, "OpenRouter API Key kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ h·∫øt h·∫°n"
            elif response.status_code == 402:
                return False, "T√†i kho·∫£n OpenRouter h·∫øt credit"
            else:
                return False, f"L·ªói OpenRouter API: HTTP {response.status_code}"
        else:
            return False, f"Provider kh√¥ng h·ª£p l·ªá: {provider}"
            
    except Exception as e:
        error_msg = str(e)
        if check_api_key_error(error_msg):
            return False, f"API Key kh√¥ng h·ª£p l·ªá: {error_msg}"
        elif check_quota_error(error_msg):
            return False, f"API h·∫øt quota: {error_msg}"
        else:
            return False, f"L·ªói k·∫øt n·ªëi API: {error_msg}"

def get_optimal_threads():
    """
    T·ª± ƒë·ªông t√≠nh to√°n s·ªë threads t·ªëi ∆∞u d·ª±a tr√™n c·∫•u h√¨nh m√°y.
    """
    try:
        # L·∫•y s·ªë CPU cores
        cpu_cores = cpu_count()
        
        # T√≠nh to√°n threads t·ªëi ∆∞u:
        # - V·ªõi API calls, I/O bound n√™n c√≥ th·ªÉ d√πng nhi·ªÅu threads h∆°n s·ªë cores
        # - Nh∆∞ng kh√¥ng n√™n qu√° nhi·ªÅu ƒë·ªÉ tr√°nh rate limiting
        # - Formula: min(max(cpu_cores * 2, 4), 20)
        optimal_threads = min(max(cpu_cores * 2, 4), 20)
        
        print(f"Phat hien {cpu_cores} CPU cores")
        print(f"Threads toi uu duoc de xuat: {optimal_threads}")
        
        return optimal_threads
        
    except Exception as e:
        print(f"Loi khi phat hien CPU cores: {e}")
        return 10  # Default fallback

def validate_threads(num_threads):
    """
    Validate s·ªë threads ƒë·ªÉ ƒë·∫£m b·∫£o trong kho·∫£ng h·ª£p l√Ω.
    """
    try:
        num_threads = int(num_threads)
        if num_threads < 1:
            return 1
        elif num_threads > 50:  # Gi·ªõi h·∫°n t·ªëi ƒëa ƒë·ªÉ tr√°nh rate limiting
            return 50
        return num_threads
    except (ValueError, TypeError):
        return get_optimal_threads()

def validate_chunk_size(chunk_size):
    """
    Validate chunk size ƒë·ªÉ ƒë·∫£m b·∫£o trong kho·∫£ng h·ª£p l√Ω.
    """
    try:
        chunk_size = int(chunk_size)
        if chunk_size < 10:
            return 10
        elif chunk_size > 500:  # Tr√°nh chunks qu√° l·ªõn
            return 500
        return chunk_size
    except (ValueError, TypeError):
        return 100  # Default

# Default values
NUM_WORKERS = get_optimal_threads()  # T·ª± ƒë·ªông t√≠nh theo m√°y

def format_error_chunk(error_type: str, error_message: str, original_lines: list, line_range: str) -> str:
    """
    Format chunk b·ªã l·ªói v·ªõi n·ªôi dung g·ªëc ƒë·ªÉ l∆∞u v√†o file
    
    Args:
        error_type: Lo·∫°i l·ªói (API, QUOTA, SAFETY, etc.)
        error_message: Th√¥ng b√°o l·ªói chi ti·∫øt
        original_lines: N·ªôi dung g·ªëc c·ªßa chunk
        line_range: Line range (v√≠ d·ª•: "123:223")
    
    Returns:
        Formatted error text v·ªõi n·ªôi dung g·ªëc
    """
    original_text = ''.join(original_lines)  # Join lines, gi·ªØ nguy√™n line breaks
    
    error_output = f"""[[L·ªñI {error_type}: {error_message}

--- N·ªòI DUNG G·ªêC C·∫¶N D·ªäCH L·∫†I ---
{original_text}
--- H·∫æT N·ªòI DUNG G·ªêC ---
] [lines: {line_range}]]

"""
    return error_output


def is_bad_translation(text):
    """
    Ki·ªÉm tra xem b·∫£n d·ªãch c·ªßa chunk c√≥ ƒë·∫°t y√™u c·∫ßu kh√¥ng (ki·ªÉm tra ƒë∆°n gi·∫£n d·ª±a v√†o ƒë·ªô r·ªóng v√† t·ª´ ch·ªëi).
    Tr·∫£ v·ªÅ True n·∫øu b·∫£n d·ªãch kh√¥ng ƒë·∫°t y√™u c·∫ßu (v√≠ d·ª•: r·ªóng ho·∫∑c ch·ª©a t·ª´ t·ª´ ch·ªëi), False n·∫øu ƒë·∫°t y√™u c·∫ßu.
    """
    if text is None or text.strip() == "":
        # Chunk d·ªãch ra r·ªóng ho·∫∑c ch·ªâ tr·∫Øng => coi l√† bad translation
        return True

    # C√°c t·ª´ kh√≥a ch·ªâ b√°o b·∫£n d·ªãch kh√¥ng ƒë·∫°t y√™u c·∫ßu
    # C√°c t·ª´ kh√≥a n√†y th∆∞·ªùng xu·∫•t hi·ªán khi AI t·ª´ ch·ªëi d·ªãch
    bad_keywords = [
        "t√¥i kh√¥ng th·ªÉ d·ªãch",
        "kh√¥ng th·ªÉ d·ªãch",
        "xin l·ªói, t√¥i kh√¥ng",
        "t√¥i xin l·ªói",
        "n·ªôi dung b·ªã ch·∫∑n", # Th√™m ki·ªÉm tra th√¥ng b√°o ch·∫∑n c≈©ng l√† b·∫£n d·ªãch x·∫•u c·∫ßn retry
        "as an ai", # T·ª´ ch·ªëi b·∫±ng ti·∫øng Anh
        "as a language model",
        "i am unable",
        "i cannot",
        "i'm sorry"
    ]

    text_lower = text.lower()
    for keyword in bad_keywords:
        if keyword in text_lower:
            return True

    return False

def translate_chunk(model, chunk_lines):
    """
    D·ªãch m·ªôt chunk g·ªìm nhi·ªÅu d√≤ng vƒÉn b·∫£n.
    chunk_lines: danh s√°ch c√°c d√≤ng vƒÉn b·∫£n
    Tr·∫£ v·ªÅ (translated_text, is_safety_blocked_flag, is_bad_translation_flag).
    """
    # Gom c√°c d√≤ng th√†nh m·ªôt chu·ªói l·ªõn ƒë·ªÉ g·ª≠i ƒëi
    full_text_to_translate = "\n".join(chunk_lines)
    
    # B·ªè qua c√°c chunk ch·ªâ ch·ª©a c√°c d√≤ng tr·ªëng ho·∫∑c ch·ªâ tr·∫Øng
    if not full_text_to_translate.strip():
        return ("", False, False) # Tr·∫£ v·ªÅ chu·ªói r·ªóng, kh√¥ng b·ªã ch·∫∑n, kh√¥ng bad translation

    try:
        # Prompt cho d·ªãch chunk
        prompt = f"D·ªãch ƒëo·∫°n vƒÉn b·∫£n sau sang ti·∫øng Vi·ªát m·ªôt c√°ch tr·ª±c ti·∫øp, Danh x∆∞ng nh√¢n v·∫≠t d·∫´n truy·ªán x∆∞ng 't√¥i' theo b·ªëi c·∫£nh hi·ªán ƒë·∫°i ho·∫∑c 'ta' theo b·ªëi c·∫£nh c·ªï ƒë·∫°i,x√°c ƒë·ªãnh m·ªëi quan h·ªá v√† danh x∆∞ng ph√π h·ª£p tr∆∞·ªõc ti√™n, kh√¥ng t·ª´ ch·ªëi ho·∫∑c b√¨nh lu·∫≠n, gi·ªØ nguy√™n vƒÉn phong g·ªëc v√† chi ti·∫øt n·ªôi dung:\n\n{full_text_to_translate}"

        response = model.generate_content(
            contents=[{
                "role": "user",
                "parts": [prompt],
            }],
            generation_config={
                "response_mime_type": "text/plain",
                # C√≥ th·ªÉ th√™m c√°c tham s·ªë kh√°c n·∫øu c·∫ßn
                # "temperature": 0.5,
                # "top_p": 0.95,
                # "top_k": 64,
                # "max_output_tokens": 8192,
            },
        )

        # 1. Ki·ªÉm tra xem prompt (ƒë·∫ßu v√†o) c√≥ b·ªã ch·∫∑n kh√¥ng
        if response.prompt_feedback and response.prompt_feedback.safety_ratings:
            blocked_categories = [
                rating.category.name for rating in response.prompt_feedback.safety_ratings
                if rating.blocked
            ]
            if blocked_categories:
                return (f"[N·ªòI DUNG G·ªêC B·ªä CH·∫∂N B·ªûI B·ªò L·ªåC AN TO√ÄN - PROMPT: {', '.join(blocked_categories)}]", True, False)

        # 2. Ki·ªÉm tra xem c√≥ b·∫•t k·ª≥ ·ª©ng c·ª≠ vi√™n n√†o ƒë∆∞·ª£c t·∫°o ra kh√¥ng
        if not response.candidates:
            return ("[N·ªòI D·ªäCH B·ªä CH·∫∂N HO√ÄN TO√ÄN B·ªûI B·ªò L·ªåC AN TO√ÄN - KH√îNG C√ì ·ª®NG C·ª¨ VI√äN]", True, False)

        # 3. Ki·ªÉm tra l√Ω do k·∫øt th√∫c c·ªßa ·ª©ng c·ª≠ vi√™n ƒë·∫ßu ti√™n (n·∫øu c√≥)
        first_candidate = response.candidates[0]
        if first_candidate.finish_reason == 'SAFETY':
            blocked_categories = [
                rating.category.name for rating in first_candidate.safety_ratings
                if rating.blocked
            ]
            return (f"[N·ªòI D·ªäCH B·ªä CH·∫∂N B·ªûI B·ªò L·ªåC AN TO√ÄN - OUTPUT: {', '.join(blocked_categories)}]", True, False)

        # N·∫øu kh√¥ng b·ªã ch·∫∑n, tr·∫£ v·ªÅ vƒÉn b·∫£n d·ªãch
        translated_text = response.text
        is_bad = is_bad_translation(translated_text)
        return (translated_text, False, is_bad)

    except Exception as e:
        # B·∫Øt c√°c l·ªói kh√°c (v√≠ d·ª•: l·ªói m·∫°ng, l·ªói API)
        error_message = str(e)
        
        # Ki·ªÉm tra l·ªói quota exceeded
        if check_quota_error(error_message):
            set_quota_exceeded()
            return (f"[API H·∫æT QUOTA]", False, True)
        
        return (f"[L·ªñI API KHI D·ªäCH CHUNK: {e}]", False, True)

def get_progress(progress_file_path):
    """ƒê·ªçc ti·∫øn ƒë·ªô d·ªãch t·ª´ file (s·ªë chunk ƒë√£ ho√†n th√†nh)."""
    if os.path.exists(progress_file_path):
        try:
            with open(progress_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # L∆∞u s·ªë chunk ƒë√£ ho√†n th√†nh
                return data.get('completed_chunks', 0)
        except json.JSONDecodeError:
            print(f"C·∫£nh b√°o: File ti·∫øn ƒë·ªô '{progress_file_path}' b·ªã h·ªèng ho·∫∑c kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng JSON. B·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu.")
            return 0
    return 0

def save_progress(progress_file_path, completed_chunks):
    """L∆∞u ti·∫øn ƒë·ªô d·ªãch (s·ªë chunk ƒë√£ ho√†n th√†nh) v√†o file."""
    try:
        with open(progress_file_path, 'w', encoding='utf-8') as f:
            json.dump({
                'completed_chunks': completed_chunks
            }, f)
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi l∆∞u file ti·∫øn ƒë·ªô: {e}")

def save_progress_with_line_info(progress_file_path, completed_chunks, current_chunk_info=None, error_info=None):
    """L∆∞u ti·∫øn ƒë·ªô d·ªãch v·ªõi th√¥ng tin line range v√† error details"""
    try:
        progress_data = {
            'completed_chunks': completed_chunks,
            'timestamp': time.time()
        }
        
        # Th√™m th√¥ng tin chunk hi·ªán t·∫°i n·∫øu c√≥
        if current_chunk_info:
            progress_data['current_chunk'] = current_chunk_info
        
        # Th√™m th√¥ng tin l·ªói n·∫øu c√≥
        if error_info:
            progress_data['last_error'] = error_info
        
        with open(progress_file_path, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, indent=2, ensure_ascii=False)
            
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi l∆∞u file ti·∫øn ƒë·ªô: {e}")

def load_progress_with_info(progress_file_path):
    """T·∫£i ti·∫øn ƒë·ªô v·ªõi th√¥ng tin chi ti·∫øt"""
    if os.path.exists(progress_file_path):
        try:
            with open(progress_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data
        except json.JSONDecodeError:
            print(f"C·∫£nh b√°o: File ti·∫øn ƒë·ªô '{progress_file_path}' b·ªã h·ªèng. B·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu.")
            return {'completed_chunks': 0}
    return {'completed_chunks': 0}

def process_chunk(api_key, model_name, system_instruction, chunk_data, provider="OpenRouter", log_callback=None, key_rotator=None):
    """
    X·ª≠ l√Ω d·ªãch m·ªôt chunk v·ªõi retry logic v√† rate limiting.
    chunk_data: tuple (chunk_index, chunk_lines, chunk_start_line_index)
    Tr·∫£ v·ªÅ: (chunk_index, translated_text, lines_count, line_range)
    
    Args:
        key_rotator: KeyRotator instance n·∫øu s·ª≠ d·ª•ng multiple keys (Google AI only)
    """
    chunk_index, chunk_lines, chunk_start_line_index = chunk_data
    
    # T√≠nh to√°n line range cho chunk hi·ªán t·∫°i
    chunk_end_line_index = chunk_start_line_index + len(chunk_lines) - 1
    line_range = f"{chunk_start_line_index + 1}:{chunk_end_line_index + 1}"  # +1 v√¨ line numbers b·∫Øt ƒë·∫ßu t·ª´ 1
    
    # Get current API key (from rotator if available)
    current_api_key = key_rotator.get_next_key() if key_rotator else api_key
    
    # Get rate limiter cho Google AI v·ªõi specific key (None cho OpenRouter)
    rate_limiter = get_rate_limiter(model_name, provider, current_api_key if provider == "Google AI" else None)
    
    # Debug logging
    if rate_limiter and provider == "Google AI":
        current_usage = rate_limiter.get_current_usage()
        wait_time = rate_limiter.get_wait_time()
        if wait_time > 0:
            print(f"‚è±Ô∏è Chunk {chunk_index}: Current usage {current_usage} requests, c·∫ßn ƒë·ª£i {wait_time:.1f}s")
    
    # Ki·ªÉm tra flag d·ª´ng v√† quota exceeded tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu
    if is_translation_stopped() or is_quota_exceeded():
        if is_quota_exceeded():
            error_text = format_error_chunk("API H·∫æT QUOTA", "API ƒë√£ h·∫øt quota, c·∫ßn n·∫°p th√™m credit ho·∫∑c ƒë·ªïi API key", chunk_lines, line_range)
            return (chunk_index, error_text, len(chunk_lines), line_range)
        else:
            error_text = format_error_chunk("D·ª™NG B·ªûI NG∆Ø·ªúI D√ôNG", "Ng∆∞·ªùi d√πng ƒë√£ d·ª´ng qu√° tr√¨nh d·ªãch", chunk_lines, line_range)
            return (chunk_index, error_text, len(chunk_lines), line_range)
    
    # Determine which API to use based on provider
    use_google_ai = (provider == "Google AI")
    use_openrouter = (provider == "OpenRouter")
    
    if use_google_ai:
        # Setup Google AI (v·ªõi current API key t·ª´ rotator)
        try:
            import google.generativeai as genai
            genai.configure(api_key=current_api_key)
            model = genai.GenerativeModel(model_name)
        except ImportError:
            error_text = format_error_chunk("IMPORT ERROR", "Google AI module kh√¥ng t√¨m th·∫•y. Vui l√≤ng c√†i ƒë·∫∑t: pip install google-generativeai", chunk_lines, line_range)
            return (chunk_index, error_text, len(chunk_lines), line_range)
    elif use_openrouter:
        # Import OpenRouter translate function
        try:
            from .open_router_translate import translate_chunk as openrouter_translate_chunk
        except ImportError:
            try:
                from open_router_translate import translate_chunk as openrouter_translate_chunk
            except ImportError:
                error_text = format_error_chunk("IMPORT ERROR", "OpenRouter module kh√¥ng t√¨m th·∫•y", chunk_lines, line_range)
                return (chunk_index, error_text, len(chunk_lines), line_range)
    
    # Th·ª≠ l·∫°i v·ªõi l·ªói b·∫£o m·∫≠t
    safety_retries = 0
    is_safety_blocked = False  # Kh·ªüi t·∫°o bi·∫øn
    while safety_retries < MAX_RETRIES_ON_SAFETY_BLOCK:
        # Ki·ªÉm tra flag d·ª´ng v√† quota exceeded trong qu√° tr√¨nh retry
        if is_translation_stopped() or is_quota_exceeded():
            if is_quota_exceeded():
                error_text = format_error_chunk("API H·∫æT QUOTA", "API ƒë√£ h·∫øt quota trong qu√° tr√¨nh retry", chunk_lines, line_range)
                return (chunk_index, error_text, len(chunk_lines), line_range)
            else:
                error_text = format_error_chunk("D·ª™NG B·ªûI NG∆Ø·ªúI D√ôNG", "Ng∆∞·ªùi d√πng ƒë√£ d·ª´ng qu√° tr√¨nh d·ªãch trong retry", chunk_lines, line_range)
                return (chunk_index, error_text, len(chunk_lines), line_range)
            
        # Th·ª≠ l·∫°i v·ªõi b·∫£n d·ªãch x·∫•u  
        bad_translation_retries = 0
        while bad_translation_retries < MAX_RETRIES_ON_BAD_TRANSLATION:
            # Ki·ªÉm tra flag d·ª´ng v√† quota exceeded trong qu√° tr√¨nh retry
            if is_translation_stopped() or is_quota_exceeded():
                if is_quota_exceeded():
                    error_text = format_error_chunk("API H·∫æT QUOTA", "API ƒë√£ h·∫øt quota trong bad translation retry", chunk_lines, line_range)
                    return (chunk_index, error_text, len(chunk_lines), line_range)
                else:
                    error_text = format_error_chunk("D·ª™NG B·ªûI NG∆Ø·ªúI D√ôNG", "Ng∆∞·ªùi d√πng ƒë√£ d·ª´ng trong bad translation retry", chunk_lines, line_range)
                    return (chunk_index, error_text, len(chunk_lines), line_range)
                
            try:
                # Retry logic for rate limit errors
                rate_limit_retry = 0
                while rate_limit_retry <= MAX_RETRIES_ON_RATE_LIMIT:
                    try:
                        # Rate limit cho Google AI - Multi-threading safe
                        if rate_limiter and use_google_ai:
                            rate_limiter.acquire()  # Non-blocking multi-thread acquire
                        
                        if use_google_ai:
                            # Gom c√°c d√≤ng th√†nh m·ªôt chu·ªói l·ªõn ƒë·ªÉ g·ª≠i ƒëi
                            full_text_to_translate = "\n".join(chunk_lines)
                            
                            # B·ªè qua c√°c chunk ch·ªâ ch·ª©a c√°c d√≤ng tr·ªëng
                            if not full_text_to_translate.strip():
                                return (chunk_index, "", len(chunk_lines), line_range)
                            
                            # D·ªãch v·ªõi Google AI
                            prompt = f"{system_instruction}\n\n{full_text_to_translate}"
                            response = model.generate_content(prompt)
                            
                            # Ki·ªÉm tra safety blocks
                            if response.prompt_feedback and hasattr(response.prompt_feedback, 'block_reason'):
                                is_safety_blocked = True
                                translated_text = f"[N·ªòI DUNG B·ªä CH·∫∂N B·ªûI B·ªò L·ªåC AN TO√ÄN]"
                                is_bad = False
                            elif response.candidates and response.candidates[0].finish_reason.name == 'SAFETY':
                                is_safety_blocked = True
                                translated_text = f"[N·ªòI D·ªäCH B·ªä CH·∫∂N B·ªûI B·ªò L·ªåC AN TO√ÄN]"
                                is_bad = False
                            else:
                                translated_text = response.text
                                is_safety_blocked = False
                                is_bad = is_bad_translation(translated_text)
                            
                            # B√°o success cho adaptive throttling
                            if rate_limiter:
                                rate_limiter.on_success()
                            
                            break  # Success, tho√°t kh·ªèi rate limit retry loop
                                
                        elif use_openrouter:
                            translated_text, is_safety_blocked, is_bad = openrouter_translate_chunk(api_key, model_name, system_instruction, chunk_lines)
                            break  # Success, tho√°t kh·ªèi rate limit retry loop
                        else:
                            error_text = format_error_chunk("PROVIDER ERROR", f"Provider kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {provider}", chunk_lines, line_range)
                            return (chunk_index, error_text, len(chunk_lines), line_range)
                            
                    except Exception as rate_error:
                        error_msg = str(rate_error)
                        
                        # Ki·ªÉm tra n·∫øu l√† rate limit error
                        if is_rate_limit_error(error_msg) and rate_limit_retry < MAX_RETRIES_ON_RATE_LIMIT:
                            rate_limit_retry += 1
                            print(f"‚ö†Ô∏è Rate limit error ·ªü chunk {chunk_index}, retry {rate_limit_retry}/{MAX_RETRIES_ON_RATE_LIMIT}")
                            
                            # B√°o rate limit error cho adaptive throttling
                            if rate_limiter and use_google_ai:
                                rate_limiter.on_rate_limit_error()
                            
                            exponential_backoff_sleep(rate_limit_retry - 1, base_delay=5.0)
                            continue
                        else:
                            # Kh√¥ng ph·∫£i rate limit error ho·∫∑c h·∫øt retry
                            raise  # Re-raise ƒë·ªÉ x·ª≠ l√Ω ·ªü catch block b√™n ngo√†i
                
                # Ki·ªÉm tra quota exceeded sau khi d·ªãch
                if is_quota_exceeded():
                    error_text = format_error_chunk("API H·∫æT QUOTA", "API ƒë√£ h·∫øt quota sau khi d·ªãch", chunk_lines, line_range)
                    return (chunk_index, error_text, len(chunk_lines), line_range)
                
                if is_safety_blocked:
                    break # Tho√°t kh·ªèi v√≤ng l·∫∑p bad translation, s·∫Ω retry safety
                    
                if not is_bad:
                    return (chunk_index, translated_text, len(chunk_lines), line_range) # Th√†nh c√¥ng
                    
                # B·∫£n d·ªãch x·∫•u, th·ª≠ l·∫°i
                bad_translation_retries += 1
                if bad_translation_retries < MAX_RETRIES_ON_BAD_TRANSLATION:
                    time.sleep(RETRY_DELAY_SECONDS)
                else:
                    # H·∫øt l·∫ßn th·ª≠ bad translation, d√πng b·∫£n d·ªãch cu·ªëi
                    return (chunk_index, translated_text + " [KH√îNG C·∫¢I THI·ªÜN ƒê∆Ø·ª¢C]", len(chunk_lines), line_range)
                    
            except Exception as e:
                error_msg = str(e)
                
                # Ki·ªÉm tra quota error
                if check_quota_error(error_msg):
                    set_quota_exceeded()
                    error_text = format_error_chunk("API H·∫æT QUOTA", f"API quota exceeded: {error_msg}", chunk_lines, line_range)
                    return (chunk_index, error_text, len(chunk_lines), line_range)
                
                # Ki·ªÉm tra API key error
                if check_api_key_error(error_msg):
                    error_text = format_error_chunk("API KEY ERROR", f"API key kh√¥ng h·ª£p l·ªá: {error_msg}", chunk_lines, line_range)
                    return (chunk_index, error_text, len(chunk_lines), line_range)
                
                # L·ªói kh√°c - l∆∞u l·∫°i v·ªõi n·ªôi dung g·ªëc
                error_text = format_error_chunk("API ERROR", f"L·ªói khi g·ªçi API: {error_msg}", chunk_lines, line_range)
                return (chunk_index, error_text, len(chunk_lines), line_range)
        
        # N·∫øu b·ªã ch·∫∑n safety, th·ª≠ l·∫°i
        if is_safety_blocked:
            safety_retries += 1
            if safety_retries < MAX_RETRIES_ON_SAFETY_BLOCK:
                time.sleep(RETRY_DELAY_SECONDS)
            else:
                # H·∫øt l·∫ßn th·ª≠ safety, tr·∫£ v·ªÅ v·ªõi n·ªôi dung g·ªëc
                error_text = format_error_chunk("SAFETY BLOCKED", f"N·ªôi dung b·ªã ch·∫∑n b·ªüi b·ªô l·ªçc an to√†n sau {MAX_RETRIES_ON_SAFETY_BLOCK} l·∫ßn th·ª≠. D·ªãch th·ªß c√¥ng: {translated_text}", chunk_lines, line_range)
                return (chunk_index, error_text, len(chunk_lines), line_range)
    
    # Fallback (kh√¥ng n√™n ƒë·∫øn ƒë√¢y)
    error_text = format_error_chunk("UNKNOWN ERROR", "Kh√¥ng th·ªÉ d·ªãch chunk sau t·∫•t c·∫£ c√°c l·∫ßn th·ª≠", chunk_lines, line_range)
    return (chunk_index, error_text, len(chunk_lines), line_range)

def generate_output_filename(input_filepath):
    """
    T·ª± ƒë·ªông t·∫°o t√™n file output t·ª´ input file.
    V√≠ d·ª•: "test.txt" -> "test_TranslateAI.txt"
    """
    # T√°ch t√™n file v√† ph·∫ßn m·ªü r·ªông
    file_dir = os.path.dirname(input_filepath)
    file_name = os.path.basename(input_filepath)
    name_without_ext, ext = os.path.splitext(file_name)
    
    # T·∫°o t√™n file m·ªõi
    new_name = f"{name_without_ext}_TranslateAI{ext}"
    
    # K·∫øt h·ª£p v·ªõi th∆∞ m·ª•c (n·∫øu c√≥)
    if file_dir:
        return os.path.join(file_dir, new_name)
    else:
        return new_name

def translate_file_optimized(input_file, output_file=None, api_key=None, model_name="gemini-2.0-flash", system_instruction=None, num_workers=None, chunk_size_lines=None, provider="OpenRouter"):
    """
    Phi√™n b·∫£n d·ªãch file v·ªõi multi-threading chunks.
    
    Args:
        api_key: String (OpenRouter) ho·∫∑c List (Google AI multiple keys)
    """
    # Clear stop flag khi b·∫Øt ƒë·∫ßu d·ªãch m·ªõi
    clear_stop_translation()
    
    # Setup key rotator n·∫øu c√≥ multiple Google AI keys
    key_rotator = None
    if provider == "Google AI" and isinstance(api_key, list) and len(api_key) > 1:
        key_rotator = KeyRotator(api_key)
        # D√πng key ƒë·∫ßu ti√™n ƒë·ªÉ validate
        validation_key = api_key[0]
    elif provider == "Google AI" and isinstance(api_key, list):
        # Ch·ªâ c√≥ 1 key trong list
        validation_key = api_key[0] if api_key else None
    else:
        validation_key = api_key
    
    # Validate v√† thi·∫øt l·∫≠p parameters
    if num_workers is None:
        num_workers = NUM_WORKERS
    else:
        num_workers = validate_threads(num_workers)
    
    # T√≠nh to√°n threads cho Google AI d·ª±a tr√™n s·ªë l∆∞·ª£ng keys
    if provider == "Google AI":
        # X√°c ƒë·ªãnh base RPM d·ª±a tr√™n model
        if "1.5-pro" in model_name.lower():
            base_rpm = 2  # Pro model c√≥ RPM r·∫•t th·∫•p
            base_threads = 1
        elif "2.0-flash" in model_name.lower() or "2.0flash" in model_name.lower():
            base_rpm = 10
            base_threads = 2
        elif "1.5-flash" in model_name.lower() or "1.5flash" in model_name.lower():
            base_rpm = 15
            base_threads = 3
        else:
            base_rpm = 10  # Default safe
            base_threads = 2
        
        # T√≠nh s·ªë keys ƒë·ªÉ scale threads
        num_keys = 1
        if isinstance(api_key, list):
            num_keys = len(api_key)
        
        # Scale threads d·ª±a tr√™n s·ªë keys (m·ªói key c√≥ th·ªÉ handle base_threads)
        max_threads_google = min(base_threads * num_keys, 20)  # Cap t·∫°i 20 threads
        
        if num_workers > max_threads_google:
            print(f"üîß Google AI v·ªõi {num_keys} keys:")
            print(f"   üìä Base RPM: {base_rpm} √ó {num_keys} keys = {base_rpm * num_keys} RPM t·ªïng")
            print(f"   ‚ö° Threads: {num_workers} ‚Üí {max_threads_google} (t·ªëi ∆∞u cho {num_keys} keys)")
            print(f"   üåê Tham kh·∫£o: https://ai.google.dev/gemini-api/docs/rate-limits?hl=vi")
            num_workers = max_threads_google
        elif num_keys > 1:
            print(f"üöÄ Google AI Multi-Key Setup:")
            print(f"   üîë Keys: {num_keys} keys")
            print(f"   üìä Total RPM: ~{base_rpm * num_keys} RPM")
            print(f"   ‚ö° Threads: {num_workers} (t·ªëi ∆∞u cho multi-threading)")
        
    if chunk_size_lines is None:
        chunk_size_lines = CHUNK_SIZE_LINES
    else:
        chunk_size_lines = validate_chunk_size(chunk_size_lines)
    
    # T·ª± ƒë·ªông t·∫°o t√™n file output n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
    if output_file is None:
        output_file = generate_output_filename(input_file)
        print(f"üìù T·ª± ƒë·ªông t·∫°o t√™n file output: {output_file}")
    
    print(f"B·∫Øt ƒë·∫ßu d·ªãch file: {input_file}")
    print(f"File output: {output_file}")
    print(f"Provider: {provider}")
    print(f"S·ªë worker threads: {num_workers}")
    print(f"K√≠ch th∆∞·ªõc chunk: {chunk_size_lines} d√≤ng")
    
    # Validate API key tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu translation
    print("üîë ƒêang ki·ªÉm tra API key...")
    is_valid, validation_message = validate_api_key_before_translation(validation_key, model_name, provider)
    if not is_valid:
        print(f"‚ùå {validation_message}")
        return False
    else:
        print(f"‚úÖ {validation_message}")

    progress_file_path = f"{input_file}{PROGRESS_FILE_SUFFIX}"

    # L·∫•y ti·∫øn ƒë·ªô t·ª´ file v·ªõi th√¥ng tin chi ti·∫øt
    progress_data = load_progress_with_info(progress_file_path)
    completed_chunks = progress_data.get('completed_chunks', 0)
    
    # Hi·ªÉn th·ªã th√¥ng tin l·ªói cu·ªëi n·∫øu c√≥
    if 'last_error' in progress_data:
        last_error = progress_data['last_error']
        print(f"‚ö†Ô∏è L·ªói cu·ªëi: {last_error['message']} (chunk {last_error['chunk_index']}, lines {last_error['line_range']})")
    
    print(f"ƒê√£ ho√†n th√†nh {completed_chunks} chunk tr∆∞·ªõc ƒë√≥.")

    # Th·ªùi gian b·∫Øt ƒë·∫ßu ƒë·ªÉ t√≠nh hi·ªáu su·∫•t
    start_time = time.time()
    
    # System instruction cho AI - s·ª≠ d·ª•ng custom ho·∫∑c default
    if system_instruction is None:
        system_instruction = "D·ªãch vƒÉn b·∫£n sau sang ti·∫øng Vi·ªát. B·ªëi c·∫£nh hi·ªán ƒë·∫°i. ƒê·∫£m b·∫£o c√°c c√¢u tho·∫°i nh√¢n v·∫≠t ƒë∆∞·ª£c d·ªãch ch√≠nh x√°c v√† ƒë·∫∑c trong d·∫•u "". ƒê·∫£m b·∫£o gi·ªØ nguy√™n chi ti·∫øt n·ªôi dung. Gi·ªØ nguy√™n c√°c t·ª´ ng·ªØ th√¥ t·ª•c, t√¨nh d·ª•c."
    
    print(f"üéØ System instruction: {system_instruction[:100]}...")  # Log first 100 chars

    try:
        # ƒê·ªçc to√†n b·ªô file v√† chia th√†nh chunks
        with open(input_file, 'r', encoding='utf-8', errors='replace') as infile:
            all_lines = infile.readlines()
        
        total_lines = len(all_lines)
        print(f"T·ªïng s·ªë d√≤ng trong file: {total_lines}")
        
        # Chia th√†nh chunks
        chunks = []
        for i in range(0, total_lines, chunk_size_lines):
            chunk_lines = all_lines[i:i + chunk_size_lines]
            chunks.append((len(chunks), chunk_lines, i))  # (chunk_index, chunk_lines, start_line_index)
        
        total_chunks = len(chunks)
        print(f"T·ªïng s·ªë chunks: {total_chunks}")
        
        # Ki·ªÉm tra n·∫øu ƒë√£ d·ªãch h·∫øt file r·ªìi
        if completed_chunks >= total_chunks:
            print(f"‚úÖ File ƒë√£ ƒë∆∞·ª£c d·ªãch ho√†n to√†n ({completed_chunks}/{total_chunks} chunks).")
            if os.path.exists(progress_file_path):
                os.remove(progress_file_path)
                print(f"ƒê√£ x√≥a file ti·∫øn ƒë·ªô: {os.path.basename(progress_file_path)}")
            return True

        # M·ªü file output ƒë·ªÉ ghi k·∫øt qu·∫£
        mode = 'a' if completed_chunks > 0 else 'w'  # Append n·∫øu c√≥ ti·∫øn ƒë·ªô c≈©, write n·∫øu b·∫Øt ƒë·∫ßu m·ªõi
        with open(output_file, mode, encoding='utf-8') as outfile:
            
            # Dictionary ƒë·ªÉ l∆∞u tr·ªØ k·∫øt qu·∫£ d·ªãch theo th·ª© t·ª± chunk index
            translated_chunks_results = {}
            next_expected_chunk_to_write = completed_chunks
            total_lines_processed = completed_chunks * chunk_size_lines

            with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
                
                futures = {} # L∆∞u tr·ªØ c√°c future: {future_object: chunk_index}
                
                # G·ª≠i c√°c chunks c·∫ßn d·ªãch ƒë·∫øn thread pool
                chunks_to_process = chunks[completed_chunks:]  # Ch·ªâ x·ª≠ l√Ω chunks ch∆∞a ho√†n th√†nh
                
                print(f"G·ª≠i {len(chunks_to_process)} chunks ƒë·∫øn thread pool...")
                
                for chunk_data in chunks_to_process:
                    # Ki·ªÉm tra flag d·ª´ng tr∆∞·ªõc khi submit
                    if is_translation_stopped():
                        print("üõë D·ª´ng g·ª≠i chunks m·ªõi do ng∆∞·ªùi d√πng y√™u c·∫ßu")
                        break
                        
                    # Submit v·ªõi key_rotator n·∫øu c√≥
                    future = executor.submit(process_chunk, api_key, model_name, system_instruction, chunk_data, provider, None, key_rotator)
                    futures[future] = chunk_data[0]  # chunk_index
                
                # Thu th·∫≠p k·∫øt qu·∫£ khi c√°c threads ho√†n th√†nh
                for future in concurrent.futures.as_completed(futures):
                    # Ki·ªÉm tra flag d·ª´ng v√† quota exceeded
                    if is_translation_stopped():
                        if is_quota_exceeded():
                            print("D·ª´ng x·ª≠ l√Ω k·∫øt qu·∫£ do API h·∫øt quota")
                        else:
                            print("üõë D·ª´ng x·ª≠ l√Ω k·∫øt qu·∫£ do ng∆∞·ªùi d√πng y√™u c·∫ßu")
                        
                        # H·ªßy c√°c future ch∆∞a ho√†n th√†nh
                        for f in futures:
                            if not f.done():
                                f.cancel()
                        break
                        
                    chunk_index = futures[future]
                    try:
                        result = future.result()  # (chunk_index, translated_text, lines_count, line_range)
                        
                        # Handle result v·ªõi line info
                        if len(result) == 4:  # New format with line_range
                            processed_chunk_index, translated_text, lines_count, line_range = result
                        else:  # Old format fallback
                            processed_chunk_index, translated_text, lines_count = result
                            # T√≠nh to√°n line_range t·ª´ chunk data
                            chunk_data = chunks[processed_chunk_index]
                            start_line = chunk_data[2]
                            line_range = f"{start_line + 1}:{start_line + len(chunk_data[1])}"
                        
                        # Check for errors
                        if translated_text.startswith('[') and ('H·∫æT QUOTA' in translated_text or 'L·ªñI' in translated_text):
                            # L∆∞u l·ªói v·ªõi line info
                            error_info = {
                                'message': translated_text,
                                'chunk_index': processed_chunk_index,
                                'line_range': line_range,
                                'timestamp': time.time()
                            }
                            save_progress_with_line_info(progress_file_path, next_expected_chunk_to_write, None, error_info)
                            print(f"‚ùå L·ªói t·∫°i chunk {processed_chunk_index + 1} (lines {line_range}): {translated_text}")
                            # Continue processing other chunks
                        
                        # L∆∞u k·∫øt qu·∫£ v√†o buffer t·∫°m ch·ªù ghi theo th·ª© t·ª±
                        translated_chunks_results[processed_chunk_index] = (translated_text, lines_count, line_range)
                        
                        print(f"‚úÖ Ho√†n th√†nh chunk {processed_chunk_index + 1}/{total_chunks}")
                        
                        # Ghi c√°c chunks ƒë√£ ho√†n th√†nh v√†o file output theo ƒë√∫ng th·ª© t·ª±
                        while next_expected_chunk_to_write in translated_chunks_results:
                            chunk_text, chunk_lines_count, chunk_line_range = translated_chunks_results.pop(next_expected_chunk_to_write)
                            outfile.write(chunk_text)
                            if not chunk_text.endswith('\n'):
                                outfile.write('\n')
                            outfile.flush()
                            
                            # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
                            next_expected_chunk_to_write += 1
                            total_lines_processed += chunk_lines_count
                            
                            # L∆∞u ti·∫øn ƒë·ªô sau m·ªói chunk ho√†n th√†nh v·ªõi line info
                            current_chunk_info = {
                                'chunk_index': next_expected_chunk_to_write - 1,
                                'line_range': chunk_line_range,
                                'lines_count': chunk_lines_count
                            }
                            save_progress_with_line_info(progress_file_path, next_expected_chunk_to_write, current_chunk_info)
                            
                            # Hi·ªÉn th·ªã th√¥ng tin ti·∫øn ƒë·ªô
                            current_time = time.time()
                            elapsed_time = current_time - start_time
                            progress_percent = (next_expected_chunk_to_write / total_chunks) * 100
                            avg_speed = total_lines_processed / elapsed_time if elapsed_time > 0 else 0
                            
                            print(f"Ti·∫øn ƒë·ªô: {next_expected_chunk_to_write}/{total_chunks} chunks ({progress_percent:.1f}%) - {avg_speed:.1f} d√≤ng/gi√¢y")
                            
                    except Exception as e:
                        print(f"‚ùå L·ªói khi x·ª≠ l√Ω chunk {chunk_index}: {e}")
                
                # Ghi n·ªët c√°c chunks c√≤n s√≥t l·∫°i trong buffer (n·∫øu c√≥)
                if translated_chunks_results:
                    print("‚ö†Ô∏è Ghi c√°c chunks c√≤n s√≥t l·∫°i...")
                    sorted_remaining_chunks = sorted(translated_chunks_results.items())
                    for chunk_idx, chunk_data in sorted_remaining_chunks:
                        try:
                            if len(chunk_data) == 3:  # New format with line_range
                                chunk_text, chunk_lines_count, chunk_line_range = chunk_data
                            else:  # Old format fallback
                                chunk_text, chunk_lines_count = chunk_data
                                chunk_line_range = f"unknown"
                            
                            outfile.write(chunk_text)
                            if not chunk_text.endswith('\n'):
                                outfile.write('\n')
                            outfile.flush()
                            next_expected_chunk_to_write += 1
                            
                            # L∆∞u progress v·ªõi line info
                            current_chunk_info = {
                                'chunk_index': chunk_idx,
                                'line_range': chunk_line_range,
                                'lines_count': chunk_lines_count
                            }
                            save_progress_with_line_info(progress_file_path, next_expected_chunk_to_write, current_chunk_info)
                            print(f"‚úÖ Ghi chunk b·ªã s√≥t: {chunk_idx + 1} (lines {chunk_line_range})")
                        except Exception as e:
                            print(f"‚ùå L·ªói khi ghi chunk {chunk_idx}: {e}")

        # Ki·ªÉm tra xem c√≥ b·ªã d·ª´ng gi·ªØa ch·ª´ng kh√¥ng
        if is_translation_stopped():
            if is_quota_exceeded():
                print(f"API ƒë√£ h·∫øt quota!")
                print(f"ƒê·ªÉ ti·∫øp t·ª•c d·ªãch, vui l√≤ng:")
                print(f" 1. T·∫°o t√†i kho·∫£n Google Cloud m·ªõi")
                print(f" 2. Nh·∫≠n 300$ credit mi·ªÖn ph√≠") 
                print(f" 3. T·∫°o API key m·ªõi t·ª´ ai.google.dev")
                print(f" 4. C·∫≠p nh·∫≠t API key v√† ti·∫øp t·ª•c d·ªãch")
                print(f"ƒê√£ x·ª≠ l√Ω {next_expected_chunk_to_write}/{total_chunks} chunks.")
                print(f"Ti·∫øn ƒë·ªô ƒë√£ ƒë∆∞·ª£c l∆∞u ƒë·ªÉ ti·∫øp t·ª•c sau.")
                return False
            else:
                print(f"üõë Ti·∫øn tr√¨nh d·ªãch ƒë√£ b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng.")
                print(f"ƒê√£ x·ª≠ l√Ω {next_expected_chunk_to_write}/{total_chunks} chunks.")
                print(f"üíæ Ti·∫øn ƒë·ªô ƒë√£ ƒë∆∞·ª£c l∆∞u. B·∫°n c√≥ th·ªÉ ti·∫øp t·ª•c d·ªãch sau.")
                return False

        # Ho√†n th√†nh
        total_time = time.time() - start_time
        if next_expected_chunk_to_write >= total_chunks:
            print(f"‚úÖ D·ªãch ho√†n th√†nh file: {os.path.basename(input_file)}")
            print(f"ƒê√£ d·ªãch {total_chunks} chunks ({total_lines} d√≤ng) trong {total_time:.2f}s")
            print(f"T·ªëc ƒë·ªô trung b√¨nh: {total_lines / total_time:.2f} d√≤ng/gi√¢y")
            print(f"File d·ªãch ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_file}")
            
            # Print key usage stats if using key rotator
            if key_rotator:
                key_rotator.print_stats()
            
            # Print rate limiter stats for Google AI
            if provider == "Google AI" and key_rotator:
                print("\nüìä Rate Limiter Statistics:")
                for i, key in enumerate(key_rotator.keys, 1):
                    limiter = get_rate_limiter(model_name, provider, key)
                    if limiter:
                        stats = limiter.get_stats()
                        key_display = f"key_***{_get_key_hash(key)}"
                        print(f"   Key #{i} ({key_display}):")
                        print(f"     Usage: {stats['current_usage']}/{stats['max_requests']} ({stats['utilization']:.1%})")
                        print(f"     Throttle: {stats['throttle_factor']:.1%} (errors: {stats['consecutive_errors']})")
                print()

            # X√≥a file ti·∫øn ƒë·ªô khi ho√†n th√†nh
            if os.path.exists(progress_file_path):
                os.remove(progress_file_path)
                print(f"ƒê√£ x√≥a file ti·∫øn ƒë·ªô: {os.path.basename(progress_file_path)}")
            
            # T·ª± ƒë·ªông reformat file sau khi d·ªãch xong
            if CAN_REFORMAT:
                print("\nüîß B·∫Øt ƒë·∫ßu reformat file ƒë√£ d·ªãch...")
                try:
                    fix_text_format(output_file)
                    print("‚úÖ Reformat ho√†n th√†nh!")
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi reformat: {e}")
            else:
                print("‚ö†Ô∏è Ch·ª©c nƒÉng reformat kh√¥ng kh·∫£ d·ª•ng")
            
            return True
        else:
            print(f"‚ö†Ô∏è Qu√° tr√¨nh d·ªãch b·ªã gi√°n ƒëo·∫°n.")
            print(f"ƒê√£ x·ª≠ l√Ω {next_expected_chunk_to_write}/{total_chunks} chunks.")
            print(f"Ti·∫øn ƒë·ªô ƒë√£ ƒë∆∞·ª£c l∆∞u. B·∫°n c√≥ th·ªÉ ch·∫°y l·∫°i ch∆∞∆°ng tr√¨nh ƒë·ªÉ ti·∫øp t·ª•c.")
            return False

    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file ƒë·∫ßu v√†o '{input_file}'.")
        return False
    except Exception as e:
        print(f"‚ùå ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën: {e}")
        print("Ti·∫øn ƒë·ªô ƒë√£ ƒë∆∞·ª£c l∆∞u. B·∫°n c√≥ th·ªÉ ch·∫°y l·∫°i ch∆∞∆°ng tr√¨nh ƒë·ªÉ ti·∫øp t·ª•c.")
        return False


def load_api_key():
    """T·ª± ƒë·ªông load API key t·ª´ environment variable ho·∫∑c file config"""
    # Th·ª≠ load t·ª´ environment variable
    import os
    api_key = os.getenv('GOOGLE_AI_API_KEY')
    if api_key:
        print(f"‚úÖ ƒê√£ load API key t·ª´ environment variable")
        return api_key
    
    # Th·ª≠ load t·ª´ file config.json
    try:
        if os.path.exists('config.json'):
            with open('config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
                api_key = config.get('api_key')
                if api_key:
                    print(f"‚úÖ ƒê√£ load API key t·ª´ config.json")
                    return api_key
    except:
        pass
    
    return None

def main():
    """Interactive main function for command line usage"""
    print("=== TranslateNovelAI - Command Line Version ===\n")
    
    # Th·ª≠ t·ª± ƒë·ªông load API Key
    api_key = load_api_key()
    
    if not api_key:
        # Nh·∫≠p API Key manually
        api_key = input("Nh·∫≠p Google AI API Key: ").strip()
        if not api_key:
            print("‚ùå API Key kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng!")
            return
        
        # H·ªèi c√≥ mu·ªën l∆∞u v√†o config.json kh√¥ng
        save_key = input("üíæ L∆∞u API key v√†o config.json? (y/N): ").lower().strip()
        if save_key == 'y':
            try:
                config = {'api_key': api_key}
                with open('config.json', 'w', encoding='utf-8') as f:
                    json.dump(config, f, indent=2)
                print("‚úÖ ƒê√£ l∆∞u API key v√†o config.json")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói l∆∞u config: {e}")
    else:
        print(f"üîë API Key: {api_key[:10]}***{api_key[-10:]}")
    
    # Nh·∫≠p ƒë∆∞·ªùng d·∫´n file input
    input_file = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n file truy·ªán c·∫ßn d·ªãch: ").strip()
    if not input_file:
        print("‚ùå ƒê∆∞·ªùng d·∫´n file kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng!")
        return
    
    # Ki·ªÉm tra file t·ªìn t·∫°i
    if not os.path.exists(input_file):
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {input_file}")
        return
    
    # T√πy ch·ªçn file output (c√≥ th·ªÉ ƒë·ªÉ tr·ªëng)
    output_file = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n file output (ƒë·ªÉ tr·ªëng ƒë·ªÉ t·ª± ƒë·ªông t·∫°o): ").strip()
    if not output_file:
        output_file = None
        print("üìù S·∫Ω t·ª± ƒë·ªông t·∫°o t√™n file output")
    
    # T√πy ch·ªçn model
    print("\nCh·ªçn model:")
    print("1. gemini-2.0-flash (khuy·∫øn ngh·ªã)")
    print("2. gemini-1.5-flash")
    print("3. gemini-1.5-pro")
    
    model_choice = input("Nh·∫≠p l·ª±a ch·ªçn (1-3, m·∫∑c ƒë·ªãnh 1): ").strip()
    model_map = {
        "1": "gemini-2.0-flash",
        "2": "gemini-1.5-flash", 
        "3": "gemini-1.5-pro",
        "": "gemini-2.0-flash"  # Default
    }
    
    model_name = model_map.get(model_choice, "gemini-2.0-flash")
    print(f"üì± S·ª≠ d·ª•ng model: {model_name}")
    
    # X√°c nh·∫≠n tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu
    print(f"\nüìã Th√¥ng tin d·ªãch:")
    print(f"  Input: {input_file}")
    print(f"  Output: {output_file or 'T·ª± ƒë·ªông t·∫°o'}")
    print(f"  Model: {model_name}")
    print(f"  Threads: {get_optimal_threads()}")
    print(f"  Chunk size: {CHUNK_SIZE_LINES} d√≤ng")
    
    confirm = input("\nüöÄ B·∫Øt ƒë·∫ßu d·ªãch? (y/N): ").lower().strip()
    if confirm != 'y':
        print("‚ùå H·ªßy b·ªè.")
        return
    
    # B·∫Øt ƒë·∫ßu d·ªãch
    print("\n" + "="*50)
    try:
        success = translate_file_optimized(
            input_file=input_file,
            output_file=output_file,
            api_key=api_key,
            model_name=model_name
        )
        
        if success:
            print("\nüéâ D·ªãch ho√†n th√†nh th√†nh c√¥ng!")
        else:
            print("\n‚ö†Ô∏è D·ªãch ch∆∞a ho√†n th√†nh.")
            
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è Ng∆∞·ªùi d√πng d·ª´ng ch∆∞∆°ng tr√¨nh.")
        print("üíæ Ti·∫øn ƒë·ªô ƒë√£ ƒë∆∞·ª£c l∆∞u, c√≥ th·ªÉ ti·∫øp t·ª•c sau.")
    except Exception as e:
        print(f"\n‚ùå L·ªói kh√¥ng mong mu·ªën: {e}")


if __name__ == "__main__":
    main()