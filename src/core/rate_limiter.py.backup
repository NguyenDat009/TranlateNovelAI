#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Multi-Threading Safe Rate Limiter cho Google AI API
Gi√∫p tr√°nh v∆∞·ª£t qu√° gi·ªõi h·∫°n RPM (Requests Per Minute) c·ªßa Google AI
H·ªó tr·ª£ th·ª±c s·ª± multi-threading v·ªõi adaptive throttling
"""

import time
import threading
from collections import deque
from datetime import datetime, timedelta


class MultiThreadRateLimiter:
    """
    Multi-threading safe rate limiter s·ª≠ d·ª•ng sliding window
    
    Google AI Free Tier Limits:
    - Gemini 2.0 Flash: 10 RPM, 1,500,000 TPM
    - Gemini 1.5 Flash: 15 RPM, 1,000,000 TPM  
    - Gemini 1.5 Pro: 2 RPM, 32,000 TPM
    
    Features:
    - Thread-safe operations
    - Adaptive throttling khi g·∫∑p rate limit errors
    - Non-blocking acquire cho multi-threading
    """
    
    def __init__(self, requests_per_minute=10, window_seconds=60):
        """
        Initialize rate limiter
        
        Args:
            requests_per_minute: S·ªë requests t·ªëi ƒëa m·ªói ph√∫t
            window_seconds: K√≠ch th∆∞·ªõc c·ª≠a s·ªï th·ªùi gian (m·∫∑c ƒë·ªãnh 60s)
        """
        self.base_max_requests = requests_per_minute
        self.max_requests = requests_per_minute
        self.window_seconds = window_seconds
        self.requests = deque()
        self.lock = threading.Lock()
        
        # Adaptive throttling
        self.consecutive_errors = 0
        self.last_error_time = None
        self.throttle_factor = 1.0
        self.min_throttle = 0.3  # T·ªëi thi·ªÉu 30% RPM g·ªëc
        self.max_throttle = 1.0  # T·ªëi ƒëa 100% RPM g·ªëc
        
    def acquire(self):
        """
        Multi-threading safe acquire permission to make a request
        S·ª≠ d·ª•ng distributed timing thay v√¨ blocking sleep
        """
        max_attempts = 10
        attempt = 0
        
        while attempt < max_attempts:
            with self.lock:
                now = datetime.now()
                self._cleanup_old_requests(now)
                
                # Ki·ªÉm tra xem c√≥ slot available kh√¥ng
                if len(self.requests) < self.max_requests:
                    # C√≥ slot, th√™m request v√† return
                    self.requests.append(now)
                    
                    # Log progress occasionally
                    if len(self.requests) % 3 == 0:
                        thread_id = threading.current_thread().ident
                        throttle_info = f" (throttled {self.throttle_factor:.1%})" if self.throttle_factor < 1.0 else ""
                        print(f"üìä Thread {thread_id}: {len(self.requests)}/{self.max_requests} requests{throttle_info}")
                    
                    return  # Success!
                
                # Kh√¥ng c√≥ slot, t√≠nh wait time
                oldest_request = self.requests[0]
                wait_time = (oldest_request + timedelta(seconds=self.window_seconds) - now).total_seconds()
            
            # Sleep NGO√ÄI lock v·ªõi distributed timing
            if wait_time > 0:
                # Distributed sleep: m·ªói thread sleep kh√°c nhau ƒë·ªÉ tr√°nh thundering herd
                thread_id = threading.current_thread().ident or 0
                jitter = (thread_id % 1000) / 1000.0  # 0-1 second jitter
                actual_sleep = min(wait_time + jitter + 1.0, 5.0)  # Th√™m 1s buffer, max 5s sleep
                
                if attempt == 0:  # Ch·ªâ log l·∫ßn ƒë·∫ßu
                    print(f"üö¶ Thread {thread_id}: Rate limit, ƒë·ª£i {actual_sleep:.1f}s...")
                
                time.sleep(actual_sleep)
            else:
                # Ng·∫Øn sleep ƒë·ªÉ tr√°nh busy waiting
                time.sleep(0.1)
            
            attempt += 1
        
        # Fallback: n·∫øu kh√¥ng get ƒë∆∞·ª£c slot sau max_attempts
        print(f"‚ö†Ô∏è Thread {threading.current_thread().ident}: Fallback acquire after {max_attempts} attempts")
        with self.lock:
            self.requests.append(datetime.now())
    
    def _calculate_wait_time(self):
        """T√≠nh wait time m√† kh√¥ng block threads kh√°c"""
        with self.lock:
            now = datetime.now()
            self._cleanup_old_requests(now)
            
            if len(self.requests) < self.max_requests:
                return 0
            
            # T√≠nh th·ªùi gian ch·ªù
            oldest_request = self.requests[0]
            wait_time = (oldest_request + timedelta(seconds=self.window_seconds) - now).total_seconds()
            return max(0, wait_time)
    
    def _cleanup_old_requests(self, now):
        """X√≥a c√°c requests c≈© ngo√†i window"""
        cutoff_time = now - timedelta(seconds=self.window_seconds)
        while self.requests and self.requests[0] < cutoff_time:
            self.requests.popleft()
    
    def get_current_usage(self):
        """Get current number of requests in the window"""
        with self.lock:
            now = datetime.now()
            self._cleanup_old_requests(now)
            return len(self.requests)
    
    def get_wait_time(self):
        """Get time to wait before next request is allowed"""
        return self._calculate_wait_time()
    
    def on_rate_limit_error(self):
        """G·ªçi khi g·∫∑p rate limit error ƒë·ªÉ adaptive throttling"""
        with self.lock:
            self.consecutive_errors += 1
            self.last_error_time = datetime.now()
            
            # Gi·∫£m throttle factor
            if self.consecutive_errors == 1:
                self.throttle_factor = 0.8  # Gi·∫£m 20%
            elif self.consecutive_errors == 2:
                self.throttle_factor = 0.6  # Gi·∫£m 40%
            elif self.consecutive_errors >= 3:
                self.throttle_factor = self.min_throttle  # Gi·∫£m xu·ªëng minimum
            
            # C·∫≠p nh·∫≠t max_requests
            old_max = self.max_requests
            self.max_requests = max(1, int(self.base_max_requests * self.throttle_factor))
            
            print(f"üö® Rate limit error #{self.consecutive_errors}!")
            print(f"   üìâ Throttling: {old_max} ‚Üí {self.max_requests} RPM ({self.throttle_factor:.1%})")
    
    def on_success(self):
        """G·ªçi khi request th√†nh c√¥ng ƒë·ªÉ recovery throttling"""
        with self.lock:
            if self.consecutive_errors > 0:
                # Ch·ªâ recovery sau 30s kh√¥ng c√≥ l·ªói
                if self.last_error_time and (datetime.now() - self.last_error_time).total_seconds() > 30:
                    self.consecutive_errors = max(0, self.consecutive_errors - 1)
                    
                    # TƒÉng d·∫ßn throttle factor
                    if self.consecutive_errors == 0:
                        self.throttle_factor = min(self.max_throttle, self.throttle_factor + 0.1)
                    
                    # C·∫≠p nh·∫≠t max_requests
                    old_max = self.max_requests
                    self.max_requests = max(1, int(self.base_max_requests * self.throttle_factor))
                    
                    if old_max != self.max_requests:
                        print(f"üìà Recovery throttling: {old_max} ‚Üí {self.max_requests} RPM ({self.throttle_factor:.1%})")
    
    def get_stats(self):
        """Get rate limiter statistics"""
        with self.lock:
            return {
                'current_usage': len(self.requests),
                'max_requests': self.max_requests,
                'base_max_requests': self.base_max_requests,
                'throttle_factor': self.throttle_factor,
                'consecutive_errors': self.consecutive_errors,
                'utilization': len(self.requests) / self.max_requests if self.max_requests > 0 else 0
            }


# Backward compatibility alias
RateLimiter = MultiThreadRateLimiter


# Rate limiters cho c√°c models Google AI kh√°c nhau
# Format: {(model_name, api_key_hash): RateLimiter}
_rate_limiters = {}
_lock = threading.Lock()


def _get_key_hash(api_key: str) -> str:
    """Get a hash of API key for use as dictionary key"""
    import hashlib
    return hashlib.md5(api_key.encode()).hexdigest()[:8]


def get_rate_limiter(model_name: str, provider: str = "Google AI", api_key: str = None, is_paid_key: bool = False) -> MultiThreadRateLimiter:
    """
    Get ho·∫∑c t·∫°o rate limiter cho model (v√† key c·ª• th·ªÉ n·∫øu c√≥)
    
    Args:
        model_name: T√™n model
        provider: Provider (ch·ªâ √°p d·ª•ng cho Google AI)
        api_key: API key (d√πng ƒë·ªÉ t·∫°o rate limiter ri√™ng cho m·ªói key)
        
    Returns:
        RateLimiter instance ho·∫∑c None n·∫øu kh√¥ng c·∫ßn rate limiting
    """
    # Ch·ªâ rate limit cho Google AI
    if provider != "Google AI":
        return None
    
    with _lock:
        # T·∫°o unique key cho rate limiter
        if api_key:
            # M·ªói API key c√≥ rate limiter ri√™ng
            key_hash = _get_key_hash(api_key)
            limiter_key = f"{model_name}_{key_hash}"
        else:
            # Backward compatibility: d√πng model_name l√†m key
            limiter_key = model_name
        
        if limiter_key not in _rate_limiters:
            rpm = 10  # Default safe value
            
            if is_paid_key:
                # Paid keys have much higher limits, e.g., 1000 RPM for Gemini 1.5 Pro.
                # We'll set a high but safe limit.
                rpm = 900
                safe_rpm = rpm  # Don't apply safety reduction for paid keys
                
                key_display = f"key_***{key_hash}" if api_key else "default"
                print(f"üîß ƒê√£ t·∫°o rate limiter cho model: {model_name} ({key_display})")
                print(f"   üí≥ S·ª≠ d·ª•ng API key tr·∫£ ph√≠. √Åp d·ª•ng gi·ªõi h·∫°n RPM cao: {safe_rpm} RPM")
            else:
                # Logic for free keys
                # X√°c ƒë·ªãnh RPM d·ª±a tr√™n model
                # Free tier limits updated Oct 2025: https://ai.google.dev/gemini-api/docs/rate-limits
                if "2.0-flash-lite" in model_name.lower():
                    rpm = 30  # Best for free tier!
                elif "2.0-flash-exp" in model_name.lower() or "2.0-flash" in model_name.lower() or "2.0flash" in model_name.lower() or "2.0_flash" in model_name.lower():
                    rpm = 15
                elif "2.5-flash-lite" in model_name.lower():
                    rpm = 15
                elif "2.5-flash" in model_name.lower():
                    rpm = 10
                elif "2.5-pro" in model_name.lower():
                    rpm = 5
                elif "1.5-flash" in model_name.lower() or "1.5flash" in model_name.lower() or "1.5_flash" in model_name.lower():
                    rpm = 15
                elif "1.5-pro" in model_name.lower() or "1.5_pro" in model_name.lower():
                    rpm = 2  # Very low!
                elif "pro" in model_name.lower():
                    rpm = 2  # Conservative for Pro models
                else:
                    rpm = 15  # Default safe value (based on test results)
                
                # ‚úÖ DIFFERENT PROJECTS: Safety factor 85% (aggressive since keys are independent)
                # M·ªói key c√≥ project ri√™ng n√™n c√≥ th·ªÉ aggressive h∆°n
                safe_rpm = int(rpm * 0.85)
                if safe_rpm < 1:
                    safe_rpm = 1
                
                key_display = f"key_***{key_hash}" if api_key else "default"
                print(f"üîß ƒê√£ t·∫°o rate limiter cho model: {model_name} ({key_display})")
                print(f"   üìä Gi·ªõi h·∫°n g·ªëc: {rpm} RPM")
                print(f"   üõ°Ô∏è Gi·ªõi h·∫°n an to√†n: {safe_rpm} RPM (85% - Different Projects)")
                print(f"   ‚ÑπÔ∏è M·ªói key c√≥ project ri√™ng = rate limit ƒë·ªôc l·∫≠p")
                print(f"   üåê Tham kh·∫£o: https://ai.google.dev/gemini-api/docs/rate-limits")
            
            _rate_limiters[limiter_key] = MultiThreadRateLimiter(requests_per_minute=safe_rpm)
        
        return _rate_limiters[limiter_key]


def clear_rate_limiters():
    """Clear all rate limiters (for testing or reset)"""
    global _rate_limiters
    with _lock:
        _rate_limiters.clear()


# Exponential backoff cho retry khi g·∫∑p rate limit errors
def exponential_backoff_sleep(retry_count: int, base_delay: float = 1.0, max_delay: float = 60.0):
    """
    Sleep v·ªõi exponential backoff
    
    Args:
        retry_count: S·ªë l·∫ßn retry hi·ªán t·∫°i (0-indexed)
        base_delay: Delay c∆° b·∫£n (gi√¢y)
        max_delay: Delay t·ªëi ƒëa (gi√¢y)
    """
    delay = min(base_delay * (2 ** retry_count), max_delay)
    print(f"‚è±Ô∏è Exponential backoff: ƒë·ª£i {delay:.1f}s (retry #{retry_count + 1})")
    time.sleep(delay)


def is_rate_limit_error(error_message: str) -> bool:
    """
    Ki·ªÉm tra xem l·ªói c√≥ ph·∫£i l√† rate limit error kh√¥ng
    
    Args:
        error_message: Th√¥ng b√°o l·ªói
        
    Returns:
        True n·∫øu l√† rate limit error
    """
    error_lower = str(error_message).lower()
    rate_limit_keywords = [
        "rate limit",
        "quota exceeded",
        "429",
        "too many requests",
        "resource exhausted",
        "requests per minute",
        "rpm",
        "rate_limit_exceeded",
        "quota_exceeded",
        "too_many_requests",
        # Google AI specific errors
        "resource has been exhausted",
        "quota_exhausted",
        "rate_limit_error",
        # HTTP status codes
        "status: 429",
        "status code: 429",
        "http 429"
    ]
    
    is_rate_limit = any(keyword in error_lower for keyword in rate_limit_keywords)
    
    if is_rate_limit:
        print(f"üö® Detected rate limit error: {error_message[:100]}...")
    
    return is_rate_limit

